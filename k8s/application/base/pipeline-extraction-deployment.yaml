apiVersion: apps/v1
kind: Deployment
metadata:
  name: pipeline-extraction-deployment
spec:
  replicas: 2 # Extraction is resource-intensive, start with 2
  revisionHistoryLimit: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: pipeline-extraction
      worker-type: extraction
  template:
    metadata:
      labels:
        app: pipeline-extraction
        worker-type: extraction
    spec:
      serviceAccountName: default
      nodeSelector:
        doks.digitalocean.com/node-pool: fast-pool # Extraction needs more resources
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: worker-type
                      operator: In
                      values:
                        - extraction
                topologyKey: "kubernetes.io/hostname" # Spread extraction workers across nodes
      containers:
        - name: pipeline
          image: tomatl/diocesan-vitality:pipeline
          imagePullPolicy: Always
          resources:
            requests:
              memory: "2.2Gi" # High memory for concurrent parish processing
              cpu: "800m"
            limits:
              memory: "4Gi"
              cpu: "1500m"
          env:
            # Database configuration
            - name: SUPABASE_URL
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: supabase-url
            - name: SUPABASE_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: supabase-key
            # AI and search API keys
            - name: GENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: genai-api-key
            - name: SEARCH_API_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: search-api-key
            - name: SEARCH_CX
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: search-cx
            # Environment configuration
            - name: HOME
              value: "/tmp"
            - name: WDM_LOCAL_CACHE
              value: "/tmp/webdriver-cache"
            # Worker identification
            - name: WORKER_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # Worker type specification
            - name: WORKER_TYPE
              value: "extraction"
            # Pipeline configuration
            - name: MAX_PARISHES_PER_DIOCESE
              value: "999999" # No cap - extract all parishes
            - name: MONITORING_URL
              value: "http://backend-service:8000"
          # Use distributed pipeline runner with extraction worker type
          command: ["python", "-m", "pipeline.distributed_pipeline_runner"]
          args:
            - "--worker_type"
            - "extraction"
            - "--max_parishes_per_diocese"
            - "$(MAX_PARISHES_PER_DIOCESE)"
            - "--monitoring_url"
            - "$(MONITORING_URL)"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: chrome-user-data
              mountPath: /app/.chrome-user-data
          livenessProbe:
            exec:
              command:
                - python
                - -c
                - |
                  import sys
                  import os
                  sys.path.append(os.getcwd())
                  from core.distributed_work_coordinator import DistributedWorkCoordinator
                  coordinator = DistributedWorkCoordinator()
                  print("healthy")
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            exec:
              command:
                - python
                - -c
                - |
                  import sys
                  import os
                  sys.path.append(os.getcwd())
                  from core.db import get_supabase_client
                  supabase = get_supabase_client()
                  if supabase:
                      print("ready")
                  else:
                      exit(1)
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 2
      volumes:
        - name: tmp
          emptyDir: {}
        - name: chrome-user-data
          emptyDir: {}
      restartPolicy: Always
      terminationGracePeriodSeconds: 120 # Allow time for extraction cleanup
