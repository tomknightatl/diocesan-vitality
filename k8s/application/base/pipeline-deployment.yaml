apiVersion: apps/v1
kind: Deployment
metadata:
  name: pipeline-deployment
spec:
  replicas: 1 # HPA will manage this
  revisionHistoryLimit: 1
  strategy:
    type: Recreate # Terminate old pod before creating new one (avoids anti-affinity conflicts)
  progressDeadlineSeconds: 600 # Fail deployment after 10 minutes to prevent accumulating pods
  selector:
    matchLabels:
      app: pipeline
  template:
    metadata:
      labels:
        app: pipeline
      annotations:
        # Force pod restart when config changes
        configmap/checksum: '{{ include (print $.Template.BasePath "/pipeline-configmap.yaml") . | sha256sum }}'
    spec:
      serviceAccountName: default
      nodeSelector:
        doks.digitalocean.com/node-pool: fast-pool
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - pipeline
                topologyKey: "kubernetes.io/hostname" # Prefer not to run 2 pipeline pods on same node
      containers:
        - name: pipeline
          image: tomatl/diocesan-vitality:pipeline
          imagePullPolicy: Always
          resources:
            requests:
              memory: "2.2Gi" # Increased based on observed 2052Mi peak usage
              cpu: "800m" # Based on observed 670m-1271m usage
            limits:
              memory: "4Gi"
              cpu: "1500m"
          env:
            # Database configuration
            - name: SUPABASE_URL
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: supabase-url
            - name: SUPABASE_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: supabase-key
            # AI and search API keys
            - name: GENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: genai-api-key
            - name: SEARCH_API_KEY
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: search-api-key
            - name: SEARCH_CX
              valueFrom:
                secretKeyRef:
                  name: diocesan-vitality-secrets
                  key: search-cx
            # Environment configuration for Chrome/Selenium
            - name: MPLCONFIGDIR
              value: "/tmp/matplotlib"
            - name: HOME
              value: "/tmp"
            - name: WDM_LOCAL_CACHE
              value: "/tmp/webdriver-cache"
            # Worker identification for coordination
            - name: WORKER_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name # Use pod name as worker ID
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # Pipeline configuration
            - name: MAX_PARISHES_PER_DIOCESE
              value: "50"
            - name: NUM_PARISHES_FOR_SCHEDULE
              value: "101"
            - name: MONITORING_URL
              value: "http://backend-service:8000"
          # Use distributed pipeline runner for worker coordination
          command: ["python", "distributed_pipeline_runner.py"]
          args:
            - "--max_parishes_per_diocese"
            - "$(MAX_PARISHES_PER_DIOCESE)"
            - "--num_parishes_for_schedule"
            - "$(NUM_PARISHES_FOR_SCHEDULE)"
            - "--monitoring_url"
            - "$(MONITORING_URL)"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: chrome-user-data
              mountPath: /app/.chrome-user-data
          # Health checks for HPA
          livenessProbe:
            exec:
              command:
                - python
                - -c
                - |
                  import sys
                  import os
                  sys.path.append(os.getcwd())
                  from core.distributed_work_coordinator import DistributedWorkCoordinator
                  coordinator = DistributedWorkCoordinator()
                  # Simple health check - if we can create coordinator, we're healthy
                  print("healthy")
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            exec:
              command:
                - python
                - -c
                - |
                  import sys
                  import os
                  sys.path.append(os.getcwd())
                  from core.db import get_supabase_client
                  # Check if we can connect to database
                  supabase = get_supabase_client()
                  if supabase:
                      print("ready")
                  else:
                      exit(1)
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 2
      volumes:
        - name: tmp
          emptyDir: {}
        - name: chrome-user-data
          emptyDir: {}
      restartPolicy: Always
      # Graceful shutdown to allow coordination cleanup
      terminationGracePeriodSeconds: 120
