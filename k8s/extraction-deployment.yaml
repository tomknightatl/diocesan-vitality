apiVersion: apps/v1
kind: Deployment
metadata:
  name: extraction-deployment
  namespace: diocesan-vitality
spec:
  replicas: 2  # HPA will manage scaling
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: extraction-worker
  template:
    metadata:
      labels:
        app: extraction-worker
        worker-type: extraction
      annotations:
        # Force pod restart when config changes
        configmap/checksum: "{{ include (print $.Template.BasePath \"/pipeline-configmap.yaml\") . | sha256sum }}"
    spec:
      serviceAccountName: default
      nodeSelector:
        doks.digitalocean.com/node-pool: fast-pool
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: worker-type
                  operator: In
                  values:
                  - extraction
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: extraction-worker
        image: tomatl/diocesan-vitality:pipeline-2025-09-15-22-58-39
        imagePullPolicy: Always
        resources:
          requests:
            memory: "2.2Gi"  # High memory for concurrent processing
            cpu: "800m"
          limits:
            memory: "4Gi"
            cpu: "1500m"
        env:
        # Worker type specialization
        - name: WORKER_TYPE
          value: "extraction"
        # Database configuration
        - name: SUPABASE_URL
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: supabase-url
        - name: SUPABASE_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: supabase-key
        # AI and search API keys
        - name: GENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: genai-api-key
        - name: SEARCH_API_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: search-api-key
        - name: SEARCH_CX
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: search-cx
        # Environment configuration for Chrome/Selenium
        - name: MPLCONFIGDIR
          value: "/tmp/matplotlib"
        - name: HOME
          value: "/tmp"
        - name: WDM_LOCAL_CACHE
          value: "/tmp/webdriver-cache"
        # Worker identification
        - name: WORKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # Pipeline configuration
        - name: MAX_PARISHES_PER_DIOCESE
          value: "50"
        - name: MONITORING_URL
          value: "http://backend-service:8000"
        # Use distributed pipeline runner
        command: ["python", "distributed_pipeline_runner.py"]
        args:
        - "--max_parishes_per_diocese"
        - "$(MAX_PARISHES_PER_DIOCESE)"
        - "--monitoring_url"
        - "$(MONITORING_URL)"
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: chrome-user-data
          mountPath: /app/.chrome-user-data
        # Health checks
        livenessProbe:
          exec:
            command:
            - python
            - -c
            - |
              import sys
              import os
              sys.path.append(os.getcwd())
              from core.distributed_work_coordinator import DistributedWorkCoordinator
              coordinator = DistributedWorkCoordinator(worker_type="extraction")
              print("healthy")
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          exec:
            command:
            - python
            - -c
            - |
              import sys
              import os
              sys.path.append(os.getcwd())
              from core.db import get_supabase_client
              supabase = get_supabase_client()
              if supabase:
                  print("ready")
              else:
                  exit(1)
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 2
      volumes:
      - name: tmp
        emptyDir: {}
      - name: chrome-user-data
        emptyDir: {}
      restartPolicy: Always
      terminationGracePeriodSeconds: 120