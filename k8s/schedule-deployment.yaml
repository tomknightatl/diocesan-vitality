apiVersion: apps/v1
kind: Deployment
metadata:
  name: schedule-deployment
  namespace: diocesan-vitality
spec:
  replicas: 1  # Start with 1, can scale up
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: schedule-worker
  template:
    metadata:
      labels:
        app: schedule-worker
        worker-type: schedule
      annotations:
        # Force pod restart when config changes
        configmap/checksum: "{{ include (print $.Template.BasePath \"/pipeline-configmap.yaml\") . | sha256sum }}"
    spec:
      serviceAccountName: default
      nodeSelector:
        doks.digitalocean.com/node-pool: fast-pool
      containers:
      - name: schedule-worker
        image: tomatl/diocesan-vitality:pipeline-2025-09-15-22-58-39
        imagePullPolicy: Always
        resources:
          requests:
            memory: "1.5Gi"  # WebDriver intensive
            cpu: "600m"
          limits:
            memory: "3Gi"
            cpu: "1000m"
        env:
        # Worker type specialization
        - name: WORKER_TYPE
          value: "schedule"
        # Database configuration
        - name: SUPABASE_URL
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: supabase-url
        - name: SUPABASE_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: supabase-key
        # AI and search API keys
        - name: GENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: genai-api-key
        - name: SEARCH_API_KEY
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: search-api-key
        - name: SEARCH_CX
          valueFrom:
            secretKeyRef:
              name: diocesan-vitality-secrets
              key: search-cx
        # Environment configuration for Chrome/Selenium
        - name: MPLCONFIGDIR
          value: "/tmp/matplotlib"
        - name: HOME
          value: "/tmp"
        - name: WDM_LOCAL_CACHE
          value: "/tmp/webdriver-cache"
        # Worker identification
        - name: WORKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # Pipeline configuration
        - name: NUM_PARISHES_FOR_SCHEDULE
          value: "101"
        - name: MONITORING_URL
          value: "http://backend-service:8000"
        # Use distributed pipeline runner
        command: ["python", "distributed_pipeline_runner.py"]
        args:
        - "--num_parishes_for_schedule"
        - "$(NUM_PARISHES_FOR_SCHEDULE)"
        - "--monitoring_url"
        - "$(MONITORING_URL)"
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: chrome-user-data
          mountPath: /app/.chrome-user-data
        # Health checks
        livenessProbe:
          exec:
            command:
            - python
            - -c
            - |
              import sys
              import os
              sys.path.append(os.getcwd())
              from core.distributed_work_coordinator import DistributedWorkCoordinator
              coordinator = DistributedWorkCoordinator(worker_type="schedule")
              print("healthy")
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          exec:
            command:
            - python
            - -c
            - |
              import sys
              import os
              sys.path.append(os.getcwd())
              from core.db import get_supabase_client
              supabase = get_supabase_client()
              if supabase:
                  print("ready")
              else:
                  exit(1)
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 2
      volumes:
      - name: tmp
        emptyDir: {}
      - name: chrome-user-data
        emptyDir: {}
      restartPolicy: Always
      terminationGracePeriodSeconds: 120