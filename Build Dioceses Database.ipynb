{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdebixheGT9gyz8u1b9Lpy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Build%20Dioceses%20Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhF7zOWbH9fN"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import necessary libraries\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"scraping.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Clone GitHub repository and configure Git\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# GitHub credentials\n",
        "GITHUB_REPO = 'USCCB'\n",
        "GITHUB_USERNAME = userdata.get('GitHubUserforUSCCB')\n",
        "GITHUB_PAT = userdata.get('GitHubPATforUSCCB')\n",
        "\n",
        "# GitHub repository URL\n",
        "REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_PAT}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\"\n",
        "\n",
        "# Check if the repository directory already exists\n",
        "if not os.path.exists(GITHUB_REPO):\n",
        "    # Clone the repository\n",
        "    !git clone {REPO_URL}\n",
        "    os.chdir(GITHUB_REPO)\n",
        "else:\n",
        "    print(f\"Repository {GITHUB_REPO} already exists. Updating...\")\n",
        "    os.chdir(GITHUB_REPO)\n",
        "    !git pull origin main\n",
        "\n",
        "# Configure Git\n",
        "!git config --global user.email \"tomk@github.leemail.me\"\n",
        "!git config --global user.name \"tomknightatl\""
      ],
      "metadata": {
        "id": "eRoD2uxNOD_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define helper functions\n",
        "\n",
        "def get_soup(url, retries=3, backoff_factor=1.0):\n",
        "    \"\"\"\n",
        "    Fetches the content at the given URL and returns a BeautifulSoup object.\n",
        "    Implements retries with exponential backoff in case of request failures.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "                       'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                       'Chrome/58.0.3029.110 Safari/537.3'),\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Accept-Encoding': 'gzip, deflate',\n",
        "        'Connection': 'keep-alive'\n",
        "    }\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            logging.info(f\"Attempt {attempt}: Fetching URL: {url}\")\n",
        "            response = requests.get(url, headers=headers, timeout=20)\n",
        "            logging.info(f\"Received status code: {response.status_code}\")\n",
        "            response.raise_for_status()\n",
        "            return BeautifulSoup(response.text, 'html.parser')\n",
        "        except requests.RequestException as e:\n",
        "            logging.warning(f\"Attempt {attempt} failed with error: {e}\")\n",
        "            if attempt == retries:\n",
        "                logging.error(f\"All {retries} attempts failed for URL: {url}\")\n",
        "                return None\n",
        "            sleep_time = backoff_factor * (2 ** (attempt - 1))\n",
        "            logging.info(f\"Retrying in {sleep_time} seconds...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "def extract_dioceses(soup):\n",
        "    \"\"\"\n",
        "    Extracts dioceses information from the parsed HTML.\n",
        "    Returns a list of dictionaries with diocese details.\n",
        "    \"\"\"\n",
        "    dioceses = []\n",
        "    diocese_containers = soup.find_all('div', class_='views-row')\n",
        "\n",
        "    logging.info(f\"Found {len(diocese_containers)} potential diocese containers\")\n",
        "\n",
        "    for i, container in enumerate(diocese_containers):\n",
        "        logging.info(f\"Processing container {i+1}\")\n",
        "\n",
        "        da_wrap = container.find('div', class_='da-wrap')\n",
        "        if not da_wrap:\n",
        "            logging.warning(f\"No da-wrap found in container {i+1}\")\n",
        "            continue\n",
        "\n",
        "        name_div = da_wrap.find('div', class_='da-title')\n",
        "        diocese_name = name_div.get_text(strip=True) if name_div else \"N/A\"\n",
        "        logging.info(f\"Diocese name: {diocese_name}\")\n",
        "\n",
        "        address_div = da_wrap.find('div', class_='da-address')\n",
        "        address_parts = []\n",
        "        if address_div:\n",
        "            for div in address_div.find_all('div', recursive=False):\n",
        "                text = div.get_text(strip=True)\n",
        "                if text:\n",
        "                    address_parts.append(text)\n",
        "\n",
        "        address = \", \".join(address_parts)\n",
        "        logging.info(f\"Address: {address}\")\n",
        "\n",
        "        website_div = da_wrap.find('div', class_='site')\n",
        "        website_url = website_div.find('a')['href'] if website_div and website_div.find('a') else \"N/A\"\n",
        "        logging.info(f\"Website: {website_url}\")\n",
        "\n",
        "        dioceses.append({\n",
        "            'Name': diocese_name,\n",
        "            'Address': address,\n",
        "            'Website': website_url\n",
        "        })\n",
        "\n",
        "    return dioceses"
      ],
      "metadata": {
        "id": "a7yOnNETIInL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Fetch and parse the HTML content from URL\n",
        "\n",
        "url = \"https://www.usccb.org/about/bishops-and-dioceses/all-dioceses\"\n",
        "soup = get_soup(url)\n",
        "\n",
        "if soup:\n",
        "    print(\"Successfully fetched and parsed the dioceses page.\")\n",
        "    # Print the first 1000 characters of the HTML to check its structure\n",
        "    print(\"First 1000 characters of the HTML:\")\n",
        "    print(soup.prettify()[:1000])\n",
        "else:\n",
        "    print(\"Failed to fetch the dioceses page. Please check your connection or the URL.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "H78OsQfwLvUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Extract dioceses information\n",
        "\n",
        "dioceses = extract_dioceses(soup)\n",
        "print(f\"Extracted information for {len(dioceses)} dioceses.\")\n",
        "\n",
        "if len(dioceses) == 0:\n",
        "    print(\"No dioceses were extracted. Printing the structure of the page:\")\n",
        "    print(soup.prettify())"
      ],
      "metadata": {
        "id": "_Ve1ydjDIKpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create a DataFrame and display results\n",
        "\n",
        "dioceses_df = pd.DataFrame(dioceses)\n",
        "print(dioceses_df.head())"
      ],
      "metadata": {
        "id": "6peqWCFhIMZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Initialize SQLite database and create table\n",
        "\n",
        "conn = sqlite3.connect('data.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS Dioceses (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        Name TEXT,\n",
        "        Address TEXT,\n",
        "        Website TEXT\n",
        "    )\n",
        "''')\n",
        "\n",
        "conn.commit()\n"
      ],
      "metadata": {
        "id": "ijMJez0kIPi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Populate Dioceses table\n",
        "\n",
        "for _, row in dioceses_df.iterrows():\n",
        "    cursor.execute('''\n",
        "        INSERT INTO Dioceses (Name, Address, Website)\n",
        "        VALUES (?, ?, ?)\n",
        "    ''', (row['Name'], row['Address'], row['Website']))\n",
        "\n",
        "conn.commit()\n",
        "print(\"Data has been inserted into the SQLite database.\")\n",
        "\n",
        "# Cell 9: Query and display data from the database\n",
        "\n",
        "cursor.execute('SELECT * FROM Dioceses LIMIT 5')\n",
        "results = cursor.fetchall()\n",
        "\n",
        "print(\"First 5 entries from the database:\")\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "print(\"Database connection closed.\")\n"
      ],
      "metadata": {
        "id": "lbPIlNkWrlr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Commit changes and push to GitHub\n",
        "# Add changes to git\n",
        "!git add data.db\n",
        "\n",
        "# Commit changes\n",
        "!git commit -m \"Added records in data.db using Build Dioceses Database.ipynb\"\n",
        "\n",
        "# Push changes to GitHub\n",
        "!git push origin main"
      ],
      "metadata": {
        "id": "yGX_WpXkO7VM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}