# USCCB Development Makefile
# Quick commands for local development

.PHONY: help dev test quick clean install start stop

help: ## Show this help message
	@echo "USCCB Development Commands"
	@echo "========================="
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-15s\033[0m %s\n", $$1, $$2}'

install: ## Install all dependencies
	@echo "üì¶ Installing Python dependencies..."
	pip install -r requirements.txt
	@if [ -d "frontend" ]; then \
		echo "üì¶ Installing frontend dependencies..."; \
		cd frontend && npm install; \
	fi
	@echo "‚úÖ Dependencies installed"

start: ## Start development services
	@echo "üöÄ Starting development environment..."
	@python scripts/dev_start.py --backend-only

start-full: ## Start backend and frontend
	@echo "üöÄ Starting full development environment..."
	@python scripts/dev_start.py

stop: ## Stop development services
	@echo "üõë Stopping services..."
	@lsof -ti:8000 | xargs -r kill
	@lsof -ti:3000 | xargs -r kill
	@echo "‚úÖ Services stopped"

test: ## Run all development tests
	@echo "üß™ Running development tests..."
	@python scripts/dev_test.py --all

test-quick: ## Run quick tests
	@echo "üß™ Running quick tests..."
	@python scripts/dev_test.py --db --ai --env

dev: ## Quick development setup check
	@echo "üîç Development environment check..."
	@python scripts/dev_test.py --env
	@python scripts/dev_quick.py stats

extract: ## Quick single parish extraction test
	@echo "üèÉ‚Äç‚ôÇÔ∏è Quick extraction test..."
	@python scripts/dev_quick.py extract

diocese: ## Quick diocese scan test
	@echo "üîç Quick diocese scan..."
	@python scripts/dev_quick.py diocese

schedule: ## Quick schedule extraction test
	@echo "‚è∞ Quick schedule test..."
	@python scripts/dev_quick.py schedule

logs: ## View recent logs
	@python scripts/dev_quick.py logs

stats: ## Show database statistics
	@python scripts/dev_quick.py stats

clean: ## Clean cache and temporary files
	@echo "üßπ Cleaning cache and temporary files..."
	@python scripts/dev_quick.py clear-cache
	@find . -name "*.pyc" -delete
	@find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
	@rm -rf .pytest_cache
	@echo "‚úÖ Cleanup complete"

kill-chrome: ## Kill stuck Chrome processes
	@python scripts/dev_quick.py kill-chrome

restart: ## Restart all services
	@python scripts/dev_quick.py restart

ports: ## Check development port usage
	@python scripts/dev_quick.py ports

pipeline: ## Run full pipeline with monitoring (small test)
	@echo "üöÄ Running pipeline test..."
	@python run_pipeline_monitored.py \
		--diocese_id 1 \
		--max_parishes_per_diocese 5 \
		--num_parishes_for_schedule 2 \
		--monitoring_url http://localhost:8000

pipeline-single: ## Run pipeline for single diocese
	@echo "üöÄ Running single diocese pipeline..."
	@python run_pipeline_monitored.py \
		--diocese_id $(DIOCESE_ID) \
		--max_parishes_per_diocese 10 \
		--skip_schedules \
		--monitoring_url http://localhost:8000

format: ## Format code with black
	@echo "üé® Formatting code..."
	@black . --exclude="venv|node_modules"
	@echo "‚úÖ Code formatted"

lint: ## Run linting
	@echo "üîç Running linting..."
	@flake8 . --exclude=venv,node_modules --max-line-length=88 --extend-ignore=E203,W503
	@echo "‚úÖ Linting complete"

env-check: ## Check environment configuration
	@python scripts/dev_test.py --env

db-check: ## Test database connection
	@python scripts/dev_test.py --db

ai-check: ## Test AI API connection
	@python scripts/dev_test.py --ai

webdriver-check: ## Test Chrome WebDriver
	@python scripts/dev_test.py --webdriver

monitor-check: ## Test monitoring integration
	@python scripts/dev_test.py --monitoring

# Infrastructure Commands
# =======================

infra-setup: ## Set up complete infrastructure (8 core steps, usage: make infra-setup CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Setting up complete infrastructure for '$$CLUSTER_LABEL'..." && \
	echo "üìã Executing 8-step infrastructure setup:" && \
	echo "   Steps 1-4: Cluster (Auth ‚Üí Check ‚Üí Create ‚Üí Context)" && \
	echo "   Steps 5-8: Tunnel (Auth ‚Üí Check ‚Üí Create ‚Üí DNS)" && \
	$(MAKE) cluster-auth && \
	$(MAKE) cluster-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) cluster-create CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) cluster-context CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) tunnel-auth && \
	$(MAKE) tunnel-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) tunnel-create CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) tunnel-dns CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "‚úÖ Core infrastructure setup complete (Steps 1-8)!" && \
	echo "üîÑ Proceeding with ArgoCD and applications..." && \
	$(MAKE) argocd-install CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) argocd-apps CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) sealed-secrets-create CLUSTER_LABEL=$$CLUSTER_LABEL && \
	$(MAKE) tunnel-test CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "üéâ Complete infrastructure setup finished for $$CLUSTER_LABEL!"

infra-destroy: ## Destroy complete infrastructure (usage: make infra-destroy CLUSTER_LABEL=dev [FORCE=yes])
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	FORCE=$${FORCE:-no} && \
	CLUSTER_NAME="dv-$$CLUSTER_LABEL" && \
	echo "üö® DESTRUCTIVE: Destroying complete infrastructure for '$$CLUSTER_LABEL'..." && \
	echo "‚ö†Ô∏è  This will permanently delete:" && \
	echo "   - DigitalOcean cluster: $$CLUSTER_NAME" && \
	echo "   - Cloudflare tunnel: do-nyc2-dv-$$CLUSTER_LABEL" && \
	echo "   - DNS records for $$CLUSTER_LABEL.{ui,api,argocd}.diocesanvitality.org" && \
	echo "   - kubectl context: do-nyc2-dv-$$CLUSTER_LABEL" && \
	if [ "$$FORCE" != "yes" ]; then \
		read -p "Are you sure? Type 'yes' to continue: " CONFIRM </dev/tty && \
		if [ "$$CONFIRM" != "yes" ]; then \
			echo "‚ùå Operation cancelled"; \
			exit 1; \
		fi; \
	else \
		echo "‚ö° FORCE=yes detected - skipping confirmation"; \
	fi && \
	echo "üóëÔ∏è  Executing infrastructure destruction (optimized order: Cluster ‚Üí Context ‚Üí Tunnel ‚Üí DNS)..." && \
	echo "" && \
	echo "üìç Step 1/4: Destroying cluster..." && \
	$(MAKE) cluster-destroy CLUSTER_LABEL=$$CLUSTER_LABEL FORCE=yes || true && \
	echo "" && \
	echo "üìç Step 2/4: Cleaning kubectl context..." && \
	$(MAKE) cluster-context-destroy CLUSTER_LABEL=$$CLUSTER_LABEL || true && \
	echo "" && \
	echo "üìç Step 3/4: Destroying Cloudflare tunnel..." && \
	$(MAKE) tunnel-destroy CLUSTER_LABEL=$$CLUSTER_LABEL FORCE=yes || true && \
	echo "" && \
	echo "üìç Step 4/4: Destroying DNS records..." && \
	$(MAKE) tunnel-dns-destroy CLUSTER_LABEL=$$CLUSTER_LABEL || true && \
	echo "" && \
	echo "üîç Verifying infrastructure destruction..." && \
	if $(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster get $$CLUSTER_NAME" 2>/dev/null | grep -q "$$CLUSTER_NAME"; then \
		echo "‚ö†Ô∏è  Warning: Cluster $$CLUSTER_NAME still exists!"; \
	else \
		echo "‚úÖ Cluster $$CLUSTER_NAME confirmed deleted"; \
	fi && \
	if kubectl config get-contexts -o name 2>/dev/null | grep -q "^do-nyc2-dv-$$CLUSTER_LABEL$$"; then \
		echo "‚ö†Ô∏è  Warning: kubectl context still exists!"; \
	else \
		echo "‚úÖ kubectl context confirmed removed"; \
	fi && \
	echo "" && \
	echo "‚úÖ Infrastructure destruction complete for $$CLUSTER_LABEL"

cluster-auth: ## Step a: A5uthenticate with DigitalOcean (usage: make cluster-auth)
	@echo "üîç Step a: Setting up DigitalOcean authentication..." && \
	if [ ! -f .env ]; then \
		echo "‚ùå .env file not found. Please copy .env.example to .env and configure your tokens" && \
		exit 1; \
	fi && \
	DIGITALOCEAN_TOKEN=$$(sed -n 's/^DIGITALOCEAN_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	if [ -z "$$DIGITALOCEAN_TOKEN" ] || [ "$$DIGITALOCEAN_TOKEN" = "<key>" ]; then \
		echo "‚ùå DIGITALOCEAN_TOKEN not set in .env file. Please add your DigitalOcean API token" && \
		exit 1; \
	fi && \
	echo "üîê Authenticating doctl with token from .env..." && \
	echo "üß™ Testing DigitalOcean API connectivity..." && \
	if timeout 5 curl -s --connect-timeout 3 https://api.digitalocean.com/v2 >/dev/null 2>&1; then \
		echo "‚úÖ DigitalOcean API is reachable" && \
		echo "üß™ Testing doctl authentication..." && \
		if DIGITALOCEAN_ACCESS_TOKEN="$$DIGITALOCEAN_TOKEN" timeout 5 doctl auth list >/dev/null 2>&1; then \
			echo "‚úÖ Step a Complete: doctl authentication verified - can access DigitalOcean account"; \
		else \
			echo "‚ùå doctl authentication failed - token may be invalid or API is experiencing issues" && \
			exit 1; \
		fi; \
	else \
		echo "‚ùå DigitalOcean API is not reachable - cannot authenticate without API access" && \
		exit 1; \
	fi

cluster-check: ## Step b: Check if cluster exists (usage: make cluster-check CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Step b: Checking if cluster exists..." && \
	CLUSTER_NAME="dv-$$CLUSTER_LABEL" && \
	$(MAKE) cluster-auth && \
	echo "üîç Querying cluster $$CLUSTER_NAME..." && \
	$(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster get $$CLUSTER_NAME" && \
	echo "‚úÖ Step b Complete: Cluster $$CLUSTER_NAME exists and is accessible"

_doctl-exec: ## Internal helper to execute doctl commands with authentication
	@DIGITALOCEAN_TOKEN=$$(sed -n 's/^DIGITALOCEAN_TOKEN=//p' .env | tr -d '\r\n"'"'"'"'') && \
	DIGITALOCEAN_ACCESS_TOKEN="$$DIGITALOCEAN_TOKEN" timeout 900 doctl $(DOCTL_CMD)

cluster-create: ## Step c: Create cluster (usage: make cluster-create CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step c: Creating DigitalOcean cluster for '$$CLUSTER_LABEL'..." && \
	CLUSTER_NAME="dv-$$CLUSTER_LABEL" && \
	$(MAKE) cluster-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "‚úÖ Step c Complete: Cluster $$CLUSTER_NAME already exists - skipping creation" || { \
		REGION="nyc2" && \
		echo "üîç Checking available Kubernetes versions..." && \
		AVAILABLE_VERSIONS=$$($(MAKE) _doctl-exec DOCTL_CMD="kubernetes options versions --format Slug --no-header" 2>/dev/null) && \
		REQUESTED_MINOR="1.33" && \
		K8S_VERSION=$$(echo "$$AVAILABLE_VERSIONS" | grep "^$$REQUESTED_MINOR" | head -1) && \
		if [ -z "$$K8S_VERSION" ]; then \
			echo "‚ö†Ô∏è  Warning: No version matching $$REQUESTED_MINOR found, using latest available" && \
			K8S_VERSION=$$(echo "$$AVAILABLE_VERSIONS" | head -1); \
		fi && \
		echo "‚úÖ Selected Kubernetes version: $$K8S_VERSION" && \
		echo "üìã Cluster configuration (matching production):" && \
		echo "   Name: $$CLUSTER_NAME" && \
		echo "   Region: $$REGION" && \
		echo "   Kubernetes version: $$K8S_VERSION" && \
		echo "   Node pools:" && \
		echo "     - slow-pool: s-1vcpu-2gb (1 node)" && \
		echo "     - fast-pool: s-2vcpu-4gb (1 node)" && \
		echo "   Auto-upgrade: false" && \
		echo "   HA Control Plane: false" && \
		echo "üèóÔ∏è  Creating cluster $$CLUSTER_NAME with dual node pools..." && \
		echo "üöÄ Starting cluster creation (this may take 5-10 minutes)..." && \
		$(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster create $$CLUSTER_NAME --region $$REGION --version $$K8S_VERSION --node-pool 'name=slow-pool;size=s-1vcpu-2gb;count=1' --node-pool 'name=fast-pool;size=s-2vcpu-4gb;count=1' --auto-upgrade=false --ha=false --tag environment:$$CLUSTER_LABEL --tag project:diocesan-vitality" & \
		CREATE_PID=$$! && \
		echo "üîç Monitoring cluster creation progress..." && \
		while kill -0 $$CREATE_PID 2>/dev/null; do \
			if CURRENT_STATUS=$$($(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster get $$CLUSTER_NAME --format Status --no-header" 2>/dev/null); then \
				echo "üìä Cluster status: $$CURRENT_STATUS ($$(date '+%H:%M:%S'))"; \
			else \
				echo "‚è≥ Cluster initializing... ($$(date '+%H:%M:%S'))"; \
			fi; \
			sleep 30; \
		done && \
		wait $$CREATE_PID && \
		echo "‚úÖ Cluster creation process completed!" && \
		echo "üîç Verifying final cluster status..." && \
		FINAL_STATUS=$$($(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster get $$CLUSTER_NAME --format Status --no-header" 2>/dev/null) && \
		echo "üìä Final cluster status: $$FINAL_STATUS" && \
		if [ "$$FINAL_STATUS" = "running" ]; then \
			echo "‚úÖ Step c Complete: Cluster is running and ready!"; \
			CLUSTER_ID=$$($(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster get $$CLUSTER_NAME --format ID --no-header" 2>/dev/null) && \
			echo "üî¢ Cluster ID: $$CLUSTER_ID"; \
		else \
			echo "‚ö†Ô∏è  Cluster status is $$FINAL_STATUS - may still be initializing"; \
			echo "‚úÖ Step c Complete: Cluster creation initiated"; \
		fi; \
	}

cluster-destroy: ## Step d: Destroy cluster (usage: make cluster-destroy CLUSTER_LABEL=dev [FORCE=yes])
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	FORCE=$${FORCE:-no} && \
	echo "üö® Step d: DESTRUCTIVE - Destroying DigitalOcean cluster for '$$CLUSTER_LABEL'..." && \
	CLUSTER_NAME="dv-$$CLUSTER_LABEL" && \
	echo "‚ö†Ô∏è  This will permanently delete cluster: $$CLUSTER_NAME" && \
	if [ "$$FORCE" != "yes" ]; then \
		read -p "Are you sure? Type 'yes' to continue: " CONFIRM </dev/tty && \
		if [ "$$CONFIRM" != "yes" ]; then \
			echo "‚ùå Operation cancelled"; \
			exit 1; \
		fi; \
	fi && \
	if $(MAKE) cluster-check CLUSTER_LABEL=$$CLUSTER_LABEL 2>/dev/null; then \
		echo "üóëÔ∏è  Deleting cluster $$CLUSTER_NAME (this may take several minutes)..." && \
		$(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster delete $$CLUSTER_NAME --force" && \
		echo "üßπ Cleaning up sealed secrets from repository..." && \
		$(MAKE) _cleanup-sealed-secrets CLUSTER_LABEL=$$CLUSTER_LABEL && \
		echo "‚úÖ Step d Complete: Cluster $$CLUSTER_NAME deleted successfully"; \
	else \
		echo "‚ÑπÔ∏è  Step d Complete: Cluster $$CLUSTER_NAME does not exist - nothing to destroy"; \
	fi

cluster-context: ## Step e: Setup kubectl context (usage: make cluster-context CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîß Step e: Setting up kubectl context for '$$CLUSTER_LABEL'..." && \
	CLUSTER_NAME="dv-$$CLUSTER_LABEL" && \
	$(MAKE) cluster-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "üîç Attempting to save kubectl configuration..." && \
	$(MAKE) _doctl-exec DOCTL_CMD="kubernetes cluster kubeconfig save $$CLUSTER_NAME" && \
	echo "‚úÖ kubectl configuration saved successfully" && \
	if kubectl config get-contexts -o name | grep -q "^do-nyc2-dv-$$CLUSTER_LABEL$$"; then \
		echo "‚ÑπÔ∏è  Context do-nyc2-dv-$$CLUSTER_LABEL already exists"; \
	else \
		kubectl config rename-context do-nyc2-$$CLUSTER_NAME do-nyc2-dv-$$CLUSTER_LABEL; \
	fi && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "üîç Verifying cluster access..." && \
	kubectl cluster-info && \
	kubectl get nodes && \
	echo "‚úÖ Step e Complete: kubectl context configured for $$CLUSTER_NAME"

cluster-context-destroy: ## Step f: Remove kubectl context (usage: make cluster-context-destroy CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üßπ Step f: Cleaning up kubectl context for '$$CLUSTER_LABEL'..." && \
	kubectl config delete-context do-nyc2-dv-$$CLUSTER_LABEL 2>/dev/null || true && \
	echo "‚úÖ Step f Complete: kubectl context destroyed for $$CLUSTER_LABEL"

tunnel-auth: ## Step g: Authenticate with Cloudflare (usage: make tunnel-auth)
	@echo "üîç Step g: Setting up Cloudflare authentication..." && \
	if [ ! -f .env ]; then \
		echo "‚ùå .env file not found. Please copy .env.example to .env and configure your tokens" && \
		exit 1; \
	fi && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	ZONE_ID=$$(sed -n 's/^CLOUDFLARE_ZONE_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	if [ -z "$$CLOUDFLARE_API_TOKEN" ] || [ -z "$$CLOUDFLARE_ACCOUNT_ID" ] || [ -z "$$ZONE_ID" ]; then \
		echo "‚ùå Missing Cloudflare credentials in .env file" && \
		echo "Required: CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, CLOUDFLARE_ZONE_ID" && \
		exit 1; \
	fi && \
	echo "üîê Authenticating with Cloudflare API..." && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	echo "üîç Verifying Cloudflare authentication..." && \
	if curl -s -X GET "https://api.cloudflare.com/client/v4/user/tokens/verify" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" | jq -e '.success == true' >/dev/null 2>&1; then \
		echo "‚úÖ Step g Complete: Cloudflare API authentication verified"; \
	else \
		echo "‚ùå Cloudflare API authentication failed. Please check your CLOUDFLARE_API_TOKEN in .env" && \
		exit 1; \
	fi

tunnel-check: ## Step h: Check if tunnel exists (usage: make tunnel-check CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Step h: Checking if tunnel exists..." && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	$(MAKE) tunnel-auth && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	echo "üîß Fetching tunnels via API..." && \
	TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json") && \
	if TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && [ -n "$$TUNNEL_ID" ] && [ "$$TUNNEL_ID" != "null" ]; then \
		echo "‚úÖ Step h Complete: Tunnel $$TUNNEL_NAME exists with ID: $$TUNNEL_ID"; \
	else \
		echo "‚ÑπÔ∏è  Step h Complete: Tunnel $$TUNNEL_NAME does not exist"; \
	fi

tunnel-create: ## Step i: Create tunnel (usage: make tunnel-create CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step i: Creating Cloudflare tunnel for '$$CLUSTER_LABEL'..." && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	$(MAKE) tunnel-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json") && \
	if TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && [ -n "$$TUNNEL_ID" ] && [ "$$TUNNEL_ID" != "null" ]; then \
		echo "‚úÖ Step i Complete: Tunnel $$TUNNEL_NAME already exists with ID: $$TUNNEL_ID"; \
	else \
		echo "üèóÔ∏è  Creating tunnel $$TUNNEL_NAME via API..." && \
		TUNNEL_CREATE_RESPONSE=$$(curl -s -X POST "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
			-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
			-H "Content-Type: application/json" \
			--data "{\"name\":\"$$TUNNEL_NAME\",\"tunnel_secret\":\"$$(openssl rand -base64 32)\"}") && \
		echo "üìÑ Tunnel creation response: $$TUNNEL_CREATE_RESPONSE" && \
		TUNNEL_ID=$$(echo "$$TUNNEL_CREATE_RESPONSE" | jq -r '.result.id' 2>/dev/null) && \
		if [ -n "$$TUNNEL_ID" ] && [ "$$TUNNEL_ID" != "null" ]; then \
			echo "‚úÖ Step i Complete: Tunnel created with ID: $$TUNNEL_ID"; \
		else \
			echo "‚ùå Failed to create tunnel" && \
			exit 1; \
		fi; \
	fi

tunnel-destroy: ## Step j: Destroy tunnel (usage: make tunnel-destroy CLUSTER_LABEL=dev [FORCE=yes])
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	FORCE=$${FORCE:-no} && \
	echo "üö® Step j: DESTRUCTIVE - Destroying Cloudflare tunnel for '$$CLUSTER_LABEL'..." && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	echo "‚ö†Ô∏è  This will permanently delete tunnel: $$TUNNEL_NAME" && \
	if [ "$$FORCE" != "yes" ]; then \
		read -p "Are you sure? Type 'yes' to continue: " CONFIRM </dev/tty && \
		if [ "$$CONFIRM" != "yes" ]; then \
			echo "‚ùå Operation cancelled"; \
			exit 1; \
		fi; \
	fi && \
	$(MAKE) tunnel-auth && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	echo "üîç Checking if tunnel exists..." && \
	TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json") && \
	if TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && [ -n "$$TUNNEL_ID" ] && [ "$$TUNNEL_ID" != "null" ]; then \
		echo "üóëÔ∏è  Deleting tunnel: $$TUNNEL_NAME (ID: $$TUNNEL_ID)" && \
		TUNNEL_DELETE_RESPONSE=$$(curl -s -X DELETE "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel/$$TUNNEL_ID" \
			-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN") && \
		if echo "$$TUNNEL_DELETE_RESPONSE" | jq -e '.success == true' >/dev/null 2>&1; then \
			echo "‚úÖ Step j Complete: Tunnel $$TUNNEL_NAME ($$TUNNEL_ID) deleted successfully"; \
		else \
			echo "‚ö†Ô∏è  Warning: Tunnel deletion response indicates issues"; \
			echo "üìÑ Response: $$TUNNEL_DELETE_RESPONSE"; \
		fi; \
	else \
		echo "‚ÑπÔ∏è  Step j Complete: Tunnel $$TUNNEL_NAME does not exist or is already deleted"; \
	fi

tunnel-dns: ## Step k: Setup tunnel DNS and public hostnames (usage: make tunnel-dns CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üåê Step k: Creating DNS records and public hostnames for '$$CLUSTER_LABEL'..." && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	$(MAKE) tunnel-check CLUSTER_LABEL=$$CLUSTER_LABEL && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	ZONE_ID=$$(sed -n 's/^CLOUDFLARE_ZONE_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json") && \
	TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && \
	if [ -z "$$TUNNEL_ID" ] || [ "$$TUNNEL_ID" = "null" ]; then \
		echo "‚ùå Tunnel $$TUNNEL_NAME does not exist. Run 'make tunnel-create' first." && \
		exit 1; \
	fi && \
	echo "üåê Creating DNS records..." && \
	for SUBDOMAIN in ui api argocd; do \
		HOSTNAME="$$CLUSTER_LABEL$$SUBDOMAIN.diocesanvitality.org" && \
		TARGET="$$TUNNEL_ID.cfargotunnel.com" && \
		echo "üîç Creating DNS record: $$HOSTNAME -> $$TARGET" && \
		DNS_CHECK_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones/$$ZONE_ID/dns_records?name=$$HOSTNAME" \
			-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN") && \
		echo "üìÑ DNS check response: $$DNS_CHECK_RESPONSE" && \
		EXISTING=$$(echo "$$DNS_CHECK_RESPONSE" | jq -r '.result[0].id // "null"') && \
		if [ "$$EXISTING" != "null" ]; then \
			echo "üîÑ Updating existing DNS record: $$HOSTNAME (ID: $$EXISTING)" && \
			DNS_UPDATE_RESPONSE=$$(curl -s -X PUT "https://api.cloudflare.com/client/v4/zones/$$ZONE_ID/dns_records/$$EXISTING" \
				-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
				-H "Content-Type: application/json" \
				--data "{\"type\":\"CNAME\",\"name\":\"$$HOSTNAME\",\"content\":\"$$TARGET\",\"proxied\":true}") && \
			echo "üìÑ DNS update response: $$DNS_UPDATE_RESPONSE"; \
		else \
			echo "üÜï Creating new DNS record: $$HOSTNAME" && \
			DNS_CREATE_RESPONSE=$$(curl -s -X POST "https://api.cloudflare.com/client/v4/zones/$$ZONE_ID/dns_records" \
				-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
				-H "Content-Type: application/json" \
				--data "{\"type\":\"CNAME\",\"name\":\"$$HOSTNAME\",\"content\":\"$$TARGET\",\"proxied\":true}") && \
			echo "üìÑ DNS create response: $$DNS_CREATE_RESPONSE"; \
		fi && \
		echo "‚úÖ DNS record configured: $$HOSTNAME"; \
	done && \
	echo "üîß Configuring tunnel public hostnames for SSL certificate generation..." && \
	TUNNEL_CONFIG_RESPONSE=$$(curl -s -X PUT "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel/$$TUNNEL_ID/configurations" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json" \
		--data '{ \
			"config": { \
				"ingress": [ \
					{ \
						"hostname": "'"$$CLUSTER_LABEL"'ui.diocesanvitality.org", \
						"service": "http://frontend-service.diocesan-vitality-'"$$CLUSTER_LABEL"'.svc.cluster.local:80" \
					}, \
					{ \
						"hostname": "'"$$CLUSTER_LABEL"'api.diocesanvitality.org", \
						"service": "http://backend-service.diocesan-vitality-'"$$CLUSTER_LABEL"'.svc.cluster.local:8000" \
					}, \
					{ \
						"hostname": "'"$$CLUSTER_LABEL"'argocd.diocesanvitality.org", \
						"service": "http://argocd-server.argocd:80" \
					}, \
					{ \
						"service": "http_status:404" \
					} \
				] \
			} \
		}') && \
	echo "üìÑ Tunnel configuration response: $$TUNNEL_CONFIG_RESPONSE" && \
	if echo "$$TUNNEL_CONFIG_RESPONSE" | jq -e '.success == true' >/dev/null 2>&1; then \
		echo "‚úÖ Tunnel public hostnames configured - SSL certificates will be automatically generated"; \
	else \
		echo "‚ö†Ô∏è  Tunnel configuration may have issues, but continuing..."; \
	fi && \
	echo "üìã Tunnel Information:" && \
	echo "   Tunnel Name: $$TUNNEL_NAME" && \
	echo "   Tunnel ID: $$TUNNEL_ID" && \
	echo "   Public Hostnames:" && \
	echo "     - $${CLUSTER_LABEL}ui.diocesanvitality.org (‚Üí frontend-service.diocesan-vitality-$$CLUSTER_LABEL.svc.cluster.local:80)" && \
	echo "     - $${CLUSTER_LABEL}api.diocesanvitality.org (‚Üí backend-service.diocesan-vitality-$$CLUSTER_LABEL.svc.cluster.local:8000)" && \
	echo "     - $${CLUSTER_LABEL}argocd.diocesanvitality.org (‚Üí argocd-server.argocd:80)" && \
	echo "‚úÖ Step 8 Complete: Tunnel DNS records and SSL certificates configured"

tunnel-dns-destroy: ## Step 8b: Remove tunnel DNS records (usage: make tunnel-dns-destroy CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üóëÔ∏è  Step 8b: Removing tunnel DNS records for '$$CLUSTER_LABEL'..." && \
	$(MAKE) tunnel-auth && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	ZONE_ID=$$(sed -n 's/^CLOUDFLARE_ZONE_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	DELETED_COUNT=0 && \
	SKIPPED_COUNT=0 && \
	for SUBDOMAIN in ui api argocd; do \
		HOSTNAME="$$CLUSTER_LABEL.$$SUBDOMAIN.diocesanvitality.org" && \
		echo "  üîç Checking: $$HOSTNAME..." && \
		DNS_CHECK_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones/$$ZONE_ID/dns_records?name=$$HOSTNAME" \
			-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN") && \
		EXISTING=$$(echo "$$DNS_CHECK_RESPONSE" | jq -r '.result[0].id // "null"') && \
		if [ "$$EXISTING" != "null" ]; then \
			echo "  üóëÔ∏è  Deleting: $$HOSTNAME (ID: $$EXISTING)" && \
			DNS_DELETE_RESPONSE=$$(curl -s -X DELETE "https://api.cloudflare.com/client/v4/zones/$$ZONE_ID/dns_records/$$EXISTING" \
				-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN") && \
			if echo "$$DNS_DELETE_RESPONSE" | jq -e '.success == true' >/dev/null 2>&1; then \
				echo "  ‚úÖ Deleted: $$HOSTNAME" && \
				DELETED_COUNT=$$((DELETED_COUNT + 1)); \
			else \
				echo "  ‚ö†Ô∏è  Warning: Deletion may have failed for $$HOSTNAME"; \
			fi; \
		else \
			echo "  ‚ÑπÔ∏è  Not found: $$HOSTNAME" && \
			SKIPPED_COUNT=$$((SKIPPED_COUNT + 1)); \
		fi; \
	done && \
	echo "" && \
	echo "üìä DNS Destruction Summary:" && \
	echo "   Deleted: $$DELETED_COUNT records" && \
	echo "   Skipped: $$SKIPPED_COUNT records (already deleted)" && \
	echo "‚úÖ Step 8b Complete: DNS records destroyed for $$CLUSTER_LABEL"


argocd-install: ## Step 9: Install ArgoCD via Helm (usage: make argocd-install CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step 9: Installing ArgoCD via Helm for '$$CLUSTER_LABEL'..." && \
	echo "üîß Switching to cluster context... ($$(date '+%H:%M:%S'))" && \
	if ! kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL; then \
		echo "‚ùå FAILED: Could not switch to kubectl context do-nyc2-dv-$$CLUSTER_LABEL at $$(date '+%H:%M:%S')" && \
		echo "üí° Check if cluster exists: doctl kubernetes cluster list" && \
		exit 1; \
	fi && \
		echo "üîß Installing Helm if needed... ($$(date '+%H:%M:%S'))" && \
		if ! $(MAKE) _install-helm; then \
			echo "‚ùå FAILED: Helm installation failed at $$(date '+%H:%M:%S')" && \
			exit 1; \
		fi && \
		echo "üì¶ Adding ArgoCD Helm repository... ($$(date '+%H:%M:%S'))" && \
		if ! helm repo add argo https://argoproj.github.io/argo-helm; then \
			echo "‚ùå FAILED: Could not add ArgoCD Helm repository at $$(date '+%H:%M:%S')" && \
			exit 1; \
		fi && \
		if ! helm repo update; then \
			echo "‚ùå FAILED: Helm repo update failed at $$(date '+%H:%M:%S')" && \
			exit 1; \
		fi && \
		echo "üèóÔ∏è  Creating ArgoCD namespace... ($$(date '+%H:%M:%S'))" && \
		if ! kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -; then \
			echo "‚ùå FAILED: Could not create argocd namespace at $$(date '+%H:%M:%S')" && \
			exit 1; \
		fi && \
		echo "üöÄ Installing ArgoCD with values-$$CLUSTER_LABEL.yaml... ($$(date '+%H:%M:%S'))" && \
		if ! helm upgrade --install argocd argo/argo-cd \
			--namespace argocd \
			--values k8s/infrastructure/argocd/values-$$CLUSTER_LABEL.yaml \
			--wait --timeout=10m; then \
			echo "‚ùå FAILED: ArgoCD Helm installation failed at $$(date '+%H:%M:%S')" && \
			echo "üí° Check Helm values file: k8s/infrastructure/argocd/values-$$CLUSTER_LABEL.yaml" && \
			exit 1; \
		fi && \
		echo "‚è≥ Waiting for ArgoCD server to be ready... ($$(date '+%H:%M:%S'))" && \
		if ! kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s; then \
			echo "‚ùå FAILED: ArgoCD server pods not ready within 5 minutes at $$(date '+%H:%M:%S')" && \
			echo "üí° Check pod status: kubectl get pods -n argocd" && \
			exit 1; \
		fi && \
		echo "üîß Configuring repository access... ($$(date '+%H:%M:%S'))" && \
		TIMEOUT=60 && START_TIME=$$(date +%s) && \
		while ! kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do \
			CURRENT_TIME=$$(date +%s) && \
			if [ $$((CURRENT_TIME - START_TIME)) -gt $$TIMEOUT ]; then \
				echo "‚ùå FAILED: ArgoCD configmap not available within 60 seconds at $$(date '+%H:%M:%S')" && \
				exit 1; \
			fi && \
			sleep 2; \
		done && \
		sleep 5 && \
		if ! kubectl patch configmap argocd-cm -n argocd --patch '{"data":{"repositories":"- url: https://github.com/tomknightatl/diocesan-vitality.git"}}'; then \
			echo "‚ùå FAILED: Could not configure repository access at $$(date '+%H:%M:%S')" && \
			exit 1; \
		fi
	@echo "üîß Setting up custom ArgoCD password..."
	@$(MAKE) _setup-argocd-password CLUSTER_LABEL=$$CLUSTER_LABEL
	@echo "üè∑Ô∏è  Registering cluster with ArgoCD..."
	@$(MAKE) _register-cluster-with-argocd CLUSTER_LABEL=$$CLUSTER_LABEL
	@echo "üöÄ Deploying root ApplicationSets..."
	@kubectl apply -f k8s/argocd/root-applicationsets-$$CLUSTER_LABEL.yaml
	@echo "‚úÖ Root ApplicationSets deployed - ArgoCD will now manage infrastructure"
	@echo "‚úÖ Step 9 Complete: ArgoCD installed via Helm for $$CLUSTER_LABEL"

_register-cluster-with-argocd: ## Register current cluster with ArgoCD with proper labels
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîó Registering cluster 'dv-$$CLUSTER_LABEL' with ArgoCD..." && \
	if ! kubectl get secret cluster-info -n default >/dev/null 2>&1; then \
		echo "üè∑Ô∏è  Creating missing cluster-info secret..."; \
		kubectl create secret generic cluster-info \
			--from-literal=environment=$$CLUSTER_LABEL \
			--from-literal=cluster-name=dv-$$CLUSTER_LABEL \
			-n default; \
	fi && \
	kubectl create secret generic argocd-cluster-local -n argocd \
		--from-literal=name=in-cluster \
		--from-literal=server=https://kubernetes.default.svc \
		--from-literal=config='{"tlsClientConfig":{"insecure":true}}' \
		--dry-run=client -o yaml | kubectl apply -f - && \
	kubectl label secret argocd-cluster-local -n argocd \
		argocd.argoproj.io/secret-type=cluster \
		environment=$$CLUSTER_LABEL \
		--overwrite && \
	echo "‚úÖ Cluster registration completed"

_setup-argocd-password: ## Setup custom ArgoCD password from .env using kubectl
	@echo "üîë Configuring custom ArgoCD password..."
	@INITIAL_PASSWORD=$$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d) && \
	CUSTOM_PASSWORD=$$(grep ARGOCD_ADMIN_PASSWORD_DEV .env 2>/dev/null | cut -d'=' -f2 || echo "") && \
	if [ -z "$$CUSTOM_PASSWORD" ]; then \
		echo "‚ö†Ô∏è  ARGOCD_ADMIN_PASSWORD_DEV not found in .env, using initial password"; \
		echo "$$INITIAL_PASSWORD" > .argocd-admin-password; \
		echo "   Initial password saved to: .argocd-admin-password"; \
	else \
		echo "üîÑ Setting custom password from .env using kubectl..."; \
		if ! python3 -c "import bcrypt" >/dev/null 2>&1; then \
			echo "üì¶ Installing bcrypt for password hashing..."; \
			pip3 install bcrypt --break-system-packages >/dev/null 2>&1 || pip3 install bcrypt >/dev/null 2>&1; \
		fi && \
		BCRYPT_HASH=$$(python3 -c "import bcrypt; print(bcrypt.hashpw(b'$$CUSTOM_PASSWORD', bcrypt.gensalt()).decode('utf-8'))") && \
		kubectl patch secret argocd-secret -n argocd --type='merge' -p="{\"data\":{\"admin.password\":\"$$(echo -n "$$BCRYPT_HASH" | base64 -w0)\"}}" && \
		kubectl delete secret argocd-initial-admin-secret -n argocd --ignore-not-found=true && \
		echo "$$CUSTOM_PASSWORD" > .argocd-admin-password && \
		echo "‚úÖ Custom password configured and saved to: .argocd-admin-password"; \
	fi

_deploy-app-of-apps: ## Deploy App-of-Apps root Application for ApplicationSets
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Deploying root Application for ApplicationSets in '$$CLUSTER_LABEL' environment..." && \
	echo "‚è≥ Waiting for ArgoCD to be fully ready... ($$(date '+%H:%M:%S'))" && \
	sleep 10 && \
	if ! kubectl apply -f k8s/argocd/root-applicationsets-$$CLUSTER_LABEL.yaml; then \
		echo "‚ùå FAILED: Could not deploy root Application at $$(date '+%H:%M:%S')" && \
		echo "üí° Check file: k8s/argocd/root-applicationsets-$$CLUSTER_LABEL.yaml" && \
		exit 1; \
	fi && \
	echo "‚è≥ Waiting for root Application to be synced... ($$(date '+%H:%M:%S'))" && \
	TIMEOUT=300 && START_TIME=$$(date +%s) && \
	while ! kubectl get application root-applicationsets-$$CLUSTER_LABEL -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null | grep -q "Synced"; do \
		CURRENT_TIME=$$(date +%s) && \
		if [ $$((CURRENT_TIME - START_TIME)) -gt $$TIMEOUT ]; then \
			echo "‚ùå FAILED: Root Application not synced within 5 minutes at $$(date '+%H:%M:%S')" && \
			echo "üí° Check Application status: kubectl get application root-applicationsets-$$CLUSTER_LABEL -n argocd" && \
			exit 1; \
		fi && \
		echo "üîÑ Waiting for Application sync... ($$((CURRENT_TIME - START_TIME))s elapsed)" && \
		sleep 5; \
	done && \
	echo "‚úÖ Root Application deployed and synced successfully" && \
	echo "üîç ApplicationSets that will be deployed:" && \
	kubectl get applicationsets -n argocd --no-headers 2>/dev/null | grep "$$CLUSTER_LABEL" | awk '{print "  - " $$1}' || echo "  (ApplicationSets will appear shortly)" && \
	echo "üí° Monitor ApplicationSets: kubectl get applicationsets -n argocd"

sealed-secret: sealed-secrets-create ## Alias for sealed-secrets-create target

argocd-password: ## Get ArgoCD admin password

argocd-apps: ## Deploy ArgoCD App-of-Apps root Application (usage: make argocd-apps CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Deploying ArgoCD App-of-Apps root Application for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	$(MAKE) _deploy-app-of-apps CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "‚úÖ ArgoCD App-of-Apps deployment complete for $$CLUSTER_LABEL"

sealed-secrets-create: ## Step 4: Create sealed secrets for tunnel and application (usage: make sealed-secrets-create CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step 4: Creating sealed secrets for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "üîß Installing kubeseal CLI if needed..." && \
	$(MAKE) _install-kubeseal && \
	echo "‚è≥ Waiting for sealed-secrets controller to be ready..." && \
	kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sealed-secrets -n kube-system --timeout=300s && \
	echo "" && \
	echo "üîê PART 1: Creating tunnel token sealed secret..." && \
	$(MAKE) _create-tunnel-sealed-secret CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "" && \
	echo "üîê PART 2: Creating application secrets sealed secret..." && \
	$(MAKE) _create-application-sealed-secret CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "" && \
	echo "üíæ Committing all sealed secrets to repository..." && \
	$(MAKE) _commit-sealed-secrets CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "" && \
	echo "‚úÖ Step 4 Complete: All sealed secrets created for $$CLUSTER_LABEL"

tunnel-test: ## Step 5: Test Cloudflare tunnel health (usage: make tunnel-test CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Step 5: Testing Cloudflare tunnel health for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "" && \
	echo "üîç Part 1: Checking sealed secrets status..." && \
	TUNNEL_SECRET_STATUS=$$(kubectl get sealedsecret cloudflared-token -n cloudflare-tunnel-$$CLUSTER_LABEL -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "NotFound") && \
	APP_SECRET_STATUS=$$(kubectl get sealedsecret diocesan-vitality-secrets -n diocesan-vitality-$$CLUSTER_LABEL -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "NotFound") && \
	if [ "$$TUNNEL_SECRET_STATUS" = "NotFound" ]; then \
		echo "‚ùå Tunnel sealed secret not found in cloudflare-tunnel-$$CLUSTER_LABEL namespace"; \
		exit 1; \
	fi && \
	if [ "$$APP_SECRET_STATUS" = "NotFound" ]; then \
		echo "‚ùå Application sealed secret not found in diocesan-vitality-$$CLUSTER_LABEL namespace"; \
		exit 1; \
	fi && \
	echo "‚úÖ Sealed secrets exist in cluster" && \
	echo "" && \
	echo "üîç Part 2: Verifying secrets were decrypted..." && \
	if ! kubectl get secret cloudflared-token -n cloudflare-tunnel-$$CLUSTER_LABEL >/dev/null 2>&1; then \
		echo "‚ùå Tunnel secret not decrypted (Secret object not found)"; \
		echo "üí° Check sealed-secrets controller logs: kubectl logs -n kube-system deployment/sealed-secrets-controller"; \
		exit 1; \
	fi && \
	if ! kubectl get secret diocesan-vitality-secrets -n diocesan-vitality-$$CLUSTER_LABEL >/dev/null 2>&1; then \
		echo "‚ùå Application secrets not decrypted (Secret object not found)"; \
		echo "üí° Check sealed-secrets controller logs: kubectl logs -n kube-system deployment/sealed-secrets-controller"; \
		exit 1; \
	fi && \
	echo "‚úÖ Secrets successfully decrypted by sealed-secrets controller" && \
	echo "" && \
	echo "üîç Part 3: Checking tunnel pod status..." && \
	if ! kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared >/dev/null 2>&1; then \
		echo "‚ùå No tunnel pods found"; \
		echo "üí° Check ApplicationSet: kubectl get applicationset cloudflare-tunnel-$$CLUSTER_LABEL-applicationset -n argocd"; \
		exit 1; \
	fi && \
	TUNNEL_POD_STATUS=$$(kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared -o jsonpath='{.items[0].status.phase}' 2>/dev/null) && \
	TUNNEL_POD_READY=$$(kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared -o jsonpath='{.items[0].status.containerStatuses[0].ready}' 2>/dev/null) && \
	if [ "$$TUNNEL_POD_STATUS" != "Running" ] || [ "$$TUNNEL_POD_READY" != "true" ]; then \
		echo "‚ùå Tunnel pod not healthy (Status: $$TUNNEL_POD_STATUS, Ready: $$TUNNEL_POD_READY)"; \
		echo "üí° Check pod logs: kubectl logs -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared"; \
		exit 1; \
	fi && \
	echo "‚úÖ Tunnel pod is Running and Ready" && \
	echo "" && \
	echo "üîç Part 4: Verifying tunnel connectivity..." && \
	TUNNEL_LOGS=$$(kubectl logs -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared --tail=50 2>/dev/null) && \
	if echo "$$TUNNEL_LOGS" | grep -q "Connection.*registered"; then \
		echo "‚úÖ Tunnel successfully registered with Cloudflare edge"; \
	elif echo "$$TUNNEL_LOGS" | grep -q "error.*authentication\|error.*token"; then \
		echo "‚ùå Tunnel authentication error detected"; \
		echo "üí° Check tunnel token: make tunnel-verify CLUSTER_LABEL=$$CLUSTER_LABEL"; \
		exit 1; \
	else \
		echo "‚ö†Ô∏è  Unable to confirm tunnel registration (logs may be insufficient)"; \
	fi && \
	echo "" && \
	echo "‚úÖ Step 5 Complete: Tunnel health check passed for $$CLUSTER_LABEL" && \
	echo "" && \
	echo "üìä Tunnel Status Summary:" && \
	echo "   Tunnel Pod: $$(kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)" && \
	echo "   Status: $$TUNNEL_POD_STATUS" && \
	echo "   Ready: $$TUNNEL_POD_READY" && \
	echo "" && \
	echo "üåê Expected URLs:" && \
	echo "   Frontend: https://$${CLUSTER_LABEL}ui.diocesanvitality.org" && \
	echo "   Backend:  https://$${CLUSTER_LABEL}api.diocesanvitality.org" && \
	echo "   ArgoCD:   https://$${CLUSTER_LABEL}argocd.diocesanvitality.org"

_create-tunnel-sealed-secret: ## Create tunnel token sealed secret
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Loading tunnel token from environment file..." && \
	if [ ! -f ".tunnel-token-$$CLUSTER_LABEL" ]; then \
		echo "‚ùå Could not find tunnel token file: .tunnel-token-$$CLUSTER_LABEL"; \
		echo "üí° Ensure tunnel verification has been run: make tunnel-verify CLUSTER_LABEL=$$CLUSTER_LABEL"; \
		exit 1; \
	fi && \
	TUNNEL_TOKEN=$$(grep "TUNNEL_TOKEN_$$CLUSTER_LABEL" .tunnel-token-$$CLUSTER_LABEL | cut -d'=' -f2-) && \
	if [ -z "$$TUNNEL_TOKEN" ]; then \
		echo "‚ùå Could not extract tunnel token from environment file"; \
		echo "üí° Ensure tunnel verification has been run: make tunnel-verify CLUSTER_LABEL=$$CLUSTER_LABEL"; \
		exit 1; \
	fi && \
	echo "‚úÖ Loaded tunnel token from environment file" && \
	echo "üîç Extracting tunnel ID for logging..." && \
	TUNNEL_INFO=$$(echo "$$TUNNEL_TOKEN" | base64 -d) && \
	TUNNEL_ID=$$(echo "$$TUNNEL_INFO" | jq -r '.t // "unknown"') && \
	echo "‚úÖ Tunnel ID: $$TUNNEL_ID" && \
	echo "üîê Creating sealed secret from tunnel token..." && \
	echo -n "$$TUNNEL_TOKEN" | kubectl create secret generic cloudflared-token \
		--dry-run=client --from-file=tunnel-token=/dev/stdin \
		--namespace=cloudflare-tunnel-$$CLUSTER_LABEL -o yaml | \
	kubeseal --controller-namespace=kube-system --controller-name=sealed-secrets-controller \
		-o yaml --namespace=cloudflare-tunnel-$$CLUSTER_LABEL > \
		k8s/infrastructure/cloudflare-tunnel/environments/$$CLUSTER_LABEL/cloudflared-token-sealedsecret.yaml && \
	echo "üîß Updating kustomization to include sealed secret..." && \
	$(MAKE) _update-kustomization-for-sealed-secret CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "‚úÖ Tunnel sealed secret created: $$TUNNEL_ID"

_create-application-sealed-secret: ## Create application secrets sealed secret
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Loading application secrets from .env file..." && \
	if [ ! -f ".env" ]; then \
		echo "‚ùå .env file not found. Please copy .env.example to .env and configure your secrets"; \
		exit 1; \
	fi && \
	SUPABASE_URL_VAR="SUPABASE_URL" && \
	SUPABASE_KEY_VAR="SUPABASE_KEY" && \
	if [ "$$CLUSTER_LABEL" != "prd" ]; then \
		ENV_SUFFIX=$$(echo $$CLUSTER_LABEL | tr '[:lower:]' '[:upper:]') && \
		SUPABASE_URL_ENV=$$(grep "^SUPABASE_URL_$$ENV_SUFFIX=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"') && \
		SUPABASE_KEY_ENV=$$(grep "^SUPABASE_KEY_$$ENV_SUFFIX=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"') && \
		if [ -n "$$SUPABASE_URL_ENV" ] && [ -n "$$SUPABASE_KEY_ENV" ]; then \
			echo "‚ÑπÔ∏è  Using environment-specific database credentials (SUPABASE_URL_$$ENV_SUFFIX, SUPABASE_KEY_$$ENV_SUFFIX)"; \
			SUPABASE_URL="$$SUPABASE_URL_ENV" && \
			SUPABASE_KEY="$$SUPABASE_KEY_ENV"; \
		else \
			echo "‚ö†Ô∏è  No environment-specific credentials found, falling back to production database"; \
			echo "üí° To use isolated $$CLUSTER_LABEL database, run: make database-create CLUSTER_LABEL=$$CLUSTER_LABEL"; \
			SUPABASE_URL=$$(grep "^SUPABASE_URL=" .env | cut -d'=' -f2- | tr -d '"') && \
			SUPABASE_KEY=$$(grep "^SUPABASE_KEY=" .env | cut -d'=' -f2- | tr -d '"'); \
		fi; \
	else \
		SUPABASE_URL=$$(grep "^SUPABASE_URL=" .env | cut -d'=' -f2- | tr -d '"') && \
		SUPABASE_KEY=$$(grep "^SUPABASE_KEY=" .env | cut -d'=' -f2- | tr -d '"'); \
	fi && \
	GENAI_API_KEY=$$(grep "^GENAI_API_KEY=" .env | cut -d'=' -f2- | tr -d '"') && \
	SEARCH_API_KEY=$$(grep "^SEARCH_API_KEY=" .env | cut -d'=' -f2- | tr -d '"') && \
	SEARCH_CX=$$(grep "^SEARCH_CX=" .env | cut -d'=' -f2- | tr -d '"') && \
	if [ -z "$$SUPABASE_URL" ] || [ -z "$$SUPABASE_KEY" ] || [ -z "$$GENAI_API_KEY" ] || [ -z "$$SEARCH_API_KEY" ] || [ -z "$$SEARCH_CX" ]; then \
		echo "‚ùå Missing required secrets in .env file. Required:"; \
		echo "   SUPABASE_URL, SUPABASE_KEY (or SUPABASE_URL_$$ENV_SUFFIX, SUPABASE_KEY_$$ENV_SUFFIX)"; \
		echo "   GENAI_API_KEY, SEARCH_API_KEY, SEARCH_CX"; \
		exit 1; \
	fi && \
	echo "‚úÖ Loaded application secrets from .env file" && \
	echo "   Database: $$SUPABASE_URL" && \
	echo "üîê Creating sealed secret for application..." && \
	kubectl create secret generic diocesan-vitality-secrets \
		--from-literal=supabase-url="$$SUPABASE_URL" \
		--from-literal=supabase-key="$$SUPABASE_KEY" \
		--from-literal=genai-api-key="$$GENAI_API_KEY" \
		--from-literal=search-api-key="$$SEARCH_API_KEY" \
		--from-literal=search-cx="$$SEARCH_CX" \
		--namespace=diocesan-vitality-$$CLUSTER_LABEL \
		--dry-run=client -o yaml | \
	kubeseal --controller-namespace=kube-system --controller-name=sealed-secrets-controller \
		-o yaml --namespace=diocesan-vitality-$$CLUSTER_LABEL > \
		k8s/environments/$$CLUSTER_LABEL/diocesan-vitality-secrets-sealedsecret.yaml && \
	echo "üîß Adding sealed secret to kustomization..." && \
	if ! grep -q "diocesan-vitality-secrets-sealedsecret.yaml" k8s/environments/$$CLUSTER_LABEL/kustomization.yaml; then \
		sed -i '/- namespace.yaml/a\  - diocesan-vitality-secrets-sealedsecret.yaml' k8s/environments/$$CLUSTER_LABEL/kustomization.yaml; \
	fi && \
	echo "üíæ Committing application sealed secret to repository..." && \
	git add k8s/environments/$$CLUSTER_LABEL/ && \
	PRE_COMMIT_ALLOW_NO_CONFIG=1 git commit -m "Add application sealed secret for diocesan-vitality-$$CLUSTER_LABEL [skip ci]" -m "Contains encrypted supabase-url, supabase-key, genai-api-key, search-api-key, search-cx" && \
	git pull --rebase && \
	git push && \
	echo "‚úÖ Application sealed secret created and committed"

_commit-sealed-secrets: ## Commit all sealed secrets to repository
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üìù Staging all sealed secret files..." && \
	git add k8s/infrastructure/cloudflare-tunnel/environments/$$CLUSTER_LABEL/ || true && \
	git add k8s/environments/$$CLUSTER_LABEL/ && \
	echo "üíæ Committing sealed secrets to repository..." && \
	PRE_COMMIT_ALLOW_NO_CONFIG=1 git commit -m "üîê Add sealed secrets for $$CLUSTER_LABEL environment [skip ci]" \
		-m "‚úÖ Tunnel Secret:" \
		-m "- cloudflared-token: Encrypted tunnel token for Cloudflare tunnel" \
		-m "" \
		-m "‚úÖ Application Secrets:" \
		-m "- supabase-url: Database connection URL" \
		-m "- supabase-key: Database API key" \
		-m "- genai-api-key: Google Gemini AI API key" \
		-m "- search-api-key: Google Custom Search API key" \
		-m "- search-cx: Google Custom Search CX ID" \
		-m "" \
		-m "üîí All secrets encrypted with cluster-specific sealed-secrets key" \
		-m "üöÄ ArgoCD will auto-deploy when synced from GitOps repository" && \
	git pull --rebase && \
	git push && \
	echo "‚úÖ All sealed secrets committed and pushed to repository"

_cleanup-sealed-secrets: ## Delete sealed secrets from repository after cluster destroy
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üßπ Cleaning up sealed secrets for $$CLUSTER_LABEL environment..." && \
	TUNNEL_SECRET="k8s/infrastructure/cloudflare-tunnel/environments/$$CLUSTER_LABEL/cloudflared-token-sealedsecret.yaml" && \
	APP_SECRET="k8s/environments/$$CLUSTER_LABEL/diocesan-vitality-secrets-sealedsecret.yaml" && \
	if [ -f "$$TUNNEL_SECRET" ]; then \
		echo "üóëÔ∏è  Removing tunnel sealed secret: $$TUNNEL_SECRET" && \
		git rm "$$TUNNEL_SECRET" || rm -f "$$TUNNEL_SECRET"; \
	fi && \
	if [ -f "$$APP_SECRET" ]; then \
		echo "üóëÔ∏è  Removing application sealed secret: $$APP_SECRET" && \
		git rm "$$APP_SECRET" || rm -f "$$APP_SECRET"; \
	fi && \
	if git diff --cached --quiet; then \
		echo "‚ÑπÔ∏è  No sealed secrets to clean up"; \
	else \
		echo "üíæ Committing sealed secret cleanup..." && \
		PRE_COMMIT_ALLOW_NO_CONFIG=1 git commit -m "üßπ Remove sealed secrets for $$CLUSTER_LABEL after cluster destroy [skip ci]" \
			-m "These sealed secrets were encrypted with the old cluster's certificate" \
			-m "and cannot be decrypted by a new cluster's sealed-secrets controller." \
			-m "" \
			-m "Run 'make sealed-secrets-create CLUSTER_LABEL=$$CLUSTER_LABEL' to regenerate" && \
		git pull --rebase && \
		git push && \
		echo "‚úÖ Sealed secrets cleaned up and changes pushed"; \
	fi

# Database Management
# ===================

database-create: ## Copy production database to environment-specific database (usage: make database-create CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üóÑÔ∏è  Database copy for '$$CLUSTER_LABEL' environment..." && \
	echo "" && \
	if [ "$$CLUSTER_LABEL" = "prd" ]; then \
		echo "‚ÑπÔ∏è  Production is the source database - no copy needed"; \
		exit 0; \
	fi && \
	echo "üîç Checking environment-specific database credentials..." && \
	if [ ! -f ".env" ]; then \
		echo "‚ùå .env file not found"; \
		exit 1; \
	fi && \
	ENV_SUFFIX=$$(echo $$CLUSTER_LABEL | tr '[:lower:]' '[:upper:]') && \
	SUPABASE_URL_DEV=$$(grep "^SUPABASE_URL_$$ENV_SUFFIX=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"') && \
	SUPABASE_KEY_DEV=$$(grep "^SUPABASE_KEY_$$ENV_SUFFIX=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"') && \
	SUPABASE_DB_PASSWORD_DEV=$$(grep "^SUPABASE_DB_PASSWORD_$$ENV_SUFFIX=" .env 2>/dev/null | cut -d'=' -f2- | tr -d '"') && \
	SUPABASE_URL_PRD=$$(grep "^SUPABASE_URL=" .env | cut -d'=' -f2- | tr -d '"') && \
	SUPABASE_KEY_PRD=$$(grep "^SUPABASE_KEY=" .env | cut -d'=' -f2- | tr -d '"') && \
	SUPABASE_DB_PASSWORD_PRD=$$(grep "^SUPABASE_DB_PASSWORD=" .env | cut -d'=' -f2- | tr -d '"') && \
	if [ -z "$$SUPABASE_URL_DEV" ] || [ -z "$$SUPABASE_KEY_DEV" ]; then \
		echo "‚ùå Missing $$CLUSTER_LABEL database credentials in .env"; \
		echo "" && \
		echo "üìã Prerequisites:" && \
		echo "1. Manually create Supabase project: diocesan-vitality-$$CLUSTER_LABEL" && \
		echo "2. Add credentials to .env:" && \
		echo "   SUPABASE_URL_$$ENV_SUFFIX=<your-dev-url>" && \
		echo "   SUPABASE_KEY_$$ENV_SUFFIX=<your-dev-key>" && \
		echo "3. Run this command again" && \
		exit 1; \
	fi && \
	echo "‚úÖ Found $$CLUSTER_LABEL database credentials" && \
	echo "" && \
	echo "üìä Database Copy Operation:" && \
	echo "   Source: $$SUPABASE_URL_PRD" && \
	echo "   Target: $$SUPABASE_URL_DEV" && \
	echo "" && \
	echo "‚ö†Ô∏è  This will:" && \
	echo "   - Export schema and data from production" && \
	echo "   - Drop existing tables in $$CLUSTER_LABEL database" && \
	echo "   - Import production data into $$CLUSTER_LABEL database" && \
	echo "" && \
	if [ -z "$$SKIP_CONFIRM" ]; then \
		read -p "Continue? Type 'yes' to proceed: " CONFIRM </dev/tty && \
		if [ "$$CONFIRM" != "yes" ]; then \
			echo "‚ùå Operation cancelled"; \
			exit 1; \
		fi; \
	else \
		echo "‚ÑπÔ∏è  Auto-confirming (SKIP_CONFIRM=1)"; \
	fi && \
	echo "" && \
	echo "üîÑ Copying production database to $$CLUSTER_LABEL..." && \
	$(MAKE) _database-copy-internal SUPABASE_URL_SRC="$$SUPABASE_URL_PRD" SUPABASE_KEY_SRC="$$SUPABASE_KEY_PRD" SUPABASE_DB_PASSWORD_SRC="$$SUPABASE_DB_PASSWORD_PRD" SUPABASE_URL_DST="$$SUPABASE_URL_DEV" SUPABASE_KEY_DST="$$SUPABASE_KEY_DEV" SUPABASE_DB_PASSWORD_DST="$$SUPABASE_DB_PASSWORD_DEV" CLUSTER_LABEL=$$CLUSTER_LABEL && \
	echo "" && \
	echo "‚úÖ Database copy complete for $$CLUSTER_LABEL" && \
	echo "" && \
	echo "Next steps:" && \
	echo "   make sealed-secrets-create CLUSTER_LABEL=$$CLUSTER_LABEL"

_database-copy-internal: ## Internal target to perform database copy using pg_dump/pg_restore
	@echo "üîß Installing dependencies..." && \
	if ! command -v psql >/dev/null 2>&1; then \
		echo "üì¶ Installing postgresql-client..." && \
		sudo apt-get update -qq && sudo apt-get install -y postgresql-client; \
	fi && \
	echo "‚úÖ Dependencies ready" && \
	echo "" && \
	echo "üîç Extracting database connection details..." && \
	SRC_HOST=$$(echo $(SUPABASE_URL_SRC) | sed 's|https://||' | sed 's|http://||' | cut -d'/' -f1) && \
	SRC_PROJECT=$$(echo $$SRC_HOST | cut -d'.' -f1) && \
	DST_HOST=$$(echo $(SUPABASE_URL_DST) | sed 's|https://||' | sed 's|http://||' | cut -d'/' -f1) && \
	DST_PROJECT=$$(echo $$DST_HOST | cut -d'.' -f1) && \
	echo "   Source project: $$SRC_PROJECT" && \
	echo "   Target project: $$DST_PROJECT" && \
	echo "" && \
	if [ -z "$(SUPABASE_DB_PASSWORD_SRC)" ] || [ -z "$(SUPABASE_DB_PASSWORD_DST)" ]; then \
		echo "‚ö†Ô∏è  Missing database passwords in .env" && \
		echo "üí° Add SUPABASE_DB_PASSWORD and SUPABASE_DB_PASSWORD_DEV to .env" && \
		exit 1; \
	fi && \
	echo "‚ö†Ô∏è  Manual database copy required (IPv6 connectivity needed for direct access)" && \
	echo "" && \
	echo "üìã Option 1: Use Supabase Dashboard (Easiest - works on free tier)" && \
	echo "   1. Export: Open https://supabase.com/dashboard/project/$$SRC_PROJECT/editor" && \
	echo "      Run SQL: SELECT * FROM dioceses; -- Copy all table data" && \
	echo "   2. Import: Open https://supabase.com/dashboard/project/$$DST_PROJECT/editor" && \
	echo "      Paste and run the same queries" && \
	echo "" && \
	echo "üìã Option 2: Use pg_dump from machine with IPv6" && \
	echo "   From a machine with IPv6 connectivity, run:" && \
	echo "   PGPASSWORD='$(SUPABASE_DB_PASSWORD_SRC)' pg_dump -h db.$$SRC_PROJECT.supabase.co -U postgres -d postgres > backup.sql" && \
	echo "   PGPASSWORD='$(SUPABASE_DB_PASSWORD_DST)' psql -h db.$$DST_PROJECT.supabase.co -U postgres -d postgres < backup.sql" && \
	echo "" && \
	echo "üí° For now, dev can share production database (already configured)" && \
	echo "   Environment-specific credentials will be used when database is populated"

database-destroy: ## Display instructions to destroy database project (usage: make database-destroy CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üóëÔ∏è  Database cleanup for '$$CLUSTER_LABEL' environment..." && \
	echo "" && \
	if [ "$$CLUSTER_LABEL" = "prd" ]; then \
		echo "‚ö†Ô∏è  Cannot destroy production database - manual action required"; \
		echo "üí° Production database should never be automatically destroyed"; \
		exit 1; \
	fi && \
	echo "‚ö†Ô∏è  Manual steps to destroy $$CLUSTER_LABEL database:" && \
	echo "" && \
	echo "1Ô∏è‚É£  Delete Supabase project:" && \
	echo "   - Go to https://app.supabase.com/" && \
	echo "   - Select project: diocesan-vitality-$$CLUSTER_LABEL" && \
	echo "   - Settings ‚Üí General ‚Üí Delete Project" && \
	echo "" && \
	echo "2Ô∏è‚É£  Optionally remove credentials from .env (manual cleanup):" && \
	echo "   You may want to remove or comment out these lines in .env:" && \
	echo "   SUPABASE_URL_$$(echo $$CLUSTER_LABEL | tr '[:lower:]' '[:upper:]')=..." && \
	echo "   SUPABASE_KEY_$$(echo $$CLUSTER_LABEL | tr '[:lower:]' '[:upper:]')=..." && \
	echo "" && \
	echo "‚úÖ Database cleanup instructions displayed for $$CLUSTER_LABEL"

argocd-verify: ## Step 6: Verify ArgoCD server is accessible at its URL (usage: make argocd-verify CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step 6: Verifying ArgoCD server accessibility for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "üîç Verifying tunnel configuration and name..." && \
	EXPECTED_TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	echo "   Expected tunnel name: $$EXPECTED_TUNNEL_NAME" && \
	if kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL >/dev/null 2>&1; then \
		TUNNEL_POD=$$(kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL -l app=cloudflared -o name | head -1) && \
		if [ -n "$$TUNNEL_POD" ]; then \
			echo "üîç Checking tunnel logs for tunnel ID and connectivity..." && \
			TUNNEL_LOGS=$$(kubectl logs $$TUNNEL_POD -n cloudflare-tunnel-$$CLUSTER_LABEL --tail=10 2>/dev/null || echo "") && \
			if echo "$$TUNNEL_LOGS" | grep -q "Starting tunnel tunnelID="; then \
				TUNNEL_ID=$$(echo "$$TUNNEL_LOGS" | grep "Starting tunnel tunnelID=" | tail -1 | sed 's/.*tunnelID=\([a-f0-9\-]*\).*/\1/') && \
				echo "   Running tunnel ID: $$TUNNEL_ID" && \
				if echo "$$TUNNEL_LOGS" | grep -q "Registered tunnel connection"; then \
					CONNECTION_COUNT=$$(echo "$$TUNNEL_LOGS" | grep -c "Registered tunnel connection" || echo "0") && \
					echo "‚úÖ Tunnel is connected with $$CONNECTION_COUNT active connections" && \
					echo "‚úÖ Tunnel name verification: $$EXPECTED_TUNNEL_NAME"; \
				else \
					echo "‚ö†Ô∏è  Tunnel may not be fully connected yet"; \
				fi; \
			else \
				echo "‚ö†Ô∏è  Could not determine tunnel status from logs"; \
			fi; \
		else \
			echo "‚ö†Ô∏è  No cloudflared pod found in namespace cloudflare-tunnel-$$CLUSTER_LABEL"; \
		fi; \
	else \
		echo "‚ö†Ô∏è  Cloudflare tunnel namespace not found: cloudflare-tunnel-$$CLUSTER_LABEL"; \
	fi && \
	echo "üîç Checking ArgoCD server pod status..." && \
	if ! kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=60s; then \
		echo "‚ùå FAILED: ArgoCD server pod not ready at $$(date '+%H:%M:%S')" && \
		echo "üí° Check pods: kubectl get pods -n argocd" && \
		exit 1; \
	fi && \
	ARGOCD_URL="https://$$CLUSTER_LABEL.argocd.diocesanvitality.org" && \
	echo "üåê Testing ArgoCD URL: $$ARGOCD_URL" && \
	echo "üåê Testing ArgoCD login screen (30 second timeout)..." && \
	TIMEOUT=30 && START_TIME=$$(date +%s) && \
	ARGOCD_ACCESSIBLE=false && \
	while true; do \
		RESPONSE=$$(curl -k -s --connect-timeout 5 --max-time 10 "$$ARGOCD_URL" 2>/dev/null || echo "") && \
		if echo "$$RESPONSE" | grep -q "Argo CD" && echo "$$RESPONSE" | grep -q "html"; then \
			echo "‚úÖ ArgoCD login screen confirmed at $$ARGOCD_URL"; \
			ARGOCD_ACCESSIBLE=true && \
			break; \
		elif [ -n "$$RESPONSE" ]; then \
			echo "üîç URL responding but not ArgoCD login screen - checking content..." && \
			if echo "$$RESPONSE" | grep -qi "error\|not found\|503\|502\|500"; then \
				echo "‚ö†Ô∏è  Server error detected in response"; \
			else \
				echo "‚ö†Ô∏è  Unexpected response content (not ArgoCD login)"; \
			fi; \
		fi && \
		CURRENT_TIME=$$(date +%s) && \
		if [ $$((CURRENT_TIME - START_TIME)) -gt $$TIMEOUT ]; then \
			echo "‚ö†Ô∏è  ArgoCD login screen not accessible within 30 seconds" && \
			if [ -n "$$RESPONSE" ]; then \
				echo "üí° URL is responding but not showing ArgoCD login screen" && \
				echo "üí° Check tunnel configuration and ArgoCD server status"; \
			else \
				echo "üí° URL not responding - DNS/tunnel may need more time to propagate" && \
				echo "üí° Try accessing $$ARGOCD_URL manually in a few minutes"; \
			fi && \
			echo "üí° Check tunnel status: kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL" && \
			break; \
		fi && \
		echo "üîÑ Waiting for ArgoCD login screen... ($$((CURRENT_TIME - START_TIME))s elapsed)" && \
		sleep 5; \
	done && \
	if [ "$$ARGOCD_ACCESSIBLE" = "true" ]; then \
		echo "üîê ArgoCD login screen verified successfully"; \
	else \
		echo "‚ö†Ô∏è  ArgoCD login screen verification incomplete - manual check recommended"; \
	fi && \
	echo "üîë ArgoCD Login Information:" && \
	echo "   URL: $$ARGOCD_URL" && \
	echo "   Username: admin" && \
	if [ -f .argocd-admin-password ]; then \
		echo "   Password: $$(cat .argocd-admin-password)"; \
	else \
		echo "   Password: $$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)"; \
	fi && \
	echo "‚úÖ Step 6 Complete: ArgoCD server verified and accessible at $$ARGOCD_URL"

docker-build: ## Step 6.5: Build and push Docker images from appropriate branch (usage: make docker-build CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step 6.5: Building and pushing Docker images for '$$CLUSTER_LABEL'..." && \
	echo "üéØ Setting source branch for $$CLUSTER_LABEL environment..." && \
	if [ "$$CLUSTER_LABEL" = "dev" ]; then \
		SOURCE_BRANCH="develop"; \
	elif [ "$$CLUSTER_LABEL" = "stg" ]; then \
		SOURCE_BRANCH="staging"; \
	else \
		SOURCE_BRANCH="main"; \
	fi && \
	echo "üìã Docker build configuration:" && \
	echo "   Environment: $$CLUSTER_LABEL" && \
	echo "   Source branch: $$SOURCE_BRANCH" && \
	echo "   Registry: Docker Hub (tomatl/diocesan-vitality)" && \
	echo "üîç Checking current git branch..." && \
	CURRENT_BRANCH=$$(git branch --show-current) && \
	echo "   Current branch: $$CURRENT_BRANCH" && \
	if [ "$$CURRENT_BRANCH" != "$$SOURCE_BRANCH" ]; then \
		echo "üîÑ Switching to $$SOURCE_BRANCH branch..." && \
		git fetch origin && \
		git checkout $$SOURCE_BRANCH && \
		git pull origin $$SOURCE_BRANCH && \
		echo "‚úÖ Switched to $$SOURCE_BRANCH branch"; \
	else \
		echo "‚úÖ Already on $$SOURCE_BRANCH branch" && \
		git pull origin $$SOURCE_BRANCH; \
	fi && \
	echo "üè∑Ô∏è  Generating image tags..." && \
	TIMESTAMP=$$(date +%Y-%m-%d-%H-%M-%S) && \
	IMAGE_TAG="$$CLUSTER_LABEL-$$TIMESTAMP" && \
	echo "   Image tag: $$IMAGE_TAG" && \
	echo "üîß Checking Docker login..." && \
	if ! docker info >/dev/null 2>&1; then \
		echo "‚ùå FAILED: Docker daemon not running at $$(date '+%H:%M:%S')" && \
		echo "üí° Start Docker daemon and ensure you're logged in: docker login" && \
		exit 1; \
	fi && \
	echo "üèóÔ∏è  Building multi-platform images..." && \
	echo "üì¶ Building backend image..." && \
	if ! docker buildx build --platform linux/amd64,linux/arm64 \
		-f backend/Dockerfile \
		-t tomatl/diocesan-vitality:backend-$$IMAGE_TAG \
		-t tomatl/diocesan-vitality:backend-$$CLUSTER_LABEL-latest \
		--push backend/; then \
		echo "‚ùå FAILED: Backend image build failed at $$(date '+%H:%M:%S')" && \
		exit 1; \
	fi && \
	echo "üì¶ Building frontend image..." && \
	if ! docker buildx build --platform linux/amd64,linux/arm64 \
		-f frontend/Dockerfile \
		-t tomatl/diocesan-vitality:frontend-$$IMAGE_TAG \
		-t tomatl/diocesan-vitality:frontend-$$CLUSTER_LABEL-latest \
		--push frontend/; then \
		echo "‚ùå FAILED: Frontend image build failed at $$(date '+%H:%M:%S')" && \
		exit 1; \
	fi && \
	echo "üì¶ Building pipeline image..." && \
	if ! docker buildx build --platform linux/amd64,linux/arm64 \
		-f Dockerfile.pipeline \
		-t tomatl/diocesan-vitality:pipeline-$$IMAGE_TAG \
		-t tomatl/diocesan-vitality:pipeline-$$CLUSTER_LABEL-latest \
		--push .; then \
		echo "‚ùå FAILED: Pipeline image build failed at $$(date '+%H:%M:%S')" && \
		exit 1; \
	fi && \
	echo "üéØ Updating Kubernetes manifests with new image tags..." && \
	MANIFEST_PATH="k8s/environments/development" && \
	if [ "$$CLUSTER_LABEL" = "stg" ]; then MANIFEST_PATH="k8s/environments/staging"; \
	elif [ "$$CLUSTER_LABEL" = "prd" ]; then MANIFEST_PATH="k8s/environments/production"; fi && \
	echo "   Manifest path: $$MANIFEST_PATH" && \
	if [ -f "$$MANIFEST_PATH/kustomization.yaml" ]; then \
		echo "üîÑ Updating image tags in Kubernetes manifests..." && \
		if [ -f "$$MANIFEST_PATH/backend-deployment.yaml" ]; then \
			sed -i "s|image: tomatl/diocesan-vitality:backend-.*|image: tomatl/diocesan-vitality:backend-$$IMAGE_TAG|g" "$$MANIFEST_PATH/backend-deployment.yaml"; \
		fi && \
		if [ -f "$$MANIFEST_PATH/frontend-deployment.yaml" ]; then \
			sed -i "s|image: tomatl/diocesan-vitality:frontend-.*|image: tomatl/diocesan-vitality:frontend-$$IMAGE_TAG|g" "$$MANIFEST_PATH/frontend-deployment.yaml"; \
		fi && \
		if [ -f "$$MANIFEST_PATH/pipeline-deployment.yaml" ]; then \
			sed -i "s|image: tomatl/diocesan-vitality:pipeline-.*|image: tomatl/diocesan-vitality:pipeline-$$IMAGE_TAG|g" "$$MANIFEST_PATH/pipeline-deployment.yaml"; \
		fi && \
		echo "üíæ Committing image tag updates to git..." && \
		git add $$MANIFEST_PATH/ && \
		git commit -m "Update $$CLUSTER_LABEL environment images to $$IMAGE_TAG" && \
		git push origin $$SOURCE_BRANCH && \
		echo "‚úÖ Image tags updated and committed to $$SOURCE_BRANCH"; \
	else \
		echo "‚ö†Ô∏è  Kubernetes manifests not found at $$MANIFEST_PATH" && \
		echo "üí° Images built and pushed, but manifests need manual update"; \
	fi && \
	echo "üìä Built images:" && \
	echo "   Backend: tomatl/diocesan-vitality:backend-$$IMAGE_TAG" && \
	echo "   Frontend: tomatl/diocesan-vitality:frontend-$$IMAGE_TAG" && \
	echo "   Pipeline: tomatl/diocesan-vitality:pipeline-$$IMAGE_TAG" && \
	echo "üí° GitOps will automatically deploy these images when ArgoCD syncs" && \
	echo "‚úÖ Step 6.5 Complete: Docker images built and pushed from $$SOURCE_BRANCH branch"

app-deploy: ## Step 7: Verify diocesan-vitality application deployment via GitOps (usage: make app-deploy CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üöÄ Step 7: Verifying diocesan-vitality application deployment for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "üîç Checking that ArgoCD ApplicationSets are ready..." && \
	if ! kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=30s; then \
		echo "‚ùå FAILED: ArgoCD not ready at $$(date '+%H:%M:%S')" && \
		echo "üí° Run: make argocd-verify CLUSTER_LABEL=$$CLUSTER_LABEL" && \
		exit 1; \
	fi && \
	echo "üéØ GitOps configuration for $$CLUSTER_LABEL environment:" && \
	if [ "$$CLUSTER_LABEL" = "dev" ]; then \
		TARGET_BRANCH="develop"; \
		echo "   Source branch: $$TARGET_BRANCH (configured in ApplicationSet)"; \
	elif [ "$$CLUSTER_LABEL" = "stg" ]; then \
		TARGET_BRANCH="staging"; \
		echo "   Source branch: $$TARGET_BRANCH (configured in ApplicationSet)"; \
	else \
		TARGET_BRANCH="main"; \
		echo "   Source branch: $$TARGET_BRANCH (configured in ApplicationSet)"; \
	fi && \
	echo "   Application: diocesan-vitality-$$CLUSTER_LABEL" && \
	echo "   Namespace: diocesan-vitality-$$CLUSTER_LABEL" && \
	echo "üîÑ Checking current application status..." && \
	if ! kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd >/dev/null 2>&1; then \
		echo "‚ùå FAILED: diocesan-vitality-$$CLUSTER_LABEL application not found at $$(date '+%H:%M:%S')" && \
		echo "üí° Ensure ArgoCD ApplicationSets are deployed: kubectl get applicationsets -n argocd" && \
		echo "üí° Check App-of-Apps root: kubectl get application root-applicationsets-$$CLUSTER_LABEL -n argocd" && \
		exit 1; \
	fi && \
	CURRENT_STATUS=$$(kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown") && \
	CURRENT_HEALTH=$$(kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown") && \
	CURRENT_BRANCH=$$(kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o jsonpath='{.spec.source.targetRevision}' 2>/dev/null || echo "Unknown") && \
	echo "   Current sync status: $$CURRENT_STATUS" && \
	echo "   Current health: $$CURRENT_HEALTH" && \
	echo "   Current branch: $$CURRENT_BRANCH" && \
	if [ "$$CURRENT_BRANCH" != "$$TARGET_BRANCH" ]; then \
		echo "‚ö†Ô∏è  Application is configured for branch '$$CURRENT_BRANCH' but should be '$$TARGET_BRANCH'" && \
		echo "üí° Check ApplicationSet configuration: k8s/argocd/diocesan-vitality-$$CLUSTER_LABEL-applicationset.yaml"; \
	fi && \
	echo "‚è≥ Monitoring application sync status (up to 2 minutes)..." && \
	TIMEOUT=120 && START_TIME=$$(date +%s) && \
	while true; do \
		SYNC_STATUS=$$(kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown") && \
		HEALTH_STATUS=$$(kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown") && \
		if [ "$$SYNC_STATUS" = "Synced" ] && [ "$$HEALTH_STATUS" = "Healthy" ]; then \
			echo "‚úÖ Application successfully synced and healthy via GitOps"; \
			break; \
		elif [ "$$SYNC_STATUS" = "Synced" ] && [ "$$HEALTH_STATUS" != "Healthy" ]; then \
			echo "‚ö†Ô∏è  Application synced but not healthy: $$HEALTH_STATUS"; \
			echo "üí° This may be normal if container images are not yet available"; \
			break; \
		fi && \
		CURRENT_TIME=$$(date +%s) && \
		if [ $$((CURRENT_TIME - START_TIME)) -gt $$TIMEOUT ]; then \
			echo "‚è≥ Application still syncing after 2 minutes" && \
			echo "üí° Current status: Sync=$$SYNC_STATUS, Health=$$HEALTH_STATUS" && \
			echo "üí° GitOps deployments may take time for initial sync" && \
			break; \
		fi && \
		echo "üîÑ Waiting for GitOps sync... Sync=$$SYNC_STATUS, Health=$$HEALTH_STATUS ($$((CURRENT_TIME - START_TIME))s elapsed)" && \
		sleep 10; \
	done && \
	echo "üìä Final deployment status:" && \
	kubectl get application diocesan-vitality-$$CLUSTER_LABEL -n argocd -o custom-columns=NAME:.metadata.name,SYNC:.status.sync.status,HEALTH:.status.health.status,BRANCH:.spec.source.targetRevision,REVISION:.status.sync.revision && \
	echo "üåê Application URLs (when healthy):" && \
	echo "   Frontend: https://$$CLUSTER_LABEL.ui.diocesanvitality.org" && \
	echo "   Backend API: https://$$CLUSTER_LABEL.api.diocesanvitality.org" && \
	echo "üí° Monitor deployment: kubectl get pods -n diocesan-vitality-$$CLUSTER_LABEL" && \
	echo "üí° GitOps approach: Application configured via ApplicationSet to deploy from $$TARGET_BRANCH branch" && \
	echo "‚úÖ Step 7 Complete: diocesan-vitality application verified via GitOps"

_install-helm: ## Install Helm CLI if not present
	@if ! command -v helm >/dev/null 2>&1; then \
		echo "üì¶ Installing Helm CLI..."; \
		curl -fsSL -o /tmp/get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 && \
		chmod 700 /tmp/get_helm.sh && \
		/tmp/get_helm.sh && \
		rm -f /tmp/get_helm.sh && \
		echo "‚úÖ Helm CLI installed successfully"; \
	else \
		echo "‚úÖ Helm CLI already installed"; \
	fi

_install-kubeseal: ## Install kubeseal CLI if not present
	@if ! command -v kubeseal >/dev/null 2>&1; then \
		echo "üì¶ Installing kubeseal CLI..."; \
		KUBESEAL_VERSION=$$(curl -s "https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest" | grep -Po '"tag_name": "v\K[^"]*'); \
		ARCH=$$(uname -m); \
		case $$ARCH in \
			x86_64) ARCH="amd64" ;; \
			aarch64) ARCH="arm64" ;; \
			armv7l) ARCH="arm" ;; \
		esac && \
		wget -q "https://github.com/bitnami-labs/sealed-secrets/releases/latest/download/kubeseal-$${KUBESEAL_VERSION}-linux-$${ARCH}.tar.gz" -O /tmp/kubeseal.tar.gz && \
		tar -xzf /tmp/kubeseal.tar.gz -C /tmp && \
		sudo mv /tmp/kubeseal /usr/local/bin/ && \
		sudo chmod +x /usr/local/bin/kubeseal && \
		rm -f /tmp/kubeseal.tar.gz && \
		echo "‚úÖ kubeseal CLI installed successfully"; \
	else \
		echo "‚úÖ kubeseal CLI already installed"; \
	fi
	@echo "üîë ArgoCD Admin Password:"
	@if [ -f .argocd-admin-password ]; then \
		cat .argocd-admin-password && echo; \
	else \
		kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d && echo || echo "‚ùå ArgoCD not installed or password not found"; \
	fi

_update-kustomization-for-sealed-secret: ## Update kustomization.yaml to include sealed secret
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	KUSTOMIZATION_FILE="k8s/infrastructure/cloudflare-tunnel/environments/$$CLUSTER_LABEL/kustomization.yaml" && \
	SEALED_SECRET_FILE="cloudflared-token-sealedsecret.yaml" && \
	echo "üîß Checking kustomization file: $$KUSTOMIZATION_FILE" && \
	if ! grep -q "$$SEALED_SECRET_FILE" "$$KUSTOMIZATION_FILE"; then \
		echo "üìù Adding sealed secret to kustomization resources..." && \
		sed -i "/resources:/a\  - $$SEALED_SECRET_FILE" "$$KUSTOMIZATION_FILE"; \
	else \
		echo "‚úÖ Sealed secret already included in kustomization"; \
	fi

infra-status: ## Check infrastructure status
	@echo "üîç Infrastructure Status:"
	@echo "========================"
	@echo "Kubectl contexts:"
	@kubectl config get-contexts | grep -E "(CURRENT|do-nyc2)" || echo "  No dev contexts found"
	@echo ""
	@echo "ArgoCD status:"
	@kubectl get pods -n argocd 2>/dev/null | grep -E "(NAME|argocd-server)" || echo "  ArgoCD not installed"
	@echo ""
	@echo "Applications:"
	@kubectl get applications -n argocd 2>/dev/null || echo "  No applications found"
	@echo ""
	@echo "Tunnel status:"
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	if [ -f .env ]; then \
		CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
		CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
		if [ -n "$$CLOUDFLARE_API_TOKEN" ] && [ -n "$$CLOUDFLARE_ACCOUNT_ID" ]; then \
			TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
				-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
				-H "Content-Type: application/json" 2>/dev/null) && \
			TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && \
			if [ -n "$$TUNNEL_ID" ] && [ "$$TUNNEL_ID" != "null" ]; then \
				TUNNEL_STATUS=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .status" 2>/dev/null | head -1) && \
				echo "  Tunnel: $$TUNNEL_NAME ($$TUNNEL_ID) - Status: $$TUNNEL_STATUS" && \
				kubectl get pods -n cloudflare-tunnel-$$CLUSTER_LABEL 2>/dev/null | grep cloudflared | head -1 || echo "  No cloudflared pod found"; \
			else \
				echo "  No tunnel found: $$TUNNEL_NAME"; \
			fi; \
		else \
			echo "  Cloudflare credentials not configured"; \
		fi; \
	else \
		echo "  No .env file found"; \
	fi

# Removed old infra-destroy - replaced with new 8-step version above

argocd-destroy: ## Destroy ArgoCD (usage: make argocd-destroy CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üßπ Destroying ArgoCD for '$$CLUSTER_LABEL'..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	echo "üîß Removing finalizers from ArgoCD applications..." && \
	kubectl get applications -n argocd -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n argocd --type='merge' -p='{"metadata":{"finalizers":null}}' || true && \
	echo "üóëÔ∏è  Deleting ArgoCD namespace..." && \
	kubectl delete namespace argocd --ignore-not-found=true

# Removed old cluster-destroy - replaced with new Step 3b version above

tunnel-verify: ## Generate tunnel token file for sealed secrets (usage: make tunnel-verify CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üîç Generating tunnel token for '$$CLUSTER_LABEL'..." && \
	TUNNEL_NAME="do-nyc2-dv-$$CLUSTER_LABEL" && \
	$(MAKE) tunnel-auth && \
	CLOUDFLARE_API_TOKEN=$$(sed -n 's/^CLOUDFLARE_API_TOKEN=//p' .env | tr -d '\r\n"'"'"'') && \
	CLOUDFLARE_ACCOUNT_ID=$$(sed -n 's/^CLOUDFLARE_ACCOUNT_ID=//p' .env | tr -d '\r\n"'"'"'') && \
	export CLOUDFLARE_API_TOKEN="$$CLOUDFLARE_API_TOKEN" && \
	echo "üîß Fetching tunnel information..." && \
	TUNNELS_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN" \
		-H "Content-Type: application/json") && \
	TUNNEL_ID=$$(echo "$$TUNNELS_RESPONSE" | jq -r ".result[] | select(.name==\"$$TUNNEL_NAME\" and .deleted_at==null) | .id" 2>/dev/null | head -1) && \
	if [ -z "$$TUNNEL_ID" ] || [ "$$TUNNEL_ID" = "null" ]; then \
		echo "‚ùå Tunnel $$TUNNEL_NAME does not exist. Run 'make tunnel-create CLUSTER_LABEL=$$CLUSTER_LABEL' first." && \
		exit 1; \
	fi && \
	echo "üîç Found tunnel: $$TUNNEL_NAME ($$TUNNEL_ID)" && \
	echo "üîê Generating tunnel token..." && \
	TOKEN_RESPONSE=$$(curl -s -X GET "https://api.cloudflare.com/client/v4/accounts/$$CLOUDFLARE_ACCOUNT_ID/cfd_tunnel/$$TUNNEL_ID/token" \
		-H "Authorization: Bearer $$CLOUDFLARE_API_TOKEN") && \
	if echo "$$TOKEN_RESPONSE" | jq -e '.success == true' >/dev/null 2>&1; then \
		TUNNEL_TOKEN=$$(echo "$$TOKEN_RESPONSE" | jq -r '.result' 2>/dev/null) && \
		if [ -n "$$TUNNEL_TOKEN" ] && [ "$$TUNNEL_TOKEN" != "null" ]; then \
			echo "‚úÖ Tunnel token generated successfully" && \
			echo "üîç Adding base64 padding if needed..." && \
			TOKEN_LEN=$$(echo -n "$$TUNNEL_TOKEN" | wc -c) && \
			PADDING_NEEDED=$$((4 - TOKEN_LEN % 4)) && \
			if [ $$PADDING_NEEDED -ne 4 ]; then \
				PADDING=$$(printf '%*s' $$PADDING_NEEDED '' | tr ' ' '=') && \
				TUNNEL_TOKEN="$$TUNNEL_TOKEN$$PADDING"; \
			fi && \
			echo "üíæ Saving tunnel token to .tunnel-token-$$CLUSTER_LABEL..." && \
			echo "TUNNEL_TOKEN_$$CLUSTER_LABEL=$$TUNNEL_TOKEN" > .tunnel-token-$$CLUSTER_LABEL && \
			echo "‚úÖ Tunnel token saved to .tunnel-token-$$CLUSTER_LABEL" && \
			echo "üîç Token preview: $$(echo "$$TUNNEL_TOKEN" | cut -c1-20)..."; \
		else \
			echo "‚ùå Failed to extract tunnel token from API response" && \
			exit 1; \
		fi; \
	else \
		echo "‚ùå Failed to generate tunnel token. API response:" && \
		echo "$$TOKEN_RESPONSE" && \
		exit 1; \
	fi

infra-test: ## Step 6: Integration testing and cleanup (usage: make infra-test CLUSTER_LABEL=dev)
	@CLUSTER_LABEL=$${CLUSTER_LABEL:-dev} && \
	echo "üß™ Step 6: Running integration tests and cleanup for '$$CLUSTER_LABEL'..." && \
	echo "üîç Testing cluster connectivity..." && \
	kubectl config use-context do-nyc2-dv-$$CLUSTER_LABEL && \
	kubectl get nodes >/dev/null && \
	echo "‚úÖ Cluster connectivity test passed" && \
	echo "üîç Testing ArgoCD installation..." && \
	kubectl get pods -n argocd | grep -q "argocd-server.*Running" && \
	echo "‚úÖ ArgoCD installation test passed" && \
	echo "üîç Testing ApplicationSets..." && \
	kubectl get applicationsets -n argocd | grep -q "diocesan-vitality-$$CLUSTER_LABEL" && \
	echo "‚úÖ ApplicationSets test passed" && \
	echo "üîç Testing sealed-secrets controller..." && \
	kubectl get pods -n kube-system -l app.kubernetes.io/name=sealed-secrets | grep -q "Running" && \
	echo "‚úÖ Sealed-secrets controller test passed" && \
	echo "üßπ Cleaning up temporary files..." && \
	rm -f .tunnel-token-$$CLUSTER_LABEL && \
	echo "‚úÖ Temporary tunnel token file removed" && \
	echo "üîê Verifying no sensitive data in environment..." && \
	if env | grep -q "TUNNEL_TOKEN"; then \
		echo "‚ö†Ô∏è  Found tunnel tokens in environment - consider unsetting them"; \
	else \
		echo "‚úÖ No tunnel tokens found in environment"; \
	fi && \
	echo "üìä Final infrastructure status:" && \
	$(MAKE) infra-status CLUSTER_LABEL=$$CLUSTER_LABEL
	@echo "‚úÖ Step 6 Complete: Integration tests passed and cleanup completed"
