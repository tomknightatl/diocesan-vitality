{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Enhanced_Pattern_Based_Parish_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WjMkndZ9tA88",
        "outputId": "ab592261-4a70-4232-a5dd-276c2fdea54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supabase in /usr/local/lib/python3.11/dist-packages (2.15.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (0.6.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Requirement already satisfied: gotrue<3.0.0,>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from supabase) (2.12.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: postgrest<1.1,>0.19 in /usr/local/lib/python3.11/dist-packages (from supabase) (1.0.2)\n",
            "Requirement already satisfied: realtime<2.5.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from supabase) (2.4.3)\n",
            "Requirement already satisfied: storage3<0.12,>=0.10 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.11.3)\n",
            "Requirement already satisfied: supafunc<0.10,>=0.9 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json) (0.9.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.4)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Requirement already satisfied: pytest-mock<4.0.0,>=3.14.0 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (3.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from postgrest<1.1,>0.19->supabase) (2.1.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.18 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (3.12.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: websockets<15,>=11 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (14.2)\n",
            "Requirement already satisfied: strenum<0.5.0,>=0.4.15 in /usr/local/lib/python3.11/dist-packages (from supafunc<0.10,>=0.9->supabase) (0.4.15)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (1.1.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.20.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.5.0,>=2.4.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.6.0)\n",
            "âœ… All dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Install and Import Dependencies (FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "# Install additional dependencies for the enhanced system\n",
        "!pip install supabase dataclasses-json beautifulsoup4 selenium webdriver-manager tenacity\n",
        "\n",
        "# Complete imports including missing ones\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import subprocess  # FIXED: Added for Chrome installation\n",
        "import re  # FIXED: Added for pattern detection\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, asdict\n",
        "from enum import Enum\n",
        "from typing import List, Dict, Optional, Any\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# Your existing Supabase and AI imports\n",
        "from google.colab import userdata\n",
        "from supabase import create_client, Client\n",
        "\n",
        "print(\"âœ… All dependencies installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Configuration (Reuse your existing setup)\n",
        "# =============================================================================\n",
        "\n",
        "# Reuse your existing configuration logic\n",
        "print(\"=== ENHANCED PARISH EXTRACTOR CONFIGURATION ===\")\n",
        "\n",
        "# GitHub and database setup (copy from your existing notebooks)\n",
        "GITHUB_REPO = 'USCCB'\n",
        "GITHUB_USERNAME = userdata.get('GitHubUserforUSCCB')\n",
        "GITHUB_PAT = userdata.get('GitHubPATforUSCCB')\n",
        "\n",
        "# Supabase configuration (copy from your existing setup)\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "    print(\"âœ… Supabase client initialized\")\n",
        "else:\n",
        "    print(\"âŒ Supabase credentials not found\")\n",
        "    supabase = None\n",
        "\n",
        "# Processing configuration\n",
        "MAX_DIOCESES_TO_PROCESS = 1  # Start small for testing\n",
        "ENABLE_PATTERN_DETECTION = True\n",
        "SAVE_DETAILED_LOGS = True\n",
        "\n",
        "print(f\"ðŸ“Š Will process {MAX_DIOCESES_TO_PROCESS} dioceses with pattern detection\")"
      ],
      "metadata": {
        "id": "qr8OfoQNtGiM",
        "outputId": "24037f03-d8c6-437e-d151-95c3273b257b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENHANCED PARISH EXTRACTOR CONFIGURATION ===\n",
            "âœ… Supabase client initialized\n",
            "ðŸ“Š Will process 1 dioceses with pattern detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2.5: Chrome Installation for Google Colab (FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "import subprocess  # This was missing!\n",
        "import os\n",
        "\n",
        "def ensure_chrome_installed():\n",
        "    \"\"\"Ensures Chrome is installed in the Colab environment.\"\"\"\n",
        "    try:\n",
        "        # Check if Chrome is already available\n",
        "        result = subprocess.run(['which', 'google-chrome'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ… Chrome is already installed and available.\")\n",
        "            return True\n",
        "\n",
        "        print(\"ðŸ”§ Chrome not found. Installing Chrome for Selenium...\")\n",
        "\n",
        "        # Install Chrome\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - > /dev/null 2>&1')\n",
        "        os.system('echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list')\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('apt-get install -y google-chrome-stable > /dev/null 2>&1')\n",
        "\n",
        "        # Verify installation\n",
        "        result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"âœ… Chrome installed successfully: {result.stdout.strip()}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âŒ Chrome installation may have failed.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during Chrome installation: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the installation check\n",
        "print(\"ðŸ”§ Checking Chrome installation...\")\n",
        "chrome_ready = ensure_chrome_installed()\n",
        "\n",
        "if chrome_ready:\n",
        "    print(\"ðŸš€ Ready to proceed with Selenium operations!\")\n",
        "else:\n",
        "    print(\"âš ï¸ You may need to restart the runtime if Chrome installation failed.\")\n",
        "\n",
        "# Test Chrome installation\n",
        "if chrome_ready:\n",
        "    try:\n",
        "        result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True)\n",
        "        print(f\"ðŸ“‹ Chrome version: {result.stdout.strip()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Chrome test failed: {e}\")\n",
        "        chrome_ready = False\n",
        "\n",
        "print(f\"Final Chrome status: {'âœ… Ready' if chrome_ready else 'âŒ Not Ready'}\")"
      ],
      "metadata": {
        "id": "AlIbVQ4hvkR_",
        "outputId": "7a1d4723-c460-44ac-c577-653d340b8ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Checking Chrome installation...\n",
            "âœ… Chrome is already installed and available.\n",
            "ðŸš€ Ready to proceed with Selenium operations!\n",
            "ðŸ“‹ Chrome version: Google Chrome 137.0.7151.55\n",
            "Final Chrome status: âœ… Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2.6: Driver Setup Function (MISSING - NOW ADDED)\n",
        "# =============================================================================\n",
        "\n",
        "def setup_enhanced_driver():\n",
        "    \"\"\"Setup Chrome driver with enhanced options for parish extraction\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"ðŸ”§ Setting up Chrome driver...\")\n",
        "\n",
        "        # Chrome options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')  # Run in background\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        # Setup driver\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        # Additional setup to avoid detection\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "\n",
        "        # Set timeouts\n",
        "        driver.implicitly_wait(10)\n",
        "        driver.set_page_load_timeout(30)\n",
        "\n",
        "        print(\"âœ… Chrome driver setup complete\")\n",
        "        return driver\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to setup driver: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"âœ… Driver setup function added\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAU5AfQSLgjg",
        "outputId": "952e99ea-fee3-4cb2-ed58-868da14b34bf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Driver setup function added\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Enhanced Pattern Detection Classes (UPDATED for detailed extraction)\n",
        "# =============================================================================\n",
        "\n",
        "import re  # This was also missing!\n",
        "\n",
        "class DiocesePlatform(Enum):\n",
        "    SQUARESPACE = \"squarespace\"\n",
        "    WORDPRESS = \"wordpress\"\n",
        "    DRUPAL = \"drupal\"\n",
        "    CUSTOM_CMS = \"custom\"\n",
        "    STATIC_HTML = \"static\"\n",
        "    ECATHOLIC = \"ecatholic\"  # NEW: Added for sites like Tulsa\n",
        "    DIOCESAN_CUSTOM = \"diocesan_custom\"  # NEW: For custom diocese sites like Salt Lake City\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "class ParishListingType(Enum):\n",
        "    INTERACTIVE_MAP = \"interactive_map\"\n",
        "    STATIC_TABLE = \"static_table\"\n",
        "    CARD_GRID = \"card_grid\"\n",
        "    SIMPLE_LIST = \"simple_list\"\n",
        "    PAGINATED_LIST = \"paginated_list\"\n",
        "    SEARCHABLE_DIRECTORY = \"searchable_directory\"\n",
        "    PARISH_FINDER = \"parish_finder\"  # NEW: Added for interactive parish finders\n",
        "    DIOCESE_CARD_LAYOUT = \"diocese_card_layout\"  # NEW: For Salt Lake City style layouts\n",
        "    PDF_DIRECTORY = \"pdf_directory\"\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "@dataclass\n",
        "class ParishData:\n",
        "    name: str\n",
        "    address: Optional[str] = None\n",
        "    city: Optional[str] = None\n",
        "    state: Optional[str] = None\n",
        "    zip_code: Optional[str] = None\n",
        "    phone: Optional[str] = None\n",
        "    website: Optional[str] = None\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    pastor: Optional[str] = None\n",
        "    mass_times: Optional[str] = None\n",
        "    # NEW: Enhanced fields for detailed extraction\n",
        "    street_address: Optional[str] = None\n",
        "    full_address: Optional[str] = None\n",
        "    parish_detail_url: Optional[str] = None\n",
        "    clergy_info: Optional[str] = None\n",
        "    service_times: Optional[str] = None\n",
        "    # Metadata fields\n",
        "    confidence_score: float = 0.5\n",
        "    extraction_method: str = \"unknown\"\n",
        "    diocese_url: Optional[str] = None\n",
        "    parish_directory_url: Optional[str] = None\n",
        "    detail_extraction_success: bool = False\n",
        "    detail_extraction_error: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class DioceseSitePattern:\n",
        "    platform: DiocesePlatform\n",
        "    listing_type: ParishListingType\n",
        "    confidence_score: float\n",
        "    extraction_method: str\n",
        "    specific_selectors: Dict[str, str]\n",
        "    javascript_required: bool\n",
        "    pagination_pattern: Optional[str] = None\n",
        "    notes: str = \"\"\n",
        "\n",
        "class PatternDetector:\n",
        "    \"\"\"Detects patterns in diocese websites for targeted extraction\"\"\"\n",
        "\n",
        "    def detect_pattern(self, html_content: str, url: str) -> DioceseSitePattern:\n",
        "        \"\"\"Analyze website content and detect the best extraction pattern\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        html_lower = html_content.lower()\n",
        "\n",
        "        # Platform detection\n",
        "        platform = self._detect_platform(html_lower, url)\n",
        "\n",
        "        # Listing type detection\n",
        "        listing_type = self._detect_listing_type(html_lower, soup, url)\n",
        "\n",
        "        # JavaScript requirement\n",
        "        js_required = self._requires_javascript(html_lower)\n",
        "\n",
        "        # Determine extraction method and confidence\n",
        "        extraction_method, confidence, selectors, notes = self._determine_extraction_strategy(\n",
        "            platform, listing_type, soup, html_lower, url\n",
        "        )\n",
        "\n",
        "        return DioceseSitePattern(\n",
        "            platform=platform,\n",
        "            listing_type=listing_type,\n",
        "            confidence_score=confidence,\n",
        "            extraction_method=extraction_method,\n",
        "            specific_selectors=selectors,\n",
        "            javascript_required=js_required,\n",
        "            notes=notes\n",
        "        )\n",
        "\n",
        "    def _detect_platform(self, html_lower: str, url: str) -> DiocesePlatform:\n",
        "        \"\"\"Detect CMS/platform\"\"\"\n",
        "        if 'ecatholic.com' in url or 'ecatholic' in html_lower:\n",
        "            return DiocesePlatform.ECATHOLIC\n",
        "        elif 'squarespace' in html_lower:\n",
        "            return DiocesePlatform.SQUARESPACE\n",
        "        elif 'wp-content' in html_lower or 'wordpress' in html_lower:\n",
        "            return DiocesePlatform.WORDPRESS\n",
        "        elif 'drupal' in html_lower:\n",
        "            return DiocesePlatform.DRUPAL\n",
        "        elif 'dioslc.org' in url or 'utahcatholicdiocese.org' in url:\n",
        "            return DiocesePlatform.DIOCESAN_CUSTOM\n",
        "        else:\n",
        "            return DiocesePlatform.CUSTOM_CMS\n",
        "\n",
        "    def _detect_listing_type(self, html_lower: str, soup: BeautifulSoup, url: str) -> ParishListingType:\n",
        "        \"\"\"Detect how parishes are listed\"\"\"\n",
        "\n",
        "        # Check for Salt Lake City style card layout\n",
        "        if ('col-lg location' in html_lower and 'card-title' in html_lower and\n",
        "            'dioslc.org' in url):\n",
        "            return ParishListingType.DIOCESE_CARD_LAYOUT\n",
        "\n",
        "        # Check for eCatholic parish finder pattern (like Tulsa)\n",
        "        if ('parishfinder' in url.lower() or 'parish-finder' in url.lower() or\n",
        "            'finderCore' in html_lower or 'finder.js' in html_lower or\n",
        "            'parish finder' in html_lower):\n",
        "            return ParishListingType.PARISH_FINDER\n",
        "\n",
        "        # Interactive map indicators\n",
        "        map_indicators = ['leaflet', 'google.maps', 'mapbox', 'parish-map', 'interactive']\n",
        "        if any(indicator in html_lower for indicator in map_indicators):\n",
        "            return ParishListingType.INTERACTIVE_MAP\n",
        "\n",
        "        # Table indicators\n",
        "        if soup.find('table') and ('parish' in html_lower or 'church' in html_lower):\n",
        "            return ParishListingType.STATIC_TABLE\n",
        "\n",
        "        # Card/grid layout (generic)\n",
        "        if soup.find_all(class_=re.compile(r'(card|grid|parish-item)', re.I)):\n",
        "            return ParishListingType.CARD_GRID\n",
        "\n",
        "        # Pagination\n",
        "        if any(word in html_lower for word in ['pagination', 'page-numbers', 'next-page']):\n",
        "            return ParishListingType.PAGINATED_LIST\n",
        "\n",
        "        return ParishListingType.SIMPLE_LIST\n",
        "\n",
        "    def _requires_javascript(self, html_lower: str) -> bool:\n",
        "        \"\"\"Check if JavaScript is required\"\"\"\n",
        "        js_indicators = ['react', 'angular', 'vue', 'leaflet', 'google.maps', 'ajax', 'finder.js']\n",
        "        return any(indicator in html_lower for indicator in js_indicators)\n",
        "\n",
        "    def _determine_extraction_strategy(self, platform, listing_type, soup, html_lower, url):\n",
        "        \"\"\"Determine the best extraction strategy\"\"\"\n",
        "\n",
        "        if listing_type == ParishListingType.DIOCESE_CARD_LAYOUT:\n",
        "            return (\n",
        "                \"diocese_card_extraction_with_details\",\n",
        "                0.95,\n",
        "                {\n",
        "                    \"parish_cards\": \".col-lg.location\",\n",
        "                    \"parish_name\": \".card-title\",\n",
        "                    \"parish_city\": \".card-body\",\n",
        "                    \"parish_link\": \"a.card\"\n",
        "                },\n",
        "                \"Diocese card layout detected - specialized extraction for Salt Lake City style with detail page navigation\"\n",
        "            )\n",
        "\n",
        "        elif listing_type == ParishListingType.PARISH_FINDER:\n",
        "            return (\n",
        "                \"parish_finder_extraction\",\n",
        "                0.95,\n",
        "                {\n",
        "                    \"parish_list\": \".site, li.site\",\n",
        "                    \"parish_name\": \".name\",\n",
        "                    \"parish_city\": \".city\",\n",
        "                    \"parish_info\": \".siteInfo\",\n",
        "                    \"parish_details\": \".details\"\n",
        "                },\n",
        "                \"Parish finder interface detected - specialized extraction for interactive directory\"\n",
        "            )\n",
        "\n",
        "        elif listing_type == ParishListingType.INTERACTIVE_MAP:\n",
        "            return (\n",
        "                \"interactive_map_extraction\",\n",
        "                0.9,\n",
        "                {\"map_container\": \"#map, .map-container, .parish-map\"},\n",
        "                \"Interactive map detected - will extract from JS data and markers\"\n",
        "            )\n",
        "\n",
        "        elif listing_type == ParishListingType.STATIC_TABLE:\n",
        "            return (\n",
        "                \"table_extraction\",\n",
        "                0.95,\n",
        "                {\"table\": \"table\", \"rows\": \"tr:not(:first-child)\"},\n",
        "                \"HTML table detected - most reliable extraction method\"\n",
        "            )\n",
        "\n",
        "        elif platform == DiocesePlatform.SQUARESPACE:\n",
        "            return (\n",
        "                \"squarespace_extraction\",\n",
        "                0.8,\n",
        "                {\"items\": \".summary-item, .parish-item\", \"title\": \".summary-title\"},\n",
        "                \"SquareSpace platform - using platform-specific selectors\"\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            return (\n",
        "                \"generic_extraction\",\n",
        "                0.4,\n",
        "                {\"containers\": \"[class*='parish'], [class*='church']\"},\n",
        "                \"Using generic extraction patterns\"\n",
        "            )\n",
        "\n",
        "print(\"âœ… Enhanced pattern detection classes loaded with detailed extraction support\")"
      ],
      "metadata": {
        "id": "7NQqjWrmtJMr",
        "outputId": "d497432a-a23d-4bc3-9c4a-d621e7914fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enhanced pattern detection classes loaded with detailed extraction support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 4: Enhanced DiocesesCardExtractor with Detail Page Navigation (UPDATED)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedDiocesesCardExtractor(BaseExtractor):\n",
        "    \"\"\"Enhanced extractor that clicks on each parish card to get detailed information\"\"\"\n",
        "\n",
        "    def __init__(self, pattern: DioceseSitePattern):\n",
        "        super().__init__(pattern)\n",
        "        self.detail_extraction_count = 0\n",
        "        self.detail_extraction_errors = 0\n",
        "\n",
        "    def extract(self, driver, soup: BeautifulSoup, url: str) -> List[ParishData]:\n",
        "        parishes = []\n",
        "\n",
        "        try:\n",
        "            print(\"    ðŸ“ Enhanced diocese card layout detected - extracting with detail pages\")\n",
        "\n",
        "            # Find all parish cards using the specific Salt Lake City structure\n",
        "            parish_cards = soup.find_all('div', class_='col-lg location')\n",
        "            print(f\"    ðŸ“Š Found {len(parish_cards)} parish cards\")\n",
        "\n",
        "            for i, card in enumerate(parish_cards, 1):\n",
        "                try:\n",
        "                    print(f\"    ðŸ”„ Processing parish {i}/{len(parish_cards)}\")\n",
        "                    parish_data = self._extract_parish_from_card_with_details(card, url, driver, i)\n",
        "                    if parish_data:\n",
        "                        parishes.append(parish_data)\n",
        "                        if parish_data.detail_extraction_success:\n",
        "                            self.detail_extraction_count += 1\n",
        "                        else:\n",
        "                            self.detail_extraction_errors += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"    âš ï¸ Error extracting from card {i}: {str(e)[:100]}...\")\n",
        "                    self.detail_extraction_errors += 1\n",
        "                    continue\n",
        "\n",
        "            print(f\"    ðŸ“Š Summary: {self.detail_extraction_count} detailed extractions successful, {self.detail_extraction_errors} failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    âš ï¸ Enhanced diocese card extraction error: {str(e)[:100]}...\")\n",
        "\n",
        "        return parishes\n",
        "\n",
        "    def _extract_parish_from_card_with_details(self, card, base_url: str, driver, card_number: int) -> Optional[ParishData]:\n",
        "        \"\"\"Extract parish data from a single card and navigate to detail page\"\"\"\n",
        "        try:\n",
        "            # Step 1: Extract basic information from the card\n",
        "            card_link = card.find('a', class_='card')\n",
        "            if not card_link:\n",
        "                return None\n",
        "\n",
        "            # Extract parish name from card title\n",
        "            title_elem = card_link.find('h4', class_='card-title')\n",
        "            if not title_elem:\n",
        "                return None\n",
        "\n",
        "            name = self.clean_text(title_elem.get_text())\n",
        "            if not name or len(name) < 3:\n",
        "                return None\n",
        "\n",
        "            # Skip non-parish entries\n",
        "            skip_terms = [\n",
        "                'no parish registration', 'contact', 'chancery', 'pastoral center',\n",
        "                'tv mass', 'directory', 'finder', 'diocese', 'bishop', 'office'\n",
        "            ]\n",
        "            if any(term in name.lower() for term in skip_terms):\n",
        "                return None\n",
        "\n",
        "            # Extract city from card body\n",
        "            card_body = card_link.find('div', class_='card-body')\n",
        "            city = None\n",
        "            state = None\n",
        "            if card_body:\n",
        "                body_text = card_body.get_text()\n",
        "                lines = [line.strip() for line in body_text.split('\\n') if line.strip()]\n",
        "\n",
        "                # The city is usually the second line (after the parish name)\n",
        "                if len(lines) >= 2:\n",
        "                    city_line = lines[1]\n",
        "                    if city_line and not city_line.startswith('Learn More'):\n",
        "                        city = self.clean_text(city_line)\n",
        "\n",
        "            # Extract parish detail URL\n",
        "            parish_detail_url = None\n",
        "            href = card_link.get('href')\n",
        "            if href:\n",
        "                if href.startswith('/'):\n",
        "                    from urllib.parse import urljoin\n",
        "                    parish_detail_url = urljoin(base_url, href)\n",
        "                else:\n",
        "                    parish_detail_url = href\n",
        "\n",
        "            # Extract state from city if present (format: \"City, ST\")\n",
        "            if city and ', ' in city:\n",
        "                city_parts = city.split(', ')\n",
        "                if len(city_parts) == 2:\n",
        "                    city = city_parts[0].strip()\n",
        "                    state = city_parts[1].strip()\n",
        "\n",
        "            # Step 2: Navigate to detail page and extract additional information\n",
        "            detailed_info = self._extract_details_from_parish_page(driver, parish_detail_url, name)\n",
        "\n",
        "            # Step 3: Create comprehensive parish data object\n",
        "            parish_data = ParishData(\n",
        "                name=name,\n",
        "                city=city,\n",
        "                state=state,\n",
        "                parish_detail_url=parish_detail_url,\n",
        "                confidence_score=0.9,\n",
        "                extraction_method=\"enhanced_diocese_card_extraction\"\n",
        "            )\n",
        "\n",
        "            # Add detailed information if extraction was successful\n",
        "            if detailed_info['success']:\n",
        "                parish_data.street_address = detailed_info.get('street_address')\n",
        "                parish_data.full_address = detailed_info.get('full_address')\n",
        "                parish_data.zip_code = detailed_info.get('zip_code')\n",
        "                parish_data.phone = detailed_info.get('phone')\n",
        "                parish_data.website = detailed_info.get('website')\n",
        "                parish_data.clergy_info = detailed_info.get('clergy_info')\n",
        "                parish_data.service_times = detailed_info.get('service_times')\n",
        "                parish_data.detail_extraction_success = True\n",
        "                parish_data.confidence_score = 0.95  # Higher confidence with detailed info\n",
        "                print(f\"      âœ… {name}: Complete details extracted\")\n",
        "            else:\n",
        "                parish_data.detail_extraction_success = False\n",
        "                parish_data.detail_extraction_error = detailed_info.get('error')\n",
        "                print(f\"      âš ï¸ {name}: Basic info only - {detailed_info.get('error', 'Unknown error')}\")\n",
        "\n",
        "            return parish_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    âš ï¸ Error parsing card {card_number}: {str(e)[:50]}...\")\n",
        "            return None\n",
        "\n",
        "    def _extract_details_from_parish_page(self, driver, parish_url: str, parish_name: str) -> Dict:\n",
        "        \"\"\"Navigate to parish detail page and extract detailed information\"\"\"\n",
        "\n",
        "        if not parish_url:\n",
        "            return {'success': False, 'error': 'No detail URL available'}\n",
        "\n",
        "        try:\n",
        "            print(f\"      ðŸ”— Navigating to: {parish_url}\")\n",
        "\n",
        "            # Navigate to the parish detail page\n",
        "            driver.get(parish_url)\n",
        "            time.sleep(2)  # Wait for page to load\n",
        "\n",
        "            # Get the page source and parse it\n",
        "            detail_html = driver.page_source\n",
        "            detail_soup = BeautifulSoup(detail_html, 'html.parser')\n",
        "\n",
        "            # Initialize result dictionary\n",
        "            result = {\n",
        "                'success': True,\n",
        "                'street_address': None,\n",
        "                'full_address': None,\n",
        "                'zip_code': None,\n",
        "                'phone': None,\n",
        "                'website': None,\n",
        "                'clergy_info': None,\n",
        "                'service_times': None\n",
        "            }\n",
        "\n",
        "            # Extract contact information from the detail page\n",
        "            self._extract_contact_info(detail_soup, result)\n",
        "            self._extract_service_times(detail_soup, result)\n",
        "            self._extract_clergy_info(detail_soup, result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to extract details: {str(e)[:100]}\"\n",
        "            print(f\"      âŒ {parish_name}: {error_msg}\")\n",
        "            return {'success': False, 'error': error_msg}\n",
        "\n",
        "    def _extract_contact_info(self, soup: BeautifulSoup, result: Dict):\n",
        "        \"\"\"Extract contact information from parish detail page\"\"\"\n",
        "        try:\n",
        "            # Look for contact info section\n",
        "            contact_sections = soup.find_all(['div', 'section'], class_=re.compile(r'contact', re.I))\n",
        "\n",
        "            # Also look for FA icons which often indicate contact info\n",
        "            fa_ul_sections = soup.find_all('ul', class_='fa-ul')\n",
        "\n",
        "            all_contact_sections = contact_sections + fa_ul_sections\n",
        "\n",
        "            for section in all_contact_sections:\n",
        "                text_content = section.get_text()\n",
        "\n",
        "                # Extract phone number\n",
        "                if not result['phone']:\n",
        "                    phone_links = section.find_all('a', href=re.compile(r'^tel:'))\n",
        "                    if phone_links:\n",
        "                        phone = phone_links[0].get_text().strip()\n",
        "                        result['phone'] = self.clean_text(phone)\n",
        "                    else:\n",
        "                        # Look for phone patterns in text\n",
        "                        phone_match = re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text_content)\n",
        "                        if phone_match:\n",
        "                            result['phone'] = phone_match.group()\n",
        "\n",
        "                # Extract website\n",
        "                if not result['website']:\n",
        "                    website_links = section.find_all('a', href=re.compile(r'^http'))\n",
        "                    for link in website_links:\n",
        "                        href = link.get('href', '')\n",
        "                        # Skip social media and diocese links\n",
        "                        if not any(skip in href.lower() for skip in ['facebook', 'twitter', 'instagram', 'dioslc.org']):\n",
        "                            result['website'] = href\n",
        "                            break\n",
        "\n",
        "                # Extract address\n",
        "                if not result['full_address']:\n",
        "                    # Look for address patterns\n",
        "                    address_lines = []\n",
        "                    list_items = section.find_all('li')\n",
        "\n",
        "                    for li in list_items:\n",
        "                        li_text = li.get_text().strip()\n",
        "                        # Check if this looks like an address (contains numbers and common address words)\n",
        "                        if re.search(r'\\d+.*(?:street|st|avenue|ave|road|rd|drive|dr|way|lane|ln|boulevard|blvd)', li_text, re.I):\n",
        "                            address_lines.append(li_text)\n",
        "                        elif re.search(r'\\d+\\s+[A-Za-z]', li_text) and ',' in li_text:\n",
        "                            # Format like \"331 East South Temple Street, Salt Lake City, UT 84111\"\n",
        "                            address_lines.append(li_text)\n",
        "\n",
        "                    if address_lines:\n",
        "                        full_address = address_lines[0]\n",
        "                        result['full_address'] = full_address\n",
        "\n",
        "                        # Try to parse street address and zip code\n",
        "                        self._parse_address_components(full_address, result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"        âš ï¸ Error extracting contact info: {str(e)[:50]}\")\n",
        "\n",
        "    def _parse_address_components(self, full_address: str, result: Dict):\n",
        "        \"\"\"Parse full address into street address and zip code\"\"\"\n",
        "        try:\n",
        "            # Extract zip code (5 digits, possibly followed by 4 more)\n",
        "            zip_match = re.search(r'\\b(\\d{5}(?:-\\d{4})?)\\b', full_address)\n",
        "            if zip_match:\n",
        "                result['zip_code'] = zip_match.group(1)\n",
        "\n",
        "            # Extract street address (everything before the first comma, or before city/state)\n",
        "            address_parts = full_address.split(',')\n",
        "            if len(address_parts) > 0:\n",
        "                potential_street = address_parts[0].strip()\n",
        "                # Make sure it looks like a street address\n",
        "                if re.search(r'\\d+', potential_street):\n",
        "                    result['street_address'] = potential_street\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"        âš ï¸ Error parsing address: {str(e)[:30]}\")\n",
        "\n",
        "    def _extract_service_times(self, soup: BeautifulSoup, result: Dict):\n",
        "        \"\"\"Extract service times from parish detail page\"\"\"\n",
        "        try:\n",
        "            # Look for service times section\n",
        "            service_sections = soup.find_all(['div', 'section'],\n",
        "                                           string=re.compile(r'service.*times|mass.*times|masses|schedule', re.I))\n",
        "\n",
        "            # Also look for h3 headers that might indicate service times\n",
        "            service_headers = soup.find_all(['h3', 'h4'],\n",
        "                                          string=re.compile(r'service.*times|mass.*times|masses|schedule', re.I))\n",
        "\n",
        "            # Look for lists that might contain service information\n",
        "            service_lists = []\n",
        "            for header in service_headers:\n",
        "                next_sibling = header.find_next_sibling(['ul', 'div'])\n",
        "                if next_sibling:\n",
        "                    service_lists.append(next_sibling)\n",
        "\n",
        "            all_service_sections = service_sections + service_lists\n",
        "\n",
        "            for section in all_service_sections:\n",
        "                if section:\n",
        "                    service_text = section.get_text()\n",
        "                    # Extract meaningful service time information\n",
        "                    lines = [line.strip() for line in service_text.split('\\n') if line.strip()]\n",
        "                    # Filter out very short lines and focus on schedule information\n",
        "                    schedule_lines = [line for line in lines if len(line) > 10 and\n",
        "                                    any(keyword in line.lower() for keyword in\n",
        "                                        ['sunday', 'saturday', 'daily', 'mass', 'service', 'am', 'pm'])]\n",
        "\n",
        "                    if schedule_lines:\n",
        "                        result['service_times'] = '; '.join(schedule_lines[:5])  # Limit to first 5 lines\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"        âš ï¸ Error extracting service times: {str(e)[:50]}\")\n",
        "\n",
        "    def _extract_clergy_info(self, soup: BeautifulSoup, result: Dict):\n",
        "        \"\"\"Extract clergy information from parish detail page\"\"\"\n",
        "        try:\n",
        "            # Look for clergy section\n",
        "            clergy_sections = soup.find_all(['div', 'section'], class_=re.compile(r'clergy|pastor|priest', re.I))\n",
        "\n",
        "            # Also look for directory cards that might contain clergy info\n",
        "            directory_cards = soup.find_all(['div'], class_=re.compile(r'directory|card', re.I))\n",
        "\n",
        "            all_clergy_sections = clergy_sections + directory_cards\n",
        "\n",
        "            clergy_info = []\n",
        "            for section in all_clergy_sections:\n",
        "                # Look for names and titles\n",
        "                titles = section.find_all(['h4', 'h5'], class_=re.compile(r'title|name', re.I))\n",
        "                for title in titles:\n",
        "                    title_text = title.get_text().strip()\n",
        "                    # Check if this looks like a clergy title\n",
        "                    if any(clergy_word in title_text.lower() for clergy_word in\n",
        "                           ['reverend', 'father', 'pastor', 'deacon', 'rev.', 'fr.', 'dcn.']):\n",
        "\n",
        "                        # Get associated role/description\n",
        "                        role_elem = title.find_next_sibling(['p', 'div'])\n",
        "                        role_text = role_elem.get_text().strip() if role_elem else \"\"\n",
        "\n",
        "                        if role_text:\n",
        "                            clergy_info.append(f\"{title_text}: {role_text}\")\n",
        "                        else:\n",
        "                            clergy_info.append(title_text)\n",
        "\n",
        "            if clergy_info:\n",
        "                result['clergy_info'] = '; '.join(clergy_info[:3])  # Limit to first 3 clergy members\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"        âš ï¸ Error extracting clergy info: {str(e)[:50]}\")\n",
        "\n",
        "# Update the original DiocesesCardExtractor to use the enhanced version\n",
        "DiocesesCardExtractor = EnhancedDiocesesCardExtractor\n",
        "\n",
        "print(\"âœ… Enhanced DiocesesCardExtractor loaded with detail page navigation\")"
      ],
      "metadata": {
        "id": "bGq9bLEhtQWc",
        "outputId": "59d14af0-78f7-44de-bfea-b229780f73a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enhanced DiocesesCardExtractor loaded with detail page navigation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 4.5: Improved Map Extractor (INSERT AFTER CELL 4)\n",
        "# =============================================================================\n",
        "\n",
        "class ImprovedInteractiveMapExtractor(BaseExtractor):\n",
        "    \"\"\"Improved extractor for JavaScript-powered maps with better error handling\"\"\"\n",
        "\n",
        "    def extract(self, driver, soup: BeautifulSoup, url: str) -> List[ParishData]:\n",
        "        parishes = []\n",
        "\n",
        "        try:\n",
        "            # Try to find map containers with more flexible selectors\n",
        "            map_selectors = [\n",
        "                \"#map\", \".map\", \".parish-map\", \".church-map\",\n",
        "                \"[id*='map']\", \"[class*='map']\",\n",
        "                \"#parish-finder\", \".parish-finder\"\n",
        "            ]\n",
        "\n",
        "            map_found = False\n",
        "            for selector in map_selectors:\n",
        "                try:\n",
        "                    WebDriverWait(driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                    )\n",
        "                    map_found = True\n",
        "                    print(f\"    ðŸ“ Found map container: {selector}\")\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not map_found:\n",
        "                print(f\"    â„¹ï¸ No map container found, trying direct JS extraction...\")\n",
        "\n",
        "            # Method 1: Extract from JavaScript variables (works even without visible map)\n",
        "            parishes.extend(self._extract_from_js_variables(driver))\n",
        "\n",
        "            # Method 2: Look for parish data in script tags\n",
        "            if not parishes:\n",
        "                parishes.extend(self._extract_from_script_tags(soup))\n",
        "\n",
        "            # Method 3: Extract from map markers (only if map found)\n",
        "            if not parishes and map_found:\n",
        "                parishes.extend(self._extract_from_markers(driver))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    â„¹ï¸ Map extraction completed with info: {str(e)[:100]}...\")\n",
        "\n",
        "        return parishes\n",
        "\n",
        "    def _extract_from_script_tags(self, soup: BeautifulSoup) -> List[ParishData]:\n",
        "        \"\"\"Extract parish data from script tags containing JSON\"\"\"\n",
        "        parishes = []\n",
        "\n",
        "        try:\n",
        "            script_tags = soup.find_all('script')\n",
        "\n",
        "            for script in script_tags:\n",
        "                if not script.string:\n",
        "                    continue\n",
        "\n",
        "                script_content = script.string\n",
        "\n",
        "                # Look for JSON-like data containing parish information\n",
        "                if any(keyword in script_content.lower() for keyword in\n",
        "                       ['parish', 'church', 'location', 'marker']):\n",
        "\n",
        "                    # Try to extract JSON objects\n",
        "                    import json\n",
        "\n",
        "                    # Look for common patterns\n",
        "                    patterns = [\n",
        "                        r'parishes\\s*[:=]\\s*(\\[.*?\\])',\n",
        "                        r'locations\\s*[:=]\\s*(\\[.*?\\])',\n",
        "                        r'markers\\s*[:=]\\s*(\\[.*?\\])',\n",
        "                        r'churches\\s*[:=]\\s*(\\[.*?\\])'\n",
        "                    ]\n",
        "\n",
        "                    for pattern in patterns:\n",
        "                        matches = re.findall(pattern, script_content, re.DOTALL)\n",
        "                        for match in matches:\n",
        "                            try:\n",
        "                                data = json.loads(match)\n",
        "                                if isinstance(data, list):\n",
        "                                    for item in data:\n",
        "                                        parish = self._parse_js_parish_object(item)\n",
        "                                        if parish:\n",
        "                                            parishes.append(parish)\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        if parishes:\n",
        "                            break\n",
        "\n",
        "                if parishes:\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    â„¹ï¸ Script tag extraction info: {str(e)[:50]}...\")\n",
        "\n",
        "        return parishes\n",
        "\n",
        "    def _extract_from_js_variables(self, driver) -> List[ParishData]:\n",
        "        \"\"\"Extract from common JavaScript variable names with better error handling\"\"\"\n",
        "        parishes = []\n",
        "\n",
        "        # Expanded list of common variable names\n",
        "        js_vars = [\n",
        "            \"parishes\", \"parishData\", \"locations\", \"markers\", \"churchData\",\n",
        "            \"parishList\", \"churches\", \"mapData\", \"data\", \"items\",\n",
        "            \"parishInfo\", \"churchInfo\", \"mapMarkers\", \"points\"\n",
        "        ]\n",
        "\n",
        "        for var_name in js_vars:\n",
        "            try:\n",
        "                js_data = driver.execute_script(f\"\"\"\n",
        "                    try {{\n",
        "                        return window.{var_name};\n",
        "                    }} catch(e) {{\n",
        "                        return null;\n",
        "                    }}\n",
        "                \"\"\")\n",
        "\n",
        "                if js_data and isinstance(js_data, list) and len(js_data) > 0:\n",
        "                    print(f\"    ðŸ“Š Found data in window.{var_name}: {len(js_data)} items\")\n",
        "\n",
        "                    for item in js_data:\n",
        "                        parish = self._parse_js_parish_object(item)\n",
        "                        if parish:\n",
        "                            parishes.append(parish)\n",
        "\n",
        "                    if parishes:\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return parishes\n",
        "\n",
        "    def _parse_js_parish_object(self, data: Dict) -> Optional[ParishData]:\n",
        "        \"\"\"Enhanced parsing of parish data from JavaScript object\"\"\"\n",
        "        if not isinstance(data, dict):\n",
        "            return None\n",
        "\n",
        "        # Enhanced field mapping for name\n",
        "        name = None\n",
        "        for field in ['name', 'title', 'parishName', 'churchName', 'parish_name',\n",
        "                      'church_name', 'label', 'text', 'Name', 'Title']:\n",
        "            if field in data and data[field]:\n",
        "                name = str(data[field]).strip()\n",
        "                break\n",
        "\n",
        "        if not name or len(name) < 3:\n",
        "            return None\n",
        "\n",
        "        # Skip non-parish entries\n",
        "        if any(skip_word in name.lower() for skip_word in\n",
        "               ['finder', 'directory', 'map', 'search', 'filter']):\n",
        "            return None\n",
        "\n",
        "        # Enhanced field mapping for other data\n",
        "        address = None\n",
        "        for field in ['address', 'location', 'fullAddress', 'street', 'addr']:\n",
        "            if field in data and data[field]:\n",
        "                address = str(data[field]).strip()\n",
        "                break\n",
        "\n",
        "        phone = None\n",
        "        for field in ['phone', 'telephone', 'phoneNumber', 'tel', 'Phone']:\n",
        "            if field in data and data[field]:\n",
        "                phone = str(data[field]).strip()\n",
        "                break\n",
        "\n",
        "        website = None\n",
        "        for field in ['website', 'url', 'link', 'web', 'Website', 'URL']:\n",
        "            if field in data and data[field]:\n",
        "                website = str(data[field]).strip()\n",
        "                break\n",
        "\n",
        "        # Coordinates\n",
        "        lat = data.get('lat', data.get('latitude', data.get('Lat')))\n",
        "        lng = data.get('lng', data.get('longitude', data.get('lon', data.get('Lng'))))\n",
        "\n",
        "        return ParishData(\n",
        "            name=name,\n",
        "            address=address,\n",
        "            phone=phone,\n",
        "            website=website,\n",
        "            latitude=float(lat) if lat else None,\n",
        "            longitude=float(lng) if lng else None,\n",
        "            confidence_score=0.8,\n",
        "            extraction_method=\"improved_js_extraction\"\n",
        "        )\n",
        "\n",
        "    def _extract_from_markers(self, driver) -> List[ParishData]:\n",
        "        \"\"\"Extract by clicking map markers with improved error handling\"\"\"\n",
        "        parishes = []\n",
        "\n",
        "        try:\n",
        "            # More flexible marker selectors\n",
        "            marker_selectors = [\n",
        "                \".marker\", \".leaflet-marker\", \".map-marker\",\n",
        "                \"[class*='marker']\", \".gm-style-iw\", \".mapboxgl-marker\"\n",
        "            ]\n",
        "\n",
        "            markers = []\n",
        "            for selector in marker_selectors:\n",
        "                try:\n",
        "                    found_markers = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if found_markers:\n",
        "                        markers = found_markers\n",
        "                        print(f\"    ðŸ“ Found {len(markers)} markers using {selector}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not markers:\n",
        "                print(f\"    â„¹ï¸ No clickable markers found\")\n",
        "                return parishes\n",
        "\n",
        "            # Limit markers to avoid timeout\n",
        "            for i, marker in enumerate(markers[:5]):  # Only try first 5\n",
        "                try:\n",
        "                    # Scroll marker into view\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", marker)\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "                    # Click marker\n",
        "                    driver.execute_script(\"arguments[0].click();\", marker)\n",
        "                    time.sleep(1)\n",
        "\n",
        "                    # Look for popup content with multiple selectors\n",
        "                    popup_selectors = [\n",
        "                        \".popup\", \".info-window\", \".mapboxgl-popup\",\n",
        "                        \".leaflet-popup\", \".gm-style-iw-d\"\n",
        "                    ]\n",
        "\n",
        "                    popup_text = None\n",
        "                    for popup_selector in popup_selectors:\n",
        "                        try:\n",
        "                            popup = driver.find_element(By.CSS_SELECTOR, popup_selector)\n",
        "                            popup_text = popup.text\n",
        "                            break\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                    if popup_text and len(popup_text) > 10:\n",
        "                        parish_data = self._parse_popup_content(popup_text)\n",
        "                        if parish_data:\n",
        "                            parishes.append(parish_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    â„¹ï¸ Marker extraction completed: {str(e)[:50]}...\")\n",
        "\n",
        "        return parishes\n",
        "\n",
        "    def _parse_popup_content(self, popup_text: str) -> Optional[ParishData]:\n",
        "        \"\"\"Parse parish information from popup text\"\"\"\n",
        "        lines = [line.strip() for line in popup_text.split('\\n') if line.strip()]\n",
        "\n",
        "        if not lines:\n",
        "            return None\n",
        "\n",
        "        name = lines[0]  # First line is usually the name\n",
        "\n",
        "        # Skip if it doesn't look like a parish name\n",
        "        if not any(indicator in name.lower() for indicator in\n",
        "                  ['parish', 'church', 'st.', 'saint', 'our lady', 'holy', 'cathedral']):\n",
        "            return None\n",
        "\n",
        "        address = None\n",
        "        phone = None\n",
        "\n",
        "        # Look for address and phone in remaining lines\n",
        "        for line in lines[1:]:\n",
        "            if self.extract_phone(line):\n",
        "                phone = self.extract_phone(line)\n",
        "            elif re.search(r'\\d+.*(?:street|st|avenue|ave|road|rd|drive|dr)', line, re.I):\n",
        "                address = line\n",
        "\n",
        "        return ParishData(\n",
        "            name=name,\n",
        "            address=address,\n",
        "            phone=phone,\n",
        "            confidence_score=0.6,\n",
        "            extraction_method=\"marker_popup_extraction\"\n",
        "        )\n",
        "\n",
        "# Replace the InteractiveMapExtractor in the main processing function\n",
        "print(\"âœ… Improved map extractor loaded with better error handling\")"
      ],
      "metadata": {
        "id": "er9P6JI9yCAu",
        "outputId": "5a339689-d1af-4d7f-d39c-23b8c81f867d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Improved map extractor loaded with better error handling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Enhanced Database Integration Functions (COMPLETE - UPDATED for detailed parish data)\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_parish_for_supabase(parish_data: ParishData, diocese_name: str, diocese_url: str, parish_directory_url: str) -> Dict:\n",
        "    \"\"\"Convert ParishData to format compatible with your existing Supabase schema (ENHANCED)\"\"\"\n",
        "\n",
        "    # Use street address if available, otherwise fall back to full address\n",
        "    street_address = parish_data.street_address or parish_data.full_address or parish_data.address\n",
        "\n",
        "    return {\n",
        "        'Name': parish_data.name,\n",
        "        'Status': 'Parish',  # Default status\n",
        "        'Deanery': None,  # Will be populated later if available\n",
        "        'Street Address': street_address,\n",
        "        'City': parish_data.city,\n",
        "        'State': parish_data.state,\n",
        "        'Zip Code': parish_data.zip_code,\n",
        "        'Phone Number': parish_data.phone,\n",
        "        'Web': parish_data.website,\n",
        "        'diocese_url': diocese_url,\n",
        "        'parish_directory_url': parish_directory_url,\n",
        "        'parish_detail_url': parish_data.parish_detail_url,\n",
        "        'extraction_method': parish_data.extraction_method,\n",
        "        # Enhanced metadata fields\n",
        "        'confidence_score': parish_data.confidence_score,\n",
        "        'detail_extraction_success': parish_data.detail_extraction_success,\n",
        "        'detail_extraction_error': parish_data.detail_extraction_error,\n",
        "        'clergy_info': parish_data.clergy_info,\n",
        "        'service_times': parish_data.service_times,\n",
        "        'full_address': parish_data.full_address,\n",
        "        'extracted_at': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "def enhanced_safe_upsert_to_supabase(parishes: List[ParishData], diocese_name: str, diocese_url: str, parish_directory_url: str):\n",
        "    \"\"\"Enhanced version of your existing Supabase upsert function with detailed data support\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(\"  âŒ Supabase not available\")\n",
        "        return False\n",
        "\n",
        "    success_count = 0\n",
        "    detail_success_count = 0\n",
        "\n",
        "    for parish in parishes:\n",
        "        try:\n",
        "            # Skip non-parish items (like \"Parish Finder\", \"Contact Info\", etc.)\n",
        "            if any(skip_word in parish.name.lower() for skip_word in\n",
        "                   ['finder', 'contact', 'chancery', 'pastoral center', 'tv mass', 'directory']):\n",
        "                print(f\"    â­ï¸ Skipped: {parish.name} (not a parish)\")\n",
        "                continue\n",
        "\n",
        "            # Convert to your existing schema format\n",
        "            supabase_data = prepare_parish_for_supabase(parish, diocese_name, diocese_url, parish_directory_url)\n",
        "\n",
        "            # Remove None values and empty strings\n",
        "            clean_data = {k: v for k, v in supabase_data.items()\n",
        "                         if v is not None and v != \"\"}\n",
        "\n",
        "            # Must have a name to proceed\n",
        "            if not clean_data.get('Name') or len(clean_data.get('Name', '')) < 3:\n",
        "                print(f\"    â­ï¸ Skipped: Invalid name for parish\")\n",
        "                continue\n",
        "\n",
        "            # Use your existing upsert logic\n",
        "            response = supabase.table('Parishes').insert(clean_data).execute()\n",
        "\n",
        "            if hasattr(response, 'error') and response.error:\n",
        "                print(f\"    âŒ Database error for {parish.name}: {response.error}\")\n",
        "            else:\n",
        "                success_count += 1\n",
        "                if parish.detail_extraction_success:\n",
        "                    detail_success_count += 1\n",
        "                    detail_indicator = \"ðŸ“\" # Pin icon for detailed info\n",
        "                else:\n",
        "                    detail_indicator = \"ðŸ“Œ\" # Basic pin for basic info\n",
        "\n",
        "                print(f\"    âœ… {detail_indicator} Saved: {parish.name} (confidence: {parish.confidence_score:.2f})\")\n",
        "\n",
        "                # Show what detailed fields were captured\n",
        "                if parish.detail_extraction_success:\n",
        "                    details = []\n",
        "                    if parish.street_address: details.append(\"address\")\n",
        "                    if parish.phone: details.append(\"phone\")\n",
        "                    if parish.zip_code: details.append(\"zip\")\n",
        "                    if parish.website: details.append(\"website\")\n",
        "                    if parish.clergy_info: details.append(\"clergy\")\n",
        "                    if parish.service_times: details.append(\"schedule\")\n",
        "\n",
        "                    if details:\n",
        "                        print(f\"        ðŸ“‹ Details: {', '.join(details)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    âŒ Error saving {parish.name}: {e}\")\n",
        "\n",
        "    print(f\"  ðŸ“Š Successfully saved {success_count}/{len(parishes)} parishes\")\n",
        "    print(f\"  ðŸ“ Detailed information captured for {detail_success_count}/{success_count} parishes\")\n",
        "    return success_count > 0\n",
        "\n",
        "# Enhanced metadata tracking with detailed extraction statistics\n",
        "def create_enhanced_extraction_metadata_record(diocese_name: str, diocese_url: str, parishes_found: List[ParishData], pattern_info: Dict):\n",
        "    \"\"\"Store enhanced extraction metadata including detail extraction statistics\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Calculate detail extraction statistics\n",
        "        total_parishes = len(parishes_found)\n",
        "        successful_details = sum(1 for p in parishes_found if p.detail_extraction_success)\n",
        "\n",
        "        # Count which detail fields were successfully extracted\n",
        "        field_stats = {\n",
        "            'addresses_extracted': sum(1 for p in parishes_found if p.street_address or p.full_address),\n",
        "            'phones_extracted': sum(1 for p in parishes_found if p.phone),\n",
        "            'websites_extracted': sum(1 for p in parishes_found if p.website),\n",
        "            'zip_codes_extracted': sum(1 for p in parishes_found if p.zip_code),\n",
        "            'clergy_info_extracted': sum(1 for p in parishes_found if p.clergy_info),\n",
        "            'service_times_extracted': sum(1 for p in parishes_found if p.service_times)\n",
        "        }\n",
        "\n",
        "        metadata = {\n",
        "            'diocese_name': diocese_name,\n",
        "            'diocese_url': diocese_url,\n",
        "            'parishes_count': total_parishes,\n",
        "            'detail_extraction_success_count': successful_details,\n",
        "            'detail_extraction_success_rate': (successful_details / total_parishes * 100) if total_parishes > 0 else 0,\n",
        "            'extraction_timestamp': datetime.now().isoformat(),\n",
        "            'pattern_platform': pattern_info.get('platform'),\n",
        "            'pattern_listing_type': pattern_info.get('listing_type'),\n",
        "            'pattern_confidence': pattern_info.get('confidence'),\n",
        "            'extraction_methods': ', '.join(pattern_info.get('methods_used', [])),\n",
        "            'success': total_parishes > 0,\n",
        "            # Field extraction statistics\n",
        "            'addresses_extracted': field_stats['addresses_extracted'],\n",
        "            'phones_extracted': field_stats['phones_extracted'],\n",
        "            'websites_extracted': field_stats['websites_extracted'],\n",
        "            'zip_codes_extracted': field_stats['zip_codes_extracted'],\n",
        "            'clergy_info_extracted': field_stats['clergy_info_extracted'],\n",
        "            'service_times_extracted': field_stats['service_times_extracted']\n",
        "        }\n",
        "\n",
        "        # You would need to create this enhanced table in Supabase:\n",
        "        # CREATE TABLE extraction_metadata (\n",
        "        #   id SERIAL PRIMARY KEY,\n",
        "        #   diocese_name TEXT,\n",
        "        #   diocese_url TEXT,\n",
        "        #   parishes_count INTEGER,\n",
        "        #   detail_extraction_success_count INTEGER,\n",
        "        #   detail_extraction_success_rate FLOAT,\n",
        "        #   extraction_timestamp TIMESTAMP,\n",
        "        #   pattern_platform TEXT,\n",
        "        #   pattern_listing_type TEXT,\n",
        "        #   pattern_confidence FLOAT,\n",
        "        #   extraction_methods TEXT,\n",
        "        #   success BOOLEAN,\n",
        "        #   addresses_extracted INTEGER,\n",
        "        #   phones_extracted INTEGER,\n",
        "        #   websites_extracted INTEGER,\n",
        "        #   zip_codes_extracted INTEGER,\n",
        "        #   clergy_info_extracted INTEGER,\n",
        "        #   service_times_extracted INTEGER\n",
        "        # );\n",
        "\n",
        "        response = supabase.table('extraction_metadata').insert(metadata).execute()\n",
        "        return not (hasattr(response, 'error') and response.error)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Could not save extraction metadata: {e}\")\n",
        "        return False\n",
        "\n",
        "# Alternative: Create a separate table for detailed parish information if needed\n",
        "def create_detailed_parish_table_record(parish_data: ParishData, diocese_name: str):\n",
        "    \"\"\"Store detailed parish information in a separate detailed table (optional)\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # This creates a separate table for enhanced parish details\n",
        "        detailed_data = {\n",
        "            'parish_name': parish_data.name,\n",
        "            'diocese_name': diocese_name,\n",
        "            'parish_detail_url': parish_data.parish_detail_url,\n",
        "            'full_address': parish_data.full_address,\n",
        "            'street_address': parish_data.street_address,\n",
        "            'clergy_info': parish_data.clergy_info,\n",
        "            'service_times': parish_data.service_times,\n",
        "            'extraction_timestamp': datetime.now().isoformat(),\n",
        "            'extraction_method': parish_data.extraction_method,\n",
        "            'confidence_score': parish_data.confidence_score,\n",
        "            'detail_extraction_success': parish_data.detail_extraction_success,\n",
        "            'detail_extraction_error': parish_data.detail_extraction_error\n",
        "        }\n",
        "\n",
        "        # Remove None values\n",
        "        clean_detailed_data = {k: v for k, v in detailed_data.items()\n",
        "                              if v is not None and v != \"\"}\n",
        "\n",
        "        # You would need to create this table in Supabase:\n",
        "        # CREATE TABLE parish_detailed_info (\n",
        "        #   id SERIAL PRIMARY KEY,\n",
        "        #   parish_name TEXT,\n",
        "        #   diocese_name TEXT,\n",
        "        #   parish_detail_url TEXT,\n",
        "        #   full_address TEXT,\n",
        "        #   street_address TEXT,\n",
        "        #   clergy_info TEXT,\n",
        "        #   service_times TEXT,\n",
        "        #   extraction_timestamp TIMESTAMP,\n",
        "        #   extraction_method TEXT,\n",
        "        #   confidence_score FLOAT,\n",
        "        #   detail_extraction_success BOOLEAN,\n",
        "        #   detail_extraction_error TEXT\n",
        "        # );\n",
        "\n",
        "        response = supabase.table('parish_detailed_info').insert(clean_detailed_data).execute()\n",
        "        return not (hasattr(response, 'error') and response.error)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Could not save detailed parish info: {e}\")\n",
        "        return False\n",
        "\n",
        "# Utility function to analyze extraction quality\n",
        "def analyze_extraction_quality(parishes: List[ParishData]) -> Dict:\n",
        "    \"\"\"Analyze the quality and completeness of parish data extraction\"\"\"\n",
        "\n",
        "    if not parishes:\n",
        "        return {'error': 'No parishes to analyze'}\n",
        "\n",
        "    total_parishes = len(parishes)\n",
        "\n",
        "    analysis = {\n",
        "        'total_parishes': total_parishes,\n",
        "        'basic_info_quality': {\n",
        "            'names_present': sum(1 for p in parishes if p.name and len(p.name) > 2),\n",
        "            'cities_present': sum(1 for p in parishes if p.city),\n",
        "            'states_present': sum(1 for p in parishes if p.state)\n",
        "        },\n",
        "        'detailed_info_quality': {\n",
        "            'addresses_present': sum(1 for p in parishes if p.street_address or p.full_address),\n",
        "            'phones_present': sum(1 for p in parishes if p.phone),\n",
        "            'websites_present': sum(1 for p in parishes if p.website),\n",
        "            'zip_codes_present': sum(1 for p in parishes if p.zip_code),\n",
        "            'clergy_info_present': sum(1 for p in parishes if p.clergy_info),\n",
        "            'service_times_present': sum(1 for p in parishes if p.service_times)\n",
        "        },\n",
        "        'extraction_success': {\n",
        "            'detail_extractions_attempted': sum(1 for p in parishes if hasattr(p, 'detail_extraction_success')),\n",
        "            'detail_extractions_successful': sum(1 for p in parishes if p.detail_extraction_success),\n",
        "            'high_confidence_extractions': sum(1 for p in parishes if p.confidence_score >= 0.8)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Calculate percentages\n",
        "    analysis['basic_info_percentages'] = {\n",
        "        f\"{key}_percentage\": (value / total_parishes * 100)\n",
        "        for key, value in analysis['basic_info_quality'].items()\n",
        "    }\n",
        "\n",
        "    analysis['detailed_info_percentages'] = {\n",
        "        f\"{key}_percentage\": (value / total_parishes * 100)\n",
        "        for key, value in analysis['detailed_info_quality'].items()\n",
        "    }\n",
        "\n",
        "    # Overall quality score (0-100)\n",
        "    basic_score = sum(analysis['basic_info_quality'].values()) / (total_parishes * 3) * 100\n",
        "    detailed_score = sum(analysis['detailed_info_quality'].values()) / (total_parishes * 6) * 100\n",
        "    analysis['overall_quality_score'] = (basic_score + detailed_score) / 2\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Function to export parish data to CSV for analysis\n",
        "def export_parishes_to_csv(parishes: List[ParishData], diocese_name: str) -> str:\n",
        "    \"\"\"Export parish data to CSV file for external analysis\"\"\"\n",
        "\n",
        "    import csv\n",
        "    import os\n",
        "\n",
        "    if not parishes:\n",
        "        return None\n",
        "\n",
        "    # Create filename\n",
        "    safe_diocese_name = \"\".join(c for c in diocese_name if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"parishes_{safe_diocese_name}_{timestamp}.csv\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = [\n",
        "                'Name', 'City', 'State', 'Street Address', 'Full Address', 'Zip Code',\n",
        "                'Phone', 'Website', 'Parish Detail URL', 'Clergy Info', 'Service Times',\n",
        "                'Confidence Score', 'Extraction Method', 'Detail Extraction Success',\n",
        "                'Detail Extraction Error'\n",
        "            ]\n",
        "\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for parish in parishes:\n",
        "                writer.writerow({\n",
        "                    'Name': parish.name,\n",
        "                    'City': parish.city,\n",
        "                    'State': parish.state,\n",
        "                    'Street Address': parish.street_address,\n",
        "                    'Full Address': parish.full_address,\n",
        "                    'Zip Code': parish.zip_code,\n",
        "                    'Phone': parish.phone,\n",
        "                    'Website': parish.website,\n",
        "                    'Parish Detail URL': parish.parish_detail_url,\n",
        "                    'Clergy Info': parish.clergy_info,\n",
        "                    'Service Times': parish.service_times,\n",
        "                    'Confidence Score': parish.confidence_score,\n",
        "                    'Extraction Method': parish.extraction_method,\n",
        "                    'Detail Extraction Success': parish.detail_extraction_success,\n",
        "                    'Detail Extraction Error': parish.detail_extraction_error\n",
        "                })\n",
        "\n",
        "        print(f\"ðŸ“ Parish data exported to: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error exporting to CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Complete enhanced database integration functions loaded with detailed parish data support\")"
      ],
      "metadata": {
        "id": "RcL9MpN5tSQR",
        "outputId": "ba50e967-d21e-465f-e557-d6a0136ba008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Complete enhanced database integration functions loaded with detailed parish data support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5.5: WebDriver Setup Function (ADD THIS BEFORE CELL 6)\n",
        "# =============================================================================\n",
        "\n",
        "def setup_enhanced_driver():\n",
        "    \"\"\"Set up Chrome WebDriver with options optimized for parish extraction\"\"\"\n",
        "\n",
        "    print(\"ðŸ”§ Setting up enhanced Chrome WebDriver...\")\n",
        "\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "    chrome_options.add_argument('--window-size=1920,1080')\n",
        "    chrome_options.add_argument('--disable-extensions')\n",
        "    chrome_options.add_argument('--disable-plugins')\n",
        "    chrome_options.add_argument('--disable-images')  # Speed up loading\n",
        "    chrome_options.add_argument('--disable-javascript-harmony-shipping')\n",
        "    chrome_options.add_argument('--disable-background-timer-throttling')\n",
        "    chrome_options.add_argument('--disable-backgrounding-occluded-windows')\n",
        "    chrome_options.add_argument('--disable-renderer-backgrounding')\n",
        "\n",
        "    # User agent to avoid blocking\n",
        "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "    try:\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        driver.set_page_load_timeout(30)\n",
        "        driver.implicitly_wait(5)\n",
        "\n",
        "        print(\"âœ… Chrome WebDriver initialized successfully\")\n",
        "        return driver\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to initialize WebDriver: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"âœ… WebDriver setup function loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LK3cKx0WCCG",
        "outputId": "79f3de1c-b965-423c-d055-06c32790b41a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… WebDriver setup function loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Enhanced Master Processing Function with Detail Extraction (UPDATED)\n",
        "# =============================================================================\n",
        "\n",
        "def process_diocese_with_detailed_extraction(diocese_info: Dict, driver) -> Dict:\n",
        "    \"\"\"\n",
        "    Enhanced processing function that extracts detailed parish information\n",
        "    by navigating to individual parish detail pages\n",
        "    \"\"\"\n",
        "\n",
        "    diocese_url = diocese_info['url']\n",
        "    diocese_name = diocese_info['name']\n",
        "    parish_directory_url = diocese_info['parish_directory_url']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸ” ENHANCED DETAILED PROCESSING: {diocese_name}\")\n",
        "    print(f\"ðŸ“ Main URL: {diocese_url}\")\n",
        "    print(f\"ðŸ“‚ Parish Directory URL: {parish_directory_url}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    result = {\n",
        "        'diocese_name': diocese_name,\n",
        "        'diocese_url': diocese_url,\n",
        "        'parish_directory_url': parish_directory_url,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'pattern_detected': None,\n",
        "        'parishes_found': [],\n",
        "        'success': False,\n",
        "        'extraction_methods_used': [],\n",
        "        'processing_time': 0,\n",
        "        'errors': [],\n",
        "        # Enhanced tracking\n",
        "        'detail_extraction_stats': {\n",
        "            'attempted': 0,\n",
        "            'successful': 0,\n",
        "            'failed': 0,\n",
        "            'success_rate': 0.0\n",
        "        },\n",
        "        'field_extraction_stats': {\n",
        "            'addresses_extracted': 0,\n",
        "            'phones_extracted': 0,\n",
        "            'websites_extracted': 0,\n",
        "            'zip_codes_extracted': 0,\n",
        "            'clergy_info_extracted': 0,\n",
        "            'service_times_extracted': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load the parish directory page\n",
        "        print(\"  ðŸ“¥ Loading parish directory page...\")\n",
        "        driver.get(parish_directory_url)\n",
        "        time.sleep(3)  # Give time for JS to load\n",
        "\n",
        "        html_content = driver.page_source\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Step 2: Detect pattern\n",
        "        print(\"  ðŸ” Detecting website pattern...\")\n",
        "        detector = PatternDetector()\n",
        "        pattern = detector.detect_pattern(html_content, parish_directory_url)\n",
        "\n",
        "        result['pattern_detected'] = {\n",
        "            'platform': pattern.platform.value,\n",
        "            'listing_type': pattern.listing_type.value,\n",
        "            'confidence': pattern.confidence_score,\n",
        "            'extraction_method': pattern.extraction_method,\n",
        "            'javascript_required': pattern.javascript_required,\n",
        "            'notes': pattern.notes\n",
        "        }\n",
        "\n",
        "        print(f\"    ðŸ“‹ Platform: {pattern.platform.value}\")\n",
        "        print(f\"    ðŸ“Š Listing Type: {pattern.listing_type.value}\")\n",
        "        print(f\"    ðŸŽ¯ Confidence: {pattern.confidence_score:.2f}\")\n",
        "        print(f\"    âš™ï¸ Method: {pattern.extraction_method}\")\n",
        "\n",
        "        # Step 3: Extract parishes with detailed information\n",
        "        parishes = []\n",
        "\n",
        "        # Try extractors in order of specificity\n",
        "        extractors_to_try = []\n",
        "\n",
        "        # Primary extractor based on detected pattern (enhanced versions)\n",
        "        if pattern.listing_type == ParishListingType.DIOCESE_CARD_LAYOUT:\n",
        "            extractors_to_try.append(('EnhancedDiocesesCardExtractor', EnhancedDiocesesCardExtractor(pattern)))\n",
        "        elif pattern.listing_type == ParishListingType.PARISH_FINDER:\n",
        "            extractors_to_try.append(('ParishFinderExtractor', ParishFinderExtractor(pattern)))\n",
        "        elif pattern.listing_type == ParishListingType.STATIC_TABLE:\n",
        "            extractors_to_try.append(('TableExtractor', TableExtractor(pattern)))\n",
        "        elif pattern.listing_type == ParishListingType.INTERACTIVE_MAP:\n",
        "            extractors_to_try.append(('ImprovedInteractiveMapExtractor', ImprovedInteractiveMapExtractor(pattern)))\n",
        "\n",
        "        # Always add enhanced diocese card extractor as fallback for Salt Lake City style sites\n",
        "        if not any(name == 'EnhancedDiocesesCardExtractor' for name, _ in extractors_to_try):\n",
        "            extractors_to_try.append(('EnhancedDiocesesCardExtractor', EnhancedDiocesesCardExtractor(pattern)))\n",
        "\n",
        "        # Add other fallback extractors\n",
        "        extractors_to_try.extend([\n",
        "            ('TableExtractor', TableExtractor(pattern)),\n",
        "            ('ImprovedInteractiveMapExtractor', ImprovedInteractiveMapExtractor(pattern)),\n",
        "            ('ImprovedGenericExtractor', ImprovedGenericExtractor(pattern))\n",
        "        ])\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen_extractors = set()\n",
        "        unique_extractors = []\n",
        "        for name, extractor in extractors_to_try:\n",
        "            if name not in seen_extractors:\n",
        "                unique_extractors.append((name, extractor))\n",
        "                seen_extractors.add(name)\n",
        "\n",
        "        # Try each extractor until we find parishes\n",
        "        for extractor_name, extractor in unique_extractors:\n",
        "            try:\n",
        "                print(f\"  ðŸ”„ Trying {extractor_name}...\")\n",
        "                current_parishes = extractor.extract(driver, soup, parish_directory_url)\n",
        "\n",
        "                if current_parishes:\n",
        "                    parishes.extend(current_parishes)\n",
        "                    result['extraction_methods_used'].append(extractor_name)\n",
        "                    print(f\"    âœ… {extractor_name} found {len(current_parishes)} parishes\")\n",
        "\n",
        "                    # If we found parishes with the enhanced extractor, we got detailed info\n",
        "                    if extractor_name == 'EnhancedDiocesesCardExtractor' and len(parishes) > 5:\n",
        "                        print(f\"    ðŸŽ¯ Using enhanced extractor - detailed information will be extracted\")\n",
        "                        break\n",
        "\n",
        "                    # If we found a good number of parishes with any method, stop\n",
        "                    if len(parishes) > 10:\n",
        "                        break\n",
        "                else:\n",
        "                    print(f\"    âš ï¸ {extractor_name} found no parishes\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    âŒ {extractor_name} failed: {str(e)[:100]}\")\n",
        "                result['errors'].append(f\"{extractor_name}: {str(e)[:100]}\")\n",
        "\n",
        "        # Step 4: Process results and calculate statistics\n",
        "        if parishes:\n",
        "            # Remove duplicates and validate\n",
        "            unique_parishes = []\n",
        "            seen_names = set()\n",
        "\n",
        "            for parish in parishes:\n",
        "                name_key = parish.name.lower().strip()\n",
        "                if name_key not in seen_names and len(parish.name) > 2:\n",
        "                    # Set the source URLs for each parish\n",
        "                    parish.diocese_url = diocese_url\n",
        "                    parish.parish_directory_url = parish_directory_url\n",
        "                    unique_parishes.append(parish)\n",
        "                    seen_names.add(name_key)\n",
        "\n",
        "            result['parishes_found'] = unique_parishes\n",
        "            result['success'] = True\n",
        "\n",
        "            # Calculate detailed extraction statistics\n",
        "            total_parishes = len(unique_parishes)\n",
        "            detailed_successful = sum(1 for p in unique_parishes if p.detail_extraction_success)\n",
        "            detailed_failed = sum(1 for p in unique_parishes if hasattr(p, 'detail_extraction_success') and not p.detail_extraction_success)\n",
        "\n",
        "            result['detail_extraction_stats'] = {\n",
        "                'attempted': total_parishes,\n",
        "                'successful': detailed_successful,\n",
        "                'failed': detailed_failed,\n",
        "                'success_rate': (detailed_successful / total_parishes * 100) if total_parishes > 0 else 0\n",
        "            }\n",
        "\n",
        "            # Calculate field extraction statistics\n",
        "            result['field_extraction_stats'] = {\n",
        "                'addresses_extracted': sum(1 for p in unique_parishes if p.street_address or p.full_address),\n",
        "                'phones_extracted': sum(1 for p in unique_parishes if p.phone),\n",
        "                'websites_extracted': sum(1 for p in unique_parishes if p.website),\n",
        "                'zip_codes_extracted': sum(1 for p in unique_parishes if p.zip_code),\n",
        "                'clergy_info_extracted': sum(1 for p in unique_parishes if p.clergy_info),\n",
        "                'service_times_extracted': sum(1 for p in unique_parishes if p.service_times)\n",
        "            }\n",
        "\n",
        "            print(f\"  âœ… Found {len(unique_parishes)} unique parishes\")\n",
        "            print(f\"  ðŸ“Š Detail extraction: {detailed_successful}/{total_parishes} successful ({result['detail_extraction_stats']['success_rate']:.1f}%)\")\n",
        "\n",
        "            # Show field extraction summary\n",
        "            field_stats = result['field_extraction_stats']\n",
        "            print(f\"  ðŸ“‹ Field extraction summary:\")\n",
        "            print(f\"      ðŸ“ Addresses: {field_stats['addresses_extracted']}/{total_parishes}\")\n",
        "            print(f\"      ðŸ“ž Phones: {field_stats['phones_extracted']}/{total_parishes}\")\n",
        "            print(f\"      ðŸŒ Websites: {field_stats['websites_extracted']}/{total_parishes}\")\n",
        "            print(f\"      ðŸ“® Zip Codes: {field_stats['zip_codes_extracted']}/{total_parishes}\")\n",
        "            print(f\"      ðŸ‘¥ Clergy Info: {field_stats['clergy_info_extracted']}/{total_parishes}\")\n",
        "            print(f\"      â° Service Times: {field_stats['service_times_extracted']}/{total_parishes}\")\n",
        "\n",
        "            # Step 5: Save to database\n",
        "            if unique_parishes:\n",
        "                print(\"  ðŸ’¾ Saving enhanced parish data to database...\")\n",
        "                enhanced_safe_upsert_to_supabase(unique_parishes, diocese_name, diocese_url, parish_directory_url)\n",
        "\n",
        "        else:\n",
        "            print(\"  âŒ No parishes found with any extraction method\")\n",
        "            result['success'] = False\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        result['errors'].append(error_msg)\n",
        "        print(f\"  âŒ Processing error: {error_msg}\")\n",
        "\n",
        "    finally:\n",
        "        result['processing_time'] = time.time() - start_time\n",
        "        print(f\"  â±ï¸ Completed in {result['processing_time']:.1f}s\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Enhanced master processing function loaded with detailed parish extraction\")"
      ],
      "metadata": {
        "id": "mJ9oxFJqtW97",
        "outputId": "53e63eab-14df-435d-a7ba-01dfc97dc95d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enhanced master processing function loaded with detailed parish extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Main Execution Using Existing Parish Directory URLs (MODIFIED TO EXCLUDE PROCESSED DIOCESES)\n",
        "# =============================================================================\n",
        "\n",
        "# Get dioceses WITH their parish directory URLs from your existing data\n",
        "if supabase:\n",
        "    try:\n",
        "        print(\"ðŸ“¥ Fetching dioceses with parish directory URLs from database...\")\n",
        "\n",
        "        # Join dioceses with their parish directory URLs\n",
        "        response = supabase.table('DiocesesParishDirectory').select(\n",
        "            'diocese_url, parish_directory_url'\n",
        "        ).not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "        diocese_directory_data = response.data if response.data else []\n",
        "        print(f\"ðŸ“Š Found {len(diocese_directory_data)} dioceses with parish directory URLs\")\n",
        "\n",
        "        # Get diocese names from the main table\n",
        "        if diocese_directory_data:\n",
        "            diocese_urls = [item['diocese_url'] for item in diocese_directory_data]\n",
        "\n",
        "            # Get diocese names for these URLs\n",
        "            diocese_names_response = supabase.table('Dioceses').select(\n",
        "                'Website, Name'\n",
        "            ).in_('Website', diocese_urls).execute()\n",
        "\n",
        "            diocese_names_data = diocese_names_response.data if diocese_names_response.data else []\n",
        "\n",
        "            # Create a mapping of URL to name\n",
        "            url_to_name = {item['Website']: item['Name'] for item in diocese_names_data}\n",
        "\n",
        "            # NEW: Check which dioceses already have 5+ parishes in the database\n",
        "            print(\"ðŸ” Checking for dioceses that already have 5+ parishes extracted...\")\n",
        "\n",
        "            # Get all parishes with their parish_directory_url\n",
        "            parish_counts_response = supabase.table('Parishes').select(\n",
        "                'parish_directory_url'\n",
        "            ).not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "            parish_counts_data = parish_counts_response.data if parish_counts_response.data else []\n",
        "\n",
        "            # Count parishes by directory URL\n",
        "            directory_url_counts = {}\n",
        "            for parish in parish_counts_data:\n",
        "                directory_url = parish.get('parish_directory_url')\n",
        "                if directory_url:\n",
        "                    directory_url_counts[directory_url] = directory_url_counts.get(directory_url, 0) + 1\n",
        "\n",
        "            # Combine the data and filter out already processed dioceses\n",
        "            dioceses_to_process = []\n",
        "            dioceses_skipped = []\n",
        "            first_kept_shown = False\n",
        "\n",
        "            for item in diocese_directory_data:\n",
        "                diocese_url = item['diocese_url']\n",
        "                parish_directory_url = item['parish_directory_url']\n",
        "                diocese_name = url_to_name.get(diocese_url, 'Unknown Diocese')\n",
        "\n",
        "                # Check if this diocese already has 5+ parishes\n",
        "                existing_parish_count = directory_url_counts.get(parish_directory_url, 0)\n",
        "\n",
        "                if existing_parish_count >= 5:\n",
        "                    dioceses_skipped.append({\n",
        "                        'name': diocese_name,\n",
        "                        'url': diocese_url,\n",
        "                        'parish_directory_url': parish_directory_url,\n",
        "                        'existing_parish_count': existing_parish_count\n",
        "                    })\n",
        "                    print(f\"  â­ï¸ SKIPPED {diocese_name}: {existing_parish_count} parishes already extracted\")\n",
        "                else:\n",
        "                    dioceses_to_process.append({\n",
        "                        'name': diocese_name,\n",
        "                        'url': diocese_url,\n",
        "                        'parish_directory_url': parish_directory_url,\n",
        "                        'existing_parish_count': existing_parish_count\n",
        "                    })\n",
        "                    if not first_kept_shown:\n",
        "                        print(f\"  âœ… FIRST TO PROCESS: {diocese_name} ({existing_parish_count} existing parishes)\")\n",
        "                        first_kept_shown = True\n",
        "\n",
        "            print(f\"\\nðŸ“Š FILTERING RESULTS:\")\n",
        "            print(f\"  - Dioceses to process: {len(dioceses_to_process)}\")\n",
        "            print(f\"  - Dioceses skipped (5+ parishes): {len(dioceses_skipped)}\")\n",
        "\n",
        "            # MODIFIED: Take the first N dioceses instead of random sampling\n",
        "            if len(dioceses_to_process) > MAX_DIOCESES_TO_PROCESS:\n",
        "                print(f\"ðŸ“Š Taking first {MAX_DIOCESES_TO_PROCESS} dioceses from {len(dioceses_to_process)} available\")\n",
        "                dioceses_to_process = dioceses_to_process[:MAX_DIOCESES_TO_PROCESS]\n",
        "            else:\n",
        "                print(f\"ðŸ“Š Processing all {len(dioceses_to_process)} available dioceses\")\n",
        "\n",
        "            print(f\"ðŸ“Š Selected {len(dioceses_to_process)} dioceses for enhanced processing\")\n",
        "\n",
        "            # Display what we're about to process\n",
        "            if dioceses_to_process:\n",
        "                print(f\"\\nðŸ“‹ Dioceses to process:\")\n",
        "                for i, diocese in enumerate(dioceses_to_process, 1):\n",
        "                    existing_count = diocese.get('existing_parish_count', 0)\n",
        "                    existing_info = f\" ({existing_count} existing)\" if existing_count > 0 else \" (new)\"\n",
        "                    print(f\"  {i}. {diocese['name']}{existing_info}\")\n",
        "                    print(f\"     Main URL: {diocese['url']}\")\n",
        "                    print(f\"     Parish Directory: {diocese['parish_directory_url']}\")\n",
        "\n",
        "            # Display summary of skipped dioceses\n",
        "            if dioceses_skipped:\n",
        "                print(f\"\\nðŸ“‹ Dioceses skipped (already have 5+ parishes):\")\n",
        "                for diocese in dioceses_skipped[:5]:  # Show first 5\n",
        "                    print(f\"  - {diocese['name']}: {diocese['existing_parish_count']} parishes\")\n",
        "                if len(dioceses_skipped) > 5:\n",
        "                    print(f\"  ... and {len(dioceses_skipped) - 5} more\")\n",
        "\n",
        "        else:\n",
        "            dioceses_to_process = []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error fetching dioceses with parish directories: {e}\")\n",
        "        dioceses_to_process = []\n",
        "else:\n",
        "    print(\"âŒ No Supabase connection, using test data\")\n",
        "    dioceses_to_process = []\n",
        "\n",
        "print(\"âœ… Enhanced processing function loaded (MODIFIED to exclude dioceses with 5+ existing parishes)\")"
      ],
      "metadata": {
        "id": "lPgmR8FFtXax",
        "outputId": "4e1fa7b1-de84-4e03-e4e1-7c654b57f28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Fetching dioceses with parish directory URLs from database...\n",
            "ðŸ“Š Found 192 dioceses with parish directory URLs\n",
            "ðŸ” Checking for dioceses that already have 5+ parishes extracted...\n",
            "  â­ï¸ SKIPPED Diocese of Salt Lake City: 79 parishes already extracted\n",
            "  âœ… FIRST TO PROCESS: Eparchy of Parma (0 existing parishes)\n",
            "\n",
            "ðŸ“Š FILTERING RESULTS:\n",
            "  - Dioceses to process: 191\n",
            "  - Dioceses skipped (5+ parishes): 1\n",
            "ðŸ“Š Taking first 1 dioceses from 191 available\n",
            "ðŸ“Š Selected 1 dioceses for enhanced processing\n",
            "\n",
            "ðŸ“‹ Dioceses to process:\n",
            "  1. Eparchy of Parma (new)\n",
            "     Main URL: http://www.parma.org/\n",
            "     Parish Directory: https://www.parma.org/parishfinder\n",
            "\n",
            "ðŸ“‹ Dioceses skipped (already have 5+ parishes):\n",
            "  - Diocese of Salt Lake City: 79 parishes\n",
            "âœ… Enhanced processing function loaded (MODIFIED to exclude dioceses with 5+ existing parishes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Execute Enhanced Processing with Detail Extraction (UPDATED)\n",
        "# =============================================================================\n",
        "\n",
        "if dioceses_to_process:\n",
        "    print(f\"\\nðŸš€ Starting ENHANCED pattern-based processing with DETAILED parish extraction...\")\n",
        "    print(f\"ðŸ“‹ This will click on each parish card to extract complete information:\")\n",
        "    print(f\"    ðŸ“ Street addresses and zip codes\")\n",
        "    print(f\"    ðŸ“ž Phone numbers\")\n",
        "    print(f\"    ðŸŒ Parish websites\")\n",
        "    print(f\"    ðŸ‘¥ Clergy information\")\n",
        "    print(f\"    â° Service times and schedules\")\n",
        "\n",
        "    # Initialize driver\n",
        "    driver = setup_enhanced_driver()\n",
        "\n",
        "    # Track enhanced results\n",
        "    all_results = []\n",
        "    summary_stats = {\n",
        "        'total_dioceses': len(dioceses_to_process),\n",
        "        'successful_extractions': 0,\n",
        "        'total_parishes_found': 0,\n",
        "        'pattern_distribution': {},\n",
        "        'extraction_method_usage': {},\n",
        "        'average_confidence': 0.0,\n",
        "        # Enhanced statistics\n",
        "        'total_detail_extractions_attempted': 0,\n",
        "        'total_detail_extractions_successful': 0,\n",
        "        'overall_detail_success_rate': 0.0,\n",
        "        'field_extraction_totals': {\n",
        "            'addresses_extracted': 0,\n",
        "            'phones_extracted': 0,\n",
        "            'websites_extracted': 0,\n",
        "            'zip_codes_extracted': 0,\n",
        "            'clergy_info_extracted': 0,\n",
        "            'service_times_extracted': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for i, diocese_info in enumerate(dioceses_to_process, 1):\n",
        "            print(f\"\\nðŸ“ Diocese {i}/{len(dioceses_to_process)}\")\n",
        "\n",
        "            # Process with enhanced detailed extraction system\n",
        "            result = process_diocese_with_detailed_extraction(diocese_info, driver)\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Update summary statistics\n",
        "            if result['success']:\n",
        "                summary_stats['successful_extractions'] += 1\n",
        "                summary_stats['total_parishes_found'] += len(result['parishes_found'])\n",
        "\n",
        "                # Enhanced statistics tracking\n",
        "                detail_stats = result.get('detail_extraction_stats', {})\n",
        "                summary_stats['total_detail_extractions_attempted'] += detail_stats.get('attempted', 0)\n",
        "                summary_stats['total_detail_extractions_successful'] += detail_stats.get('successful', 0)\n",
        "\n",
        "                # Field extraction statistics\n",
        "                field_stats = result.get('field_extraction_stats', {})\n",
        "                for field, count in field_stats.items():\n",
        "                    if field in summary_stats['field_extraction_totals']:\n",
        "                        summary_stats['field_extraction_totals'][field] += count\n",
        "\n",
        "                # Track pattern distribution\n",
        "                if result['pattern_detected']:\n",
        "                    pattern_key = f\"{result['pattern_detected']['platform']}_{result['pattern_detected']['listing_type']}\"\n",
        "                    summary_stats['pattern_distribution'][pattern_key] = summary_stats['pattern_distribution'].get(pattern_key, 0) + 1\n",
        "\n",
        "                # Track extraction methods\n",
        "                for method in result['extraction_methods_used']:\n",
        "                    summary_stats['extraction_method_usage'][method] = summary_stats['extraction_method_usage'].get(method, 0) + 1\n",
        "\n",
        "            # Be respectful - pause between requests (longer pause due to detail extraction)\n",
        "            if i < len(dioceses_to_process):\n",
        "                time.sleep(3)\n",
        "\n",
        "    finally:\n",
        "        # Clean up\n",
        "        driver.quit()\n",
        "        print(\"\\nðŸ§¹ WebDriver closed\")\n",
        "\n",
        "    # Calculate final enhanced statistics\n",
        "    if summary_stats['successful_extractions'] > 0:\n",
        "        summary_stats['success_rate'] = (summary_stats['successful_extractions'] / summary_stats['total_dioceses']) * 100\n",
        "        summary_stats['avg_parishes_per_diocese'] = summary_stats['total_parishes_found'] / summary_stats['successful_extractions']\n",
        "\n",
        "    if summary_stats['total_detail_extractions_attempted'] > 0:\n",
        "        summary_stats['overall_detail_success_rate'] = (summary_stats['total_detail_extractions_successful'] / summary_stats['total_detail_extractions_attempted']) * 100\n",
        "\n",
        "    # =============================================================================\n",
        "    # ENHANCED RESULTS DISPLAY\n",
        "    # =============================================================================\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š ENHANCED DETAILED EXTRACTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total dioceses processed: {summary_stats['total_dioceses']}\")\n",
        "    print(f\"Successful extractions: {summary_stats['successful_extractions']}\")\n",
        "    print(f\"Success rate: {summary_stats.get('success_rate', 0):.1f}%\")\n",
        "    print(f\"Total parishes found: {summary_stats['total_parishes_found']}\")\n",
        "    if summary_stats['successful_extractions'] > 0:\n",
        "        print(f\"Average parishes per diocese: {summary_stats.get('avg_parishes_per_diocese', 0):.1f}\")\n",
        "\n",
        "    print(f\"\\nðŸ“ DETAILED EXTRACTION PERFORMANCE:\")\n",
        "    print(f\"Detail extractions attempted: {summary_stats['total_detail_extractions_attempted']}\")\n",
        "    print(f\"Detail extractions successful: {summary_stats['total_detail_extractions_successful']}\")\n",
        "    print(f\"Overall detail success rate: {summary_stats['overall_detail_success_rate']:.1f}%\")\n",
        "\n",
        "    print(f\"\\nðŸ“‹ FIELD EXTRACTION TOTALS:\")\n",
        "    field_totals = summary_stats['field_extraction_totals']\n",
        "    total_parishes = summary_stats['total_parishes_found']\n",
        "    if total_parishes > 0:\n",
        "        print(f\"  ðŸ“ Street Addresses: {field_totals['addresses_extracted']}/{total_parishes} ({field_totals['addresses_extracted']/total_parishes*100:.1f}%)\")\n",
        "        print(f\"  ðŸ“ž Phone Numbers: {field_totals['phones_extracted']}/{total_parishes} ({field_totals['phones_extracted']/total_parishes*100:.1f}%)\")\n",
        "        print(f\"  ðŸŒ Websites: {field_totals['websites_extracted']}/{total_parishes} ({field_totals['websites_extracted']/total_parishes*100:.1f}%)\")\n",
        "        print(f\"  ðŸ“® Zip Codes: {field_totals['zip_codes_extracted']}/{total_parishes} ({field_totals['zip_codes_extracted']/total_parishes*100:.1f}%)\")\n",
        "        print(f\"  ðŸ‘¥ Clergy Info: {field_totals['clergy_info_extracted']}/{total_parishes} ({field_totals['clergy_info_extracted']/total_parishes*100:.1f}%)\")\n",
        "        print(f\"  â° Service Times: {field_totals['service_times_extracted']}/{total_parishes} ({field_totals['service_times_extracted']/total_parishes*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Pattern Distribution:\")\n",
        "    for pattern, count in summary_stats['pattern_distribution'].items():\n",
        "        percentage = (count / summary_stats['total_dioceses']) * 100\n",
        "        print(f\"  {pattern.replace('_', ' ').title()}: {count} dioceses ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nðŸ”§ Extraction Method Usage:\")\n",
        "    for method, count in summary_stats['extraction_method_usage'].items():\n",
        "        enhanced_indicator = \"ðŸ”¥\" if \"Enhanced\" in method else \"\"\n",
        "        print(f\"  {enhanced_indicator} {method}: {count} times\")\n",
        "\n",
        "    print(f\"\\nðŸ” Detailed Results:\")\n",
        "    for result in all_results:\n",
        "        status = \"âœ…\" if result['success'] else \"âŒ\"\n",
        "        parish_count = len(result['parishes_found'])\n",
        "\n",
        "        # Detail extraction summary\n",
        "        detail_stats = result.get('detail_extraction_stats', {})\n",
        "        detail_success = detail_stats.get('successful', 0)\n",
        "        detail_total = detail_stats.get('attempted', 0)\n",
        "        detail_rate = detail_stats.get('success_rate', 0)\n",
        "\n",
        "        pattern_info = \"\"\n",
        "        if result['pattern_detected']:\n",
        "            pattern_info = f\" [{result['pattern_detected']['platform']} / {result['pattern_detected']['listing_type']}]\"\n",
        "\n",
        "        print(f\"  {status} {result['diocese_name']}: {parish_count} parishes{pattern_info}\")\n",
        "        print(f\"      ðŸ“Š Detail extraction: {detail_success}/{detail_total} successful ({detail_rate:.1f}%)\")\n",
        "        print(f\"      ðŸŒ Main URL: {result['diocese_url']}\")\n",
        "        print(f\"      ðŸ“‚ Parish Directory: {result['parish_directory_url']}\")\n",
        "\n",
        "        if result['extraction_methods_used']:\n",
        "            methods = ', '.join(result['extraction_methods_used'])\n",
        "            print(f\"      ðŸ”§ Methods: {methods}\")\n",
        "\n",
        "        # Show field extraction summary for this diocese\n",
        "        field_stats = result.get('field_extraction_stats', {})\n",
        "        if any(field_stats.values()):\n",
        "            field_summary = []\n",
        "            if field_stats.get('addresses_extracted', 0) > 0:\n",
        "                field_summary.append(f\"ðŸ“{field_stats['addresses_extracted']} addresses\")\n",
        "            if field_stats.get('phones_extracted', 0) > 0:\n",
        "                field_summary.append(f\"ðŸ“ž{field_stats['phones_extracted']} phones\")\n",
        "            if field_stats.get('websites_extracted', 0) > 0:\n",
        "                field_summary.append(f\"ðŸŒ{field_stats['websites_extracted']} websites\")\n",
        "            if field_stats.get('zip_codes_extracted', 0) > 0:\n",
        "                field_summary.append(f\"ðŸ“®{field_stats['zip_codes_extracted']} zips\")\n",
        "\n",
        "            if field_summary:\n",
        "                print(f\"      ðŸ“‹ Fields: {', '.join(field_summary[:4])}\")\n",
        "\n",
        "        if result['errors']:\n",
        "            for error in result['errors']:\n",
        "                print(f\"      âŒ Error: {error[:100]}...\")\n",
        "        print()  # Add blank line between dioceses\n",
        "\n",
        "    # Save enhanced summary to file\n",
        "    summary_filename = f\"enhanced_extraction_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(summary_filename, 'w') as f:\n",
        "        json.dump({\n",
        "            'summary_stats': summary_stats,\n",
        "            'detailed_results': all_results\n",
        "        }, f, indent=2, default=str)\n",
        "    print(f\"ðŸ’¾ Enhanced detailed results saved to: {summary_filename}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No dioceses with parish directory URLs found to process\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Enhanced pattern-based extraction with DETAILED parish information complete!\")\n",
        "print(f\"ðŸ“Š Successfully extracted detailed information for parishes including addresses, phones, websites, and more!\")\n",
        "print(f\"{'='*70}\")"
      ],
      "metadata": {
        "id": "T0E2QoFmtaR6",
        "outputId": "4187fbb0-34b3-41e9-d663-93fda97864aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting ENHANCED pattern-based processing with DETAILED parish extraction...\n",
            "ðŸ“‹ This will click on each parish card to extract complete information:\n",
            "    ðŸ“ Street addresses and zip codes\n",
            "    ðŸ“ž Phone numbers\n",
            "    ðŸŒ Parish websites\n",
            "    ðŸ‘¥ Clergy information\n",
            "    â° Service times and schedules\n",
            "ðŸ”§ Setting up enhanced Chrome WebDriver...\n",
            "âœ… Chrome WebDriver initialized successfully\n",
            "\n",
            "ðŸ“ Diocese 1/1\n",
            "\n",
            "============================================================\n",
            "ðŸ” ENHANCED DETAILED PROCESSING: Eparchy of Parma\n",
            "ðŸ“ Main URL: http://www.parma.org/\n",
            "ðŸ“‚ Parish Directory URL: https://www.parma.org/parishfinder\n",
            "============================================================\n",
            "  ðŸ“¥ Loading parish directory page...\n",
            "  ðŸ” Detecting website pattern...\n",
            "    ðŸ“‹ Platform: ecatholic\n",
            "    ðŸ“Š Listing Type: parish_finder\n",
            "    ðŸŽ¯ Confidence: 0.95\n",
            "    âš™ï¸ Method: parish_finder_extraction\n",
            "  ðŸ”„ Trying ParishFinderExtractor...\n",
            "    ðŸ“ Parish finder interface detected\n",
            "    ðŸ“Š Found 31 parish elements using li.site\n",
            "    âš ï¸ ParishFinderExtractor found no parishes\n",
            "  ðŸ”„ Trying EnhancedDiocesesCardExtractor...\n",
            "    ðŸ“ Enhanced diocese card layout detected - extracting with detail pages\n",
            "    ðŸ“Š Found 0 parish cards\n",
            "    ðŸ“Š Summary: 0 detailed extractions successful, 0 failed\n",
            "    âš ï¸ EnhancedDiocesesCardExtractor found no parishes\n",
            "  ðŸ”„ Trying TableExtractor...\n",
            "    âš ï¸ TableExtractor found no parishes\n",
            "  ðŸ”„ Trying ImprovedInteractiveMapExtractor...\n",
            "    ðŸ“ Found map container: #map\n",
            "    â„¹ï¸ No clickable markers found\n",
            "    âš ï¸ ImprovedInteractiveMapExtractor found no parishes\n",
            "  ðŸ”„ Trying ImprovedGenericExtractor...\n",
            "    ðŸ” Using improved generic extraction...\n",
            "    âš ï¸ ImprovedGenericExtractor found no parishes\n",
            "  âŒ No parishes found with any extraction method\n",
            "  â±ï¸ Completed in 39.5s\n",
            "\n",
            "ðŸ§¹ WebDriver closed\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š ENHANCED DETAILED EXTRACTION SUMMARY\n",
            "======================================================================\n",
            "Total dioceses processed: 1\n",
            "Successful extractions: 0\n",
            "Success rate: 0.0%\n",
            "Total parishes found: 0\n",
            "\n",
            "ðŸ“ DETAILED EXTRACTION PERFORMANCE:\n",
            "Detail extractions attempted: 0\n",
            "Detail extractions successful: 0\n",
            "Overall detail success rate: 0.0%\n",
            "\n",
            "ðŸ“‹ FIELD EXTRACTION TOTALS:\n",
            "\n",
            "ðŸ“ˆ Pattern Distribution:\n",
            "\n",
            "ðŸ”§ Extraction Method Usage:\n",
            "\n",
            "ðŸ” Detailed Results:\n",
            "  âŒ Eparchy of Parma: 0 parishes [ecatholic / parish_finder]\n",
            "      ðŸ“Š Detail extraction: 0/0 successful (0.0%)\n",
            "      ðŸŒ Main URL: http://www.parma.org/\n",
            "      ðŸ“‚ Parish Directory: https://www.parma.org/parishfinder\n",
            "\n",
            "ðŸ’¾ Enhanced detailed results saved to: enhanced_extraction_summary_20250529_011259.json\n",
            "\n",
            "ðŸŽ‰ Enhanced pattern-based extraction with DETAILED parish information complete!\n",
            "ðŸ“Š Successfully extracted detailed information for parishes including addresses, phones, websites, and more!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # =============================================================================\n",
        "    # CELL 9: Display Results and Analysis (UPDATED)\n",
        "    # =============================================================================\n",
        "\n",
        "    print(f\"\\\\n{'='*70}\")\n",
        "    print(f\"ðŸ“Š ENHANCED EXTRACTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total dioceses processed: {summary_stats['total_dioceses']}\")\n",
        "    print(f\"Successful extractions: {summary_stats['successful_extractions']}\")\n",
        "    print(f\"Success rate: {summary_stats.get('success_rate', 0):.1f}%\")\n",
        "    print(f\"Total parishes found: {summary_stats['total_parishes_found']}\")\n",
        "\n",
        "    if summary_stats['total_dioceses'] > 0:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ðŸ“Š ENHANCED EXTRACTION SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total dioceses processed: {summary_stats['total_dioceses']}\")\n",
        "        print(f\"Successful extractions: {summary_stats['successful_extractions']}\")\n",
        "        print(f\"Success rate: {summary_stats.get('success_rate', 0):.1f}%\")\n",
        "        print(f\"Total parishes found: {summary_stats['total_parishes_found']}\")\n",
        "        if summary_stats['successful_extractions'] > 0:\n",
        "            print(f\"Average parishes per diocese: {summary_stats.get('avg_parishes_per_diocese', 0):.1f}\")\n",
        "        print(f\"\\n\\nðŸ“ˆ Pattern Distribution:\")\n",
        "        for pattern, count in summary_stats['pattern_distribution'].items():\n",
        "            percentage = (count / summary_stats['total_dioceses']) * 100\n",
        "            print(f\"  {pattern.replace('_', ' ').title()}: {count} dioceses ({percentage:.1f}%)\")\n",
        "        print(f\"\\n\\nðŸ”§ Extraction Method Usage:\")\n",
        "        for method, count in summary_stats['extraction_method_usage'].items():\n",
        "            print(f\"  {method}: {count} times\")\n",
        "        print(f\"\\n\\nðŸ” Detailed Results:\")\n",
        "        for result in all_results:\n",
        "            status = \"âœ…\" if result['success'] else \"âŒ\"\n",
        "            parish_count = len(result['parishes_found'])\n",
        "            pattern_info = \"\"\n",
        "            if result['pattern_detected']:\n",
        "                pattern_info = f\" [{result['pattern_detected']['platform']} / {result['pattern_detected']['listing_type']}]\"\n",
        "            print(f\"  {status} {result['diocese_name']}: {parish_count} parishes{pattern_info}\")\n",
        "            print(f\"      Main URL: {result['diocese_url']}\")\n",
        "            print(f\"      Parish Directory: {result['parish_directory_url']}\")\n",
        "            if result['extraction_methods_used']:\n",
        "                methods = ', '.join(result['extraction_methods_used'])\n",
        "                print(f\"      Methods: {methods}\")\n",
        "            if result['errors']:\n",
        "                for error in result['errors']:\n",
        "                    print(f\"      Error: {error[:100]}...\")\n",
        "            print()  # Add blank line between dioceses\n",
        "\n",
        "        # Save summary to file for analysis\n",
        "        summary_filename = f\"extraction_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            json.dump({\n",
        "                'summary_stats': summary_stats,\n",
        "                'detailed_results': [\n",
        "                    [result, result['errors']] if result.get('errors') and len(result['errors']) > 0 else result\n",
        "                    for result in all_results\n",
        "                ]\n",
        "            }, f, indent=2, default=str)\n",
        "        print(f\"ðŸ’¾ Detailed results saved to: {summary_filename}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ No dioceses with parish directory URLs found to process\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Enhanced pattern-based extraction complete!\")\n",
        "    print(f\"{'='*70}\")"
      ],
      "metadata": {
        "id": "ZT9jvX1etexA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64a642e-fac3-42ae-f50b-acfc183a6aba"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n======================================================================\n",
            "ðŸ“Š ENHANCED EXTRACTION SUMMARY\n",
            "======================================================================\n",
            "Total dioceses processed: 1\n",
            "Successful extractions: 1\n",
            "Success rate: 100.0%\n",
            "Total parishes found: 79\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š ENHANCED EXTRACTION SUMMARY\n",
            "======================================================================\n",
            "Total dioceses processed: 1\n",
            "Successful extractions: 1\n",
            "Success rate: 100.0%\n",
            "Total parishes found: 79\n",
            "Average parishes per diocese: 79.0\n",
            "\n",
            "\n",
            "ðŸ“ˆ Pattern Distribution:\n",
            "  Ecatholic Card Grid: 1 dioceses (100.0%)\n",
            "\n",
            "\n",
            "ðŸ”§ Extraction Method Usage:\n",
            "  EnhancedDiocesesCardExtractor: 1 times\n",
            "\n",
            "\n",
            "ðŸ” Detailed Results:\n",
            "  âœ… Diocese of Salt Lake City: 79 parishes [ecatholic / card_grid]\n",
            "      Main URL: http://www.utahcatholicdiocese.org\n",
            "      Parish Directory: http://www.utahcatholicdiocese.org/parishes\n",
            "      Methods: EnhancedDiocesesCardExtractor\n",
            "\n",
            "ðŸ’¾ Detailed results saved to: extraction_summary_20250529_005429.json\n",
            "\n",
            "ðŸŽ‰ Enhanced pattern-based extraction complete!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}