{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Build_Parishes_Database_Using_AgenticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import required code and install packages\n",
        "!pip install supabase google-generativeai psycopg2-binary tenacity selenium webdriver-manager\n",
        "!wget https://raw.githubusercontent.com/tomknightatl/USCCB/main/llm_utils.py"
      ],
      "metadata": {
        "id": "5kZpoTCpVIut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIm-qDFgrqK3"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import required libraries and install Chrome\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Import Selenium for JavaScript-heavy sites\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "\n",
        "# Chrome Installation for Google Colab\n",
        "def ensure_chrome_installed():\n",
        "    \"\"\"Ensures Chrome is installed in the Colab environment.\"\"\"\n",
        "    try:\n",
        "        # Check if Chrome is already available\n",
        "        result = subprocess.run(['which', 'google-chrome'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Chrome is already installed and available.\")\n",
        "            return True\n",
        "\n",
        "        print(\"üîß Chrome not found. Installing Chrome for Selenium...\")\n",
        "\n",
        "        # Install Chrome\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - > /dev/null 2>&1')\n",
        "        os.system('echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list')\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('apt-get install -y google-chrome-stable > /dev/null 2>&1')\n",
        "\n",
        "        # Verify installation\n",
        "        result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ Chrome installed successfully: {result.stdout.strip()}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Chrome installation may have failed.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during Chrome installation: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the installation check\n",
        "print(\"\\nüîß Checking Chrome installation...\")\n",
        "chrome_ready = ensure_chrome_installed()\n",
        "\n",
        "if chrome_ready:\n",
        "    print(\"üöÄ Ready to proceed with Selenium operations!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è You may need to restart the runtime if Chrome installation failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enhanced configuration and setup\n",
        "print(\"=== ENHANCED PARISH DATABASE BUILDER ===\")\n",
        "print(\"--- User Configurable Parameters & Setup ---\")\n",
        "\n",
        "# --- Processing Configuration ---\n",
        "MAX_URLS_TO_PROCESS = 3  # Start small for testing\n",
        "USE_SELENIUM = True  # Enable JavaScript rendering\n",
        "SAVE_DEBUG_FILES = True  # Save scraped content for debugging\n",
        "RETRY_FAILED_URLS = True  # Retry failed URLs with different methods\n",
        "\n",
        "# Create debug directory\n",
        "if SAVE_DEBUG_FILES:\n",
        "    os.makedirs('debug_content', exist_ok=True)\n",
        "    print(\"Debug directory created for saving scraped content\")\n",
        "\n",
        "print(f\"Processing will be limited to {MAX_URLS_TO_PROCESS} URLs.\")\n",
        "print(f\"JavaScript rendering: {'Enabled' if USE_SELENIUM else 'Disabled'}\")\n",
        "print(f\"Debug mode: {'Enabled' if SAVE_DEBUG_FILES else 'Disabled'}\")\n",
        "\n",
        "# --- Supabase Configuration ---\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "supabase: Client = None\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    try:\n",
        "        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "        print(\"‚úì Supabase client initialized successfully\")\n",
        "\n",
        "        # Test connection and check table structure\n",
        "        try:\n",
        "            test_response = supabase.table('Parishes').select('*').limit(1).execute()\n",
        "            print(\"‚úì Parishes table accessible\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Warning: Could not access Parishes table: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error initializing Supabase client: {e}\")\n",
        "        supabase = None\n",
        "else:\n",
        "    print(\"‚úó Supabase credentials not found in secrets\")\n",
        "    print(\"Required secrets: SUPABASE_URL, SUPABASE_KEY\")\n",
        "\n",
        "# --- GenAI Configuration ---\n",
        "GENAI_API_KEY = userdata.get('GENAI_API_KEY_USCCB')\n",
        "\n",
        "if GENAI_API_KEY:\n",
        "    try:\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        # Test the API\n",
        "        test_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        test_response = test_model.generate_content(\"Say 'API working'\")\n",
        "        print(\"‚úì GenAI configured and tested successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error configuring GenAI: {e}\")\n",
        "        GENAI_API_KEY = None\n",
        "else:\n",
        "    print(\"‚úó GenAI API Key not found (Secret: GENAI_API_KEY_USCCB)\")\n",
        "\n",
        "# --- Enhanced Selenium WebDriver Setup ---\n",
        "def setup_webdriver():\n",
        "    if not USE_SELENIUM:\n",
        "        return None\n",
        "\n",
        "    if not chrome_ready:\n",
        "        print(\"‚ö† Chrome not available - skipping Selenium setup\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "        chrome_options.add_argument('--disable-extensions')\n",
        "        chrome_options.add_argument('--disable-plugins')\n",
        "        chrome_options.add_argument('--disable-images')  # Speed up loading\n",
        "        chrome_options.add_argument('--disable-javascript')  # We'll enable selectively\n",
        "\n",
        "        # Try to use system Chrome first\n",
        "        chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "        # Set up ChromeDriver\n",
        "        try:\n",
        "            service = Service(ChromeDriverManager().install())\n",
        "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "            print(\"‚úì Selenium WebDriver initialized with ChromeDriverManager\")\n",
        "            return driver\n",
        "        except Exception as e:\n",
        "            print(f\"ChromeDriverManager failed: {e}\")\n",
        "            # Fallback to system chromedriver\n",
        "            try:\n",
        "                driver = webdriver.Chrome(options=chrome_options)\n",
        "                print(\"‚úì Selenium WebDriver initialized with system chromedriver\")\n",
        "                return driver\n",
        "            except Exception as e2:\n",
        "                print(f\"System chromedriver also failed: {e2}\")\n",
        "                return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Warning: Could not initialize Selenium: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Data Retrieval from Supabase ---\n",
        "urls_to_process = []\n",
        "if supabase:\n",
        "    try:\n",
        "        print(\"\\nFetching parish directory URLs...\")\n",
        "        response = supabase.table('DiocesesParishDirectory').select('parish_directory_url').not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "        if response.data:\n",
        "            fetched_urls = [item['parish_directory_url'] for item in response.data if item['parish_directory_url']]\n",
        "            print(f\"Found {len(fetched_urls)} URLs in database\")\n",
        "\n",
        "            if MAX_URLS_TO_PROCESS and len(fetched_urls) > MAX_URLS_TO_PROCESS:\n",
        "                urls_to_process = random.sample(fetched_urls, MAX_URLS_TO_PROCESS)\n",
        "                print(f\"Selected {len(urls_to_process)} URLs for processing\")\n",
        "            else:\n",
        "                urls_to_process = fetched_urls\n",
        "                print(f\"Will process all {len(urls_to_process)} URLs\")\n",
        "        else:\n",
        "            print(\"No parish directory URLs found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URLs: {e}\")\n",
        "        urls_to_process = []\n",
        "\n",
        "if not urls_to_process:\n",
        "    print(\"\\n‚ö† No URLs to process - using test URLs\")\n",
        "    urls_to_process = [\n",
        "        \"https://www.dioceseoftyler.org/parishes/\",\n",
        "        \"https://www.diopueblo.org/parishes\",\n",
        "        \"http://www.miamiarch.org/CatholicDiocese.php\"\n",
        "    ]\n",
        "\n",
        "print(f\"\\nüìã Ready to process {len(urls_to_process)} URLs\")\n",
        "for i, url in enumerate(urls_to_process, 1):\n",
        "    print(f\"  {i}. {url}\")\n",
        "print(\"--- Setup Complete ---\\n\")"
      ],
      "metadata": {
        "id": "KIVTfVlOrtIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Enhanced web scraping and content extraction functions\n",
        "from llm_utils import invoke_gemini_model\n",
        "\n",
        "def extract_domain(url):\n",
        "    \"\"\"Extract domain from URL\"\"\"\n",
        "    return urlparse(url).netloc\n",
        "\n",
        "def save_debug_content(url, content, method=\"unknown\"):\n",
        "    \"\"\"Save scraped content for debugging\"\"\"\n",
        "    if not SAVE_DEBUG_FILES:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        domain = extract_domain(url).replace('.', '_')\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"debug_content/{domain}_{method}_{timestamp}.html\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"<!-- URL: {url} -->\\n\")\n",
        "            f.write(f\"<!-- Method: {method} -->\\n\")\n",
        "            f.write(f\"<!-- Timestamp: {timestamp} -->\\n\\n\")\n",
        "            f.write(content)\n",
        "\n",
        "        print(f\"  üìÑ Debug content saved: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö† Could not save debug content: {e}\")\n",
        "\n",
        "def scrape_with_requests(url, timeout=10):\n",
        "    \"\"\"Scrape content using requests + BeautifulSoup\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text content\n",
        "        text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "        save_debug_content(url, response.text, \"requests\")\n",
        "\n",
        "        return text_content, len(text_content), \"requests\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Requests failed: {e}\")\n",
        "        return None, 0, \"requests_failed\"\n",
        "\n",
        "def scrape_with_selenium(url, driver, timeout=15):\n",
        "    \"\"\"Scrape content using Selenium for JavaScript rendering\"\"\"\n",
        "    if not driver:\n",
        "        return None, 0, \"selenium_unavailable\"\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for page to load\n",
        "        time.sleep(3)\n",
        "\n",
        "        # Try to wait for common content indicators\n",
        "        try:\n",
        "            WebDriverWait(driver, timeout).until(\n",
        "                lambda d: len(d.find_elements(By.TAG_NAME, \"body\")) > 0\n",
        "            )\n",
        "        except:\n",
        "            pass  # Continue anyway\n",
        "\n",
        "        # Get page source after JavaScript execution\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "        save_debug_content(url, page_source, \"selenium\")\n",
        "\n",
        "        return text_content, len(text_content), \"selenium\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Selenium failed: {e}\")\n",
        "        return None, 0, \"selenium_failed\"\n",
        "\n",
        "def enhanced_content_extraction(url, driver=None):\n",
        "    \"\"\"Try multiple methods to extract content from a URL\"\"\"\n",
        "    print(f\"\\nüîç Extracting content from: {url}\")\n",
        "\n",
        "    methods_tried = []\n",
        "    best_content = None\n",
        "    best_length = 0\n",
        "    best_method = None\n",
        "\n",
        "    # Method 1: Try requests first (faster)\n",
        "    print(\"  üì° Trying requests method...\")\n",
        "    content, length, method = scrape_with_requests(url)\n",
        "    methods_tried.append((content, length, method))\n",
        "\n",
        "    if content and length > best_length:\n",
        "        best_content, best_length, best_method = content, length, method\n",
        "        print(f\"  ‚úì Requests success: {length} characters\")\n",
        "\n",
        "    # Method 2: Try Selenium if requests failed or content is too short\n",
        "    if USE_SELENIUM and (not content or length < 1000):\n",
        "        print(\"  üåê Trying Selenium method...\")\n",
        "        content, length, method = scrape_with_selenium(url, driver)\n",
        "        methods_tried.append((content, length, method))\n",
        "\n",
        "        if content and length > best_length:\n",
        "            best_content, best_length, best_method = content, length, method\n",
        "            print(f\"  ‚úì Selenium success: {length} characters\")\n",
        "\n",
        "    if best_content:\n",
        "        print(f\"  üéØ Best method: {best_method} ({best_length} characters)\")\n",
        "        return best_content[:50000]  # Limit content length for API\n",
        "    else:\n",
        "        print(f\"  ‚úó All methods failed for {url}\")\n",
        "        return None\n",
        "\n",
        "print(\"Enhanced scraping functions loaded!\")"
      ],
      "metadata": {
        "id": "xlQd-ThXruqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Enhanced Gemini processing with better prompts\n",
        "\n",
        "def create_enhanced_prompt(url, content):\n",
        "    \"\"\"Create a more detailed prompt for better parish extraction\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert at extracting Catholic parish information from web content.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. The URL {url} contains a parish directory or parish listing page\n",
        "2. Extract information about Catholic parishes from the provided content\n",
        "3. If the page contains MULTIPLE parishes, extract data for ALL parishes found\n",
        "4. If the page is a directory/landing page with NO specific parish details, return an empty array\n",
        "5. Look for parishes, churches, missions, and Catholic communities\n",
        "6. Return ONLY valid JSON - no explanatory text before or after\n",
        "\n",
        "EXPECTED OUTPUT FORMAT:\n",
        "For multiple parishes, return a JSON array:\n",
        "[\n",
        "  {{\n",
        "    \"Name\": \"Parish Name\",\n",
        "    \"Status\": \"Parish/Mission/Chapel\",\n",
        "    \"Deanery\": \"Deanery Name\",\n",
        "    \"EST\": \"Established Year\",\n",
        "    \"Street Address\": \"Street Address\",\n",
        "    \"City\": \"City\",\n",
        "    \"State\": \"State\",\n",
        "    \"Zipcode\": \"Zipcode\",\n",
        "    \"Phone Number\": \"Phone\",\n",
        "    \"Website\": \"URL\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "For a single parish, return a JSON object:\n",
        "{{\n",
        "  \"Name\": \"Parish Name\",\n",
        "  \"Status\": \"Parish/Mission/Chapel\",\n",
        "  \"Deanery\": \"Deanery Name\",\n",
        "  \"EST\": \"Established Year\",\n",
        "  \"Street Address\": \"Street Address\",\n",
        "  \"City\": \"City\",\n",
        "  \"State\": \"State\",\n",
        "  \"Zipcode\": \"Zipcode\",\n",
        "  \"Phone Number\": \"Phone\",\n",
        "  \"Website\": \"URL\"\n",
        "}}\n",
        "\n",
        "If no parish information is found, return: []\n",
        "\n",
        "Use null for missing values. Extract phone numbers, websites, and addresses carefully.\n",
        "\n",
        "WEBPAGE CONTENT:\n",
        "{content[:45000]}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def process_url_with_enhanced_gemini(url, content):\n",
        "    \"\"\"Process URL content with enhanced Gemini prompting\"\"\"\n",
        "\n",
        "    if not content:\n",
        "        print(f\"  ‚úó No content to process for {url}\")\n",
        "        return None\n",
        "\n",
        "    if not GENAI_API_KEY:\n",
        "        print(f\"  ‚úó GenAI not configured - skipping LLM processing\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"  ü§ñ Processing with Gemini... ({len(content)} chars)\")\n",
        "\n",
        "        prompt = create_enhanced_prompt(url, content)\n",
        "        response_text = invoke_gemini_model(prompt_text=prompt, model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "        print(f\"  üìù Gemini response length: {len(response_text)} characters\")\n",
        "\n",
        "        # Save raw response for debugging\n",
        "        if SAVE_DEBUG_FILES:\n",
        "            domain = extract_domain(url).replace('.', '_')\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            response_file = f\"debug_content/{domain}_gemini_response_{timestamp}.json\"\n",
        "            with open(response_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(response_text)\n",
        "            print(f\"  üìÑ Raw response saved: {response_file}\")\n",
        "\n",
        "        # Clean up response\n",
        "        cleaned_response = response_text.strip()\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        if cleaned_response.startswith(\"```json\"):\n",
        "            cleaned_response = cleaned_response[7:]\n",
        "        if cleaned_response.startswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[3:]\n",
        "        if cleaned_response.endswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "        cleaned_response = cleaned_response.strip()\n",
        "\n",
        "        # Parse JSON\n",
        "        try:\n",
        "            parsed_data = json.loads(cleaned_response)\n",
        "\n",
        "            # Handle both single objects and arrays\n",
        "            if isinstance(parsed_data, list):\n",
        "                print(f\"  ‚úì Extracted {len(parsed_data)} parishes\")\n",
        "                return parsed_data\n",
        "            elif isinstance(parsed_data, dict):\n",
        "                print(f\"  ‚úì Extracted 1 parish: {parsed_data.get('Name', 'Unknown')}\")\n",
        "                return [parsed_data]  # Convert to array for consistent handling\n",
        "            else:\n",
        "                print(f\"  ‚ö† Unexpected data type: {type(parsed_data)}\")\n",
        "                return None\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  ‚úó JSON parsing failed: {e}\")\n",
        "            print(f\"  üìù Response preview: {cleaned_response[:200]}...\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Gemini processing failed: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Enhanced Gemini processing functions loaded!\")"
      ],
      "metadata": {
        "id": "IGR1tf_gXk8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Database operations with exact column mapping\n",
        "\n",
        "def validate_parish_data(parish_data, source_url):\n",
        "    \"\"\"Validate and clean parish data before database insertion\"\"\"\n",
        "\n",
        "    if not isinstance(parish_data, dict):\n",
        "        return None\n",
        "\n",
        "    # Ensure all required fields exist\n",
        "    required_fields = ['Name', 'Status', 'Deanery', 'EST', 'Street Address',\n",
        "                       'City', 'State', 'Zipcode', 'Phone Number', 'Website']\n",
        "\n",
        "    validated_data = {}\n",
        "\n",
        "    for field in required_fields:\n",
        "        value = parish_data.get(field)\n",
        "        # Convert empty strings to None\n",
        "        if value == \"\" or value == \"null\" or value == \"NULL\":\n",
        "            value = None\n",
        "        validated_data[field] = value\n",
        "\n",
        "    # Add metadata\n",
        "    validated_data['source_url'] = source_url\n",
        "    validated_data['domain'] = extract_domain(source_url)\n",
        "    validated_data['processed_at'] = datetime.now().isoformat()\n",
        "\n",
        "    return validated_data\n",
        "\n",
        "def prepare_for_supabase_exact(parish_data):\n",
        "    \"\"\"Convert parish data to match your exact Supabase column names\"\"\"\n",
        "\n",
        "    # Map to your EXACT column names from the schema\n",
        "    return {\n",
        "        'Name': parish_data.get('Name'),\n",
        "        'Status': parish_data.get('Status'),\n",
        "        'Deanery': parish_data.get('Deanery'),\n",
        "        'Street Address': parish_data.get('Street Address'),  # Exact match\n",
        "        'City': parish_data.get('City'),\n",
        "        'State': parish_data.get('State'),\n",
        "        'Zip Code': parish_data.get('Zipcode'),  # Map Zipcode -> Zip Code\n",
        "        'Phone Number': parish_data.get('Phone Number'),  # Exact match\n",
        "        'Web': parish_data.get('Website'),  # Map Website -> Web\n",
        "        # Note: No EST column in your table, so we skip it\n",
        "        # Note: id and created_at are auto-generated by Supabase\n",
        "    }\n",
        "\n",
        "def safe_upsert_to_supabase_exact(parish_data_list, source_url):\n",
        "    \"\"\"Safely insert parish data using exact column mapping\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(f\"  ‚úó Supabase not available - skipping database write\")\n",
        "        return False\n",
        "\n",
        "    if not parish_data_list:\n",
        "        print(f\"  ‚ö† No parish data to save\")\n",
        "        return False\n",
        "\n",
        "    print(f\"  üìã Using exact column mapping for {len(parish_data_list)} parishes\")\n",
        "\n",
        "    success_count = 0\n",
        "\n",
        "    for i, parish_data in enumerate(parish_data_list):\n",
        "        try:\n",
        "            # Validate data\n",
        "            validated_data = validate_parish_data(parish_data, source_url)\n",
        "            if not validated_data:\n",
        "                print(f\"    ‚ö† Skipping invalid parish data #{i+1}\")\n",
        "                continue\n",
        "\n",
        "            # Prepare for Supabase with exact column mapping\n",
        "            supabase_data = prepare_for_supabase_exact(validated_data)\n",
        "\n",
        "            # Skip if no meaningful data (all key fields are null)\n",
        "            key_fields = ['Name', 'Street Address', 'City', 'Phone Number']\n",
        "            if all(supabase_data.get(field) is None for field in key_fields):\n",
        "                print(f\"    ‚ö† Skipping parish #{i+1} - no meaningful data\")\n",
        "                continue\n",
        "\n",
        "            parish_name = supabase_data.get('Name', f'Parish_{i+1}')\n",
        "\n",
        "            # Remove any None values to avoid issues\n",
        "            clean_data = {k: v for k, v in supabase_data.items() if v is not None}\n",
        "\n",
        "            # Simple insert (no upsert since we don't have a unique constraint)\n",
        "            try:\n",
        "                response = supabase.table('Parishes').insert(clean_data).execute()\n",
        "\n",
        "                if hasattr(response, 'error') and response.error:\n",
        "                    print(f\"    ‚úó Database error for {parish_name}: {response.error}\")\n",
        "                else:\n",
        "                    print(f\"    ‚úì Saved: {parish_name}\")\n",
        "                    success_count += 1\n",
        "\n",
        "            except Exception as db_error:\n",
        "                print(f\"    ‚úó Database exception for {parish_name}: {db_error}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚úó Processing error for parish #{i+1}: {e}\")\n",
        "\n",
        "    print(f\"  üìä Successfully saved {success_count}/{len(parish_data_list)} parishes\")\n",
        "    return success_count > 0\n",
        "\n",
        "# Test the exact mapping\n",
        "print(\"‚úÖ Exact column mapping loaded!\")\n",
        "print(\"üìã Column mapping:\")\n",
        "print(\"   AI 'Zipcode' -> DB 'Zip Code'\")\n",
        "print(\"   AI 'Website' -> DB 'Web'\")\n",
        "print(\"   AI 'EST' -> Skipped (column doesn't exist)\")\n",
        "print(\"   All other fields map directly\")"
      ],
      "metadata": {
        "id": "dcJ8LbalXnub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Main processing loop with exact mapping\n",
        "\n",
        "def process_single_url(url, driver, attempt=1, max_attempts=2):\n",
        "    \"\"\"Process a single URL with retry logic\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîÑ Processing URL (Attempt {attempt}/{max_attempts}): {url}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract content\n",
        "        content = enhanced_content_extraction(url, driver)\n",
        "\n",
        "        if not content or len(content.strip()) < 100:\n",
        "            print(f\"  ‚úó Insufficient content extracted ({len(content) if content else 0} chars)\")\n",
        "\n",
        "            if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "                print(f\"  üîÑ Retrying with different approach...\")\n",
        "                time.sleep(5)  # Wait before retry\n",
        "                return process_single_url(url, driver, attempt + 1, max_attempts)\n",
        "            else:\n",
        "                return {\n",
        "                    'url': url,\n",
        "                    'status': 'failed',\n",
        "                    'reason': 'insufficient_content',\n",
        "                    'parishes_found': 0\n",
        "                }\n",
        "\n",
        "        # Step 2: Process with Gemini\n",
        "        parish_data_list = process_url_with_enhanced_gemini(url, content)\n",
        "\n",
        "        if not parish_data_list:\n",
        "            print(f\"  ‚úó No parish data extracted by Gemini\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'no_data',\n",
        "                'reason': 'gemini_extraction_failed',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Step 3: Save to database\n",
        "        if len(parish_data_list) == 0:\n",
        "            print(f\"  ‚ÑπÔ∏è Page appears to be a directory/landing page with no specific parish data\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'directory_page',\n",
        "                'reason': 'no_parish_details',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Use the exact mapping function\n",
        "        success = safe_upsert_to_supabase_exact(parish_data_list, url)\n",
        "\n",
        "        print(f\"  üéØ Processing complete for {url}\")\n",
        "        print(f\"     Found: {len(parish_data_list)} parishes\")\n",
        "        print(f\"     Database: {'Success' if success else 'Failed'}\")\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'status': 'success' if success else 'db_failed',\n",
        "            'reason': 'completed',\n",
        "            'parishes_found': len(parish_data_list),\n",
        "            'parishes_saved': success\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Exception processing {url}: {e}\")\n",
        "\n",
        "        if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "            print(f\"  üîÑ Retrying due to exception...\")\n",
        "            time.sleep(5)\n",
        "            return process_single_url(url, driver, attempt + 1, max_attempts)\n",
        "        else:\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'error',\n",
        "                'reason': str(e),\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "# Initialize WebDriver\n",
        "print(\"\\nüöÄ Starting enhanced parish processing...\")\n",
        "driver = setup_webdriver()\n",
        "\n",
        "# Track results\n",
        "results = []\n",
        "total_parishes_found = 0\n",
        "successful_urls = 0\n",
        "\n",
        "try:\n",
        "    for i, url in enumerate(urls_to_process, 1):\n",
        "        print(f\"\\n\\nüìç URL {i}/{len(urls_to_process)}\")\n",
        "\n",
        "        result = process_single_url(url, driver)\n",
        "        results.append(result)\n",
        "\n",
        "        total_parishes_found += result.get('parishes_found', 0)\n",
        "        if result.get('status') == 'success':\n",
        "            successful_urls += 1\n",
        "\n",
        "        # Add delay between requests to be respectful\n",
        "        if i < len(urls_to_process):\n",
        "            print(f\"\\n‚è≥ Waiting 3 seconds before next URL...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "finally:\n",
        "    # Clean up WebDriver\n",
        "    if driver:\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"\\nüßπ WebDriver closed\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"üìä PROCESSING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"URLs processed: {len(results)}\")\n",
        "print(f\"Successful URLs: {successful_urls}\")\n",
        "print(f\"Total parishes found: {total_parishes_found}\")\n",
        "print(f\"\\nDetailed results:\")\n",
        "\n",
        "for result in results:\n",
        "    status_emoji = {\n",
        "        'success': '‚úÖ',\n",
        "        'directory_page': 'üìÅ',\n",
        "        'no_data': '‚ùå',\n",
        "        'failed': '‚ùå',\n",
        "        'error': 'üí•',\n",
        "        'db_failed': '‚ö†Ô∏è'\n",
        "    }.get(result['status'], '‚ùì')\n",
        "\n",
        "    print(f\"  {status_emoji} {result['url']}\")\n",
        "    print(f\"     Status: {result['status']} | Parishes: {result['parishes_found']} | Reason: {result['reason']}\")\n",
        "\n",
        "print(f\"\\nüéâ Enhanced processing complete!\")\n",
        "if SAVE_DEBUG_FILES:\n",
        "    print(f\"üìÑ Debug files saved in 'debug_content/' folder\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "oJC6J2EdXtku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Fixed analysis and debugging tools\n",
        "\n",
        "def analyze_debug_files():\n",
        "    \"\"\"Analyze saved debug files to understand extraction issues\"\"\"\n",
        "\n",
        "    if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "        print(\"No debug files to analyze\")\n",
        "        return\n",
        "\n",
        "    debug_files = [f for f in os.listdir('debug_content') if f.endswith('.html')]\n",
        "\n",
        "    if not debug_files:\n",
        "        print(\"No HTML debug files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüîç Analyzing {len(debug_files)} debug files...\")\n",
        "\n",
        "    for file in debug_files:  # Analyze all files\n",
        "        file_path = f\"debug_content/{file}\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Basic analysis\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # Count potential parish indicators\n",
        "            parish_keywords = ['parish', 'church', 'cathedral', 'mission', 'chapel', 'catholic']\n",
        "            phone_patterns = content.count('(') + content.count('-')\n",
        "            address_indicators = content.lower().count('street') + content.lower().count('avenue') + content.lower().count('road') + content.lower().count('drive')\n",
        "\n",
        "            keyword_count = sum(content.lower().count(keyword) for keyword in parish_keywords)\n",
        "\n",
        "            print(f\"\\nüìÑ {file}:\")\n",
        "            print(f\"   Content length: {len(content):,} chars\")\n",
        "            print(f\"   Parish keywords: {keyword_count}\")\n",
        "            print(f\"   Phone indicators: {phone_patterns}\")\n",
        "            print(f\"   Address indicators: {address_indicators}\")\n",
        "            print(f\"   Links found: {len(soup.find_all('a'))}\")\n",
        "\n",
        "            # Get clean text content\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "            text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "            print(f\"   Clean text length: {len(text_content):,} chars\")\n",
        "            print(f\"   Sample text: {text_content[:200]}...\")\n",
        "\n",
        "            # Look for specific parish data patterns\n",
        "            if 'phone' in text_content.lower() or 'address' in text_content.lower():\n",
        "                print(f\"   ‚úì Contains potential contact info\")\n",
        "            if any(keyword in text_content.lower() for keyword in parish_keywords):\n",
        "                print(f\"   ‚úì Contains parish-related keywords\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error analyzing {file}: {e}\")\n",
        "\n",
        "def show_gemini_responses():\n",
        "    \"\"\"Show Gemini response files for debugging\"\"\"\n",
        "\n",
        "    if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "        print(\"No debug files available\")\n",
        "        return\n",
        "\n",
        "    response_files = [f for f in os.listdir('debug_content') if 'gemini_response' in f]\n",
        "\n",
        "    if not response_files:\n",
        "        print(\"No Gemini response files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nü§ñ Found {len(response_files)} Gemini response files:\")\n",
        "\n",
        "    for file in response_files:\n",
        "        file_path = f\"debug_content/{file}\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            print(f\"\\nüìÑ {file}:\")\n",
        "            print(f\"   Length: {len(content)} chars\")\n",
        "            print(f\"   Content: '{content}'\")\n",
        "\n",
        "            if content.strip() == \"[]\":\n",
        "                print(f\"   ‚ö† Empty array - Gemini found no parish data\")\n",
        "            elif len(content.strip()) < 10:\n",
        "                print(f\"   ‚ö† Very short response - possible error\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error reading {file}: {e}\")\n",
        "\n",
        "def test_gemini_with_sample():\n",
        "    \"\"\"Test Gemini with a sample parish text to see if it's working\"\"\"\n",
        "\n",
        "    if not GENAI_API_KEY:\n",
        "        print(\"‚ùå GenAI not configured\")\n",
        "        return\n",
        "\n",
        "    sample_text = \"\"\"\n",
        "    St. Mary's Catholic Church\n",
        "    123 Main Street\n",
        "    Anytown, TX 75001\n",
        "    Phone: (555) 123-4567\n",
        "    Website: www.stmarys.org\n",
        "\n",
        "    Our Lady of Guadalupe Parish\n",
        "    456 Oak Avenue\n",
        "    Somewhere, TX 75002\n",
        "    Phone: (555) 987-6543\n",
        "    Website: www.olg.org\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üß™ Testing Gemini with sample parish data...\")\n",
        "\n",
        "    try:\n",
        "        from llm_utils import invoke_gemini_model\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Extract parish information from this text. Return valid JSON only:\n",
        "\n",
        "{sample_text}\n",
        "\n",
        "Format as array of objects with keys: Name, Status, Deanery, EST, Street Address, City, State, Zipcode, Phone Number, Website\n",
        "\"\"\"\n",
        "\n",
        "        response = invoke_gemini_model(prompt_text=prompt)\n",
        "        print(f\"\\nüìù Gemini response:\")\n",
        "        print(f\"Length: {len(response)} chars\")\n",
        "        print(f\"Content: {response}\")\n",
        "\n",
        "        # Try to parse\n",
        "        try:\n",
        "            import json\n",
        "            parsed = json.loads(response.strip())\n",
        "            print(f\"‚úì Valid JSON parsed successfully\")\n",
        "            print(f\"‚úì Found {len(parsed)} items\")\n",
        "        except:\n",
        "            print(f\"‚úó Invalid JSON response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing Gemini: {e}\")\n",
        "\n",
        "def inspect_actual_content():\n",
        "    \"\"\"Look at the actual content being sent to Gemini\"\"\"\n",
        "\n",
        "    if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "        print(\"No debug files available\")\n",
        "        return\n",
        "\n",
        "    debug_files = [f for f in os.listdir('debug_content') if f.endswith('.html')]\n",
        "\n",
        "    for file in debug_files[:2]:  # Check first 2 files\n",
        "        file_path = f\"debug_content/{file}\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Extract the same way our script does\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "            print(f\"\\nüìÑ Content analysis for {file}:\")\n",
        "            print(f\"Raw HTML length: {len(content):,} chars\")\n",
        "            print(f\"Processed text length: {len(text_content):,} chars\")\n",
        "            print(f\"\\nFirst 500 chars of processed text:\")\n",
        "            print(f\"'{text_content[:500]}'\")\n",
        "            print(f\"\\nLast 500 chars of processed text:\")\n",
        "            print(f\"'{text_content[-500:]}'\")\n",
        "\n",
        "            # Check if it looks like a parish directory\n",
        "            has_parish_words = any(word in text_content.lower() for word in ['parish', 'church', 'catholic'])\n",
        "            has_contact_info = any(word in text_content.lower() for word in ['phone', 'address', 'email'])\n",
        "\n",
        "            print(f\"\\nContent assessment:\")\n",
        "            print(f\"  Contains parish words: {has_parish_words}\")\n",
        "            print(f\"  Contains contact info: {has_contact_info}\")\n",
        "\n",
        "            if not has_parish_words:\n",
        "                print(f\"  ‚ö† This might not be a parish directory page\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error inspecting {file}: {e}\")\n",
        "\n",
        "def check_supabase_connection():\n",
        "    \"\"\"Test Supabase connection and table structure\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(\"‚ùå Supabase not configured\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Test basic connection\n",
        "        response = supabase.table('Parishes').select('*').limit(5).execute()\n",
        "        print(f\"‚úÖ Supabase connection working\")\n",
        "        print(f\"üìä Sample records in Parishes table: {len(response.data)}\")\n",
        "\n",
        "        if response.data:\n",
        "            print(f\"üìã Sample record structure:\")\n",
        "            for key in response.data[0].keys():\n",
        "                print(f\"   - {key}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Supabase connection test failed: {e}\")\n",
        "\n",
        "print(\"\\nüõ†Ô∏è Enhanced debug tools loaded!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"  - analyze_debug_files() - Analyze scraped HTML content\")\n",
        "print(\"  - show_gemini_responses() - View AI responses\")\n",
        "print(\"  - test_gemini_with_sample() - Test Gemini with known parish data\")\n",
        "print(\"  - inspect_actual_content() - See exactly what text is sent to Gemini\")\n",
        "print(\"  - check_supabase_connection() - Test database connection\")\n",
        "\n",
        "print(\"\\nüîç Let's analyze what happened in your last run:\")\n",
        "analyze_debug_files()\n",
        "show_gemini_responses()\n",
        "\n",
        "print(\"\\nüß™ Testing Gemini with sample data:\")\n",
        "test_gemini_with_sample()"
      ],
      "metadata": {
        "id": "mFKNpgmWXx6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}