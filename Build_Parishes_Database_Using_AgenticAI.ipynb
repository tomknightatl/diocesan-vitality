{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Build_Parishes_Database_Using_AgenticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import required code and install packages\n",
        "!pip install supabase google-generativeai psycopg2-binary tenacity selenium webdriver-manager\n",
        "!wget https://raw.githubusercontent.com/tomknightatl/USCCB/main/llm_utils.py"
      ],
      "metadata": {
        "id": "5kZpoTCpVIut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIm-qDFgrqK3"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import required libraries and install Chrome\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Import Selenium for JavaScript-heavy sites\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "\n",
        "# Chrome Installation for Google Colab\n",
        "def ensure_chrome_installed():\n",
        "    \"\"\"Ensures Chrome is installed in the Colab environment.\"\"\"\n",
        "    try:\n",
        "        # Check if Chrome is already available\n",
        "        result = subprocess.run(['which', 'google-chrome'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Chrome is already installed and available.\")\n",
        "            return True\n",
        "\n",
        "        print(\"üîß Chrome not found. Installing Chrome for Selenium...\")\n",
        "\n",
        "        # Install Chrome\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - > /dev/null 2>&1')\n",
        "        os.system('echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list')\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('apt-get install -y google-chrome-stable > /dev/null 2>&1')\n",
        "\n",
        "        # Verify installation\n",
        "        result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ Chrome installed successfully: {result.stdout.strip()}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Chrome installation may have failed.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during Chrome installation: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the installation check\n",
        "print(\"\\nüîß Checking Chrome installation...\")\n",
        "chrome_ready = ensure_chrome_installed()\n",
        "\n",
        "if chrome_ready:\n",
        "    print(\"üöÄ Ready to proceed with Selenium operations!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è You may need to restart the runtime if Chrome installation failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enhanced configuration and setup\n",
        "print(\"=== ENHANCED PARISH DATABASE BUILDER ===\")\n",
        "print(\"--- User Configurable Parameters & Setup ---\")\n",
        "\n",
        "# --- Processing Configuration ---\n",
        "MAX_URLS_TO_PROCESS = 3  # Start small for testing\n",
        "USE_SELENIUM = True  # Enable JavaScript rendering\n",
        "SAVE_DEBUG_FILES = True  # Save scraped content for debugging\n",
        "RETRY_FAILED_URLS = True  # Retry failed URLs with different methods\n",
        "\n",
        "# Create debug directory\n",
        "if SAVE_DEBUG_FILES:\n",
        "    os.makedirs('debug_content', exist_ok=True)\n",
        "    print(\"Debug directory created for saving scraped content\")\n",
        "\n",
        "print(f\"Processing will be limited to {MAX_URLS_TO_PROCESS} URLs.\")\n",
        "print(f\"JavaScript rendering: {'Enabled' if USE_SELENIUM else 'Disabled'}\")\n",
        "print(f\"Debug mode: {'Enabled' if SAVE_DEBUG_FILES else 'Disabled'}\")\n",
        "\n",
        "# --- Supabase Configuration ---\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "supabase: Client = None\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    try:\n",
        "        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "        print(\"‚úì Supabase client initialized successfully\")\n",
        "\n",
        "        # Test connection and check table structure\n",
        "        try:\n",
        "            test_response = supabase.table('Parishes').select('*').limit(1).execute()\n",
        "            print(\"‚úì Parishes table accessible\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Warning: Could not access Parishes table: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error initializing Supabase client: {e}\")\n",
        "        supabase = None\n",
        "else:\n",
        "    print(\"‚úó Supabase credentials not found in secrets\")\n",
        "    print(\"Required secrets: SUPABASE_URL, SUPABASE_KEY\")\n",
        "\n",
        "# --- GenAI Configuration ---\n",
        "GENAI_API_KEY = userdata.get('GENAI_API_KEY_USCCB')\n",
        "\n",
        "if GENAI_API_KEY:\n",
        "    try:\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        # Test the API\n",
        "        test_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        test_response = test_model.generate_content(\"Say 'API working'\")\n",
        "        print(\"‚úì GenAI configured and tested successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error configuring GenAI: {e}\")\n",
        "        GENAI_API_KEY = None\n",
        "else:\n",
        "    print(\"‚úó GenAI API Key not found (Secret: GENAI_API_KEY_USCCB)\")\n",
        "\n",
        "# --- Enhanced Selenium WebDriver Setup ---\n",
        "def setup_webdriver():\n",
        "    if not USE_SELENIUM:\n",
        "        return None\n",
        "\n",
        "    if not chrome_ready:\n",
        "        print(\"‚ö† Chrome not available - skipping Selenium setup\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "        chrome_options.add_argument('--disable-extensions')\n",
        "        chrome_options.add_argument('--disable-plugins')\n",
        "        chrome_options.add_argument('--disable-images')  # Speed up loading\n",
        "        chrome_options.add_argument('--disable-javascript')  # We'll enable selectively\n",
        "\n",
        "        # Try to use system Chrome first\n",
        "        chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "        # Set up ChromeDriver\n",
        "        try:\n",
        "            service = Service(ChromeDriverManager().install())\n",
        "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "            print(\"‚úì Selenium WebDriver initialized with ChromeDriverManager\")\n",
        "            return driver\n",
        "        except Exception as e:\n",
        "            print(f\"ChromeDriverManager failed: {e}\")\n",
        "            # Fallback to system chromedriver\n",
        "            try:\n",
        "                driver = webdriver.Chrome(options=chrome_options)\n",
        "                print(\"‚úì Selenium WebDriver initialized with system chromedriver\")\n",
        "                return driver\n",
        "            except Exception as e2:\n",
        "                print(f\"System chromedriver also failed: {e2}\")\n",
        "                return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Warning: Could not initialize Selenium: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Data Retrieval from Supabase ---\n",
        "urls_to_process = []\n",
        "if supabase:\n",
        "    try:\n",
        "        print(\"\\nFetching parish directory URLs...\")\n",
        "        response = supabase.table('DiocesesParishDirectory').select('parish_directory_url').not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "        if response.data:\n",
        "            fetched_urls = [item['parish_directory_url'] for item in response.data if item['parish_directory_url']]\n",
        "            print(f\"Found {len(fetched_urls)} URLs in database\")\n",
        "\n",
        "            if MAX_URLS_TO_PROCESS and len(fetched_urls) > MAX_URLS_TO_PROCESS:\n",
        "                urls_to_process = random.sample(fetched_urls, MAX_URLS_TO_PROCESS)\n",
        "                print(f\"Selected {len(urls_to_process)} URLs for processing\")\n",
        "            else:\n",
        "                urls_to_process = fetched_urls\n",
        "                print(f\"Will process all {len(urls_to_process)} URLs\")\n",
        "        else:\n",
        "            print(\"No parish directory URLs found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URLs: {e}\")\n",
        "        urls_to_process = []\n",
        "\n",
        "if not urls_to_process:\n",
        "    print(\"\\n‚ö† No URLs to process - using test URLs\")\n",
        "    urls_to_process = [\n",
        "        \"https://www.dioceseoftyler.org/parishes/\",\n",
        "        \"https://www.diopueblo.org/parishes\",\n",
        "        \"http://www.miamiarch.org/CatholicDiocese.php\"\n",
        "    ]\n",
        "\n",
        "print(f\"\\nüìã Ready to process {len(urls_to_process)} URLs\")\n",
        "for i, url in enumerate(urls_to_process, 1):\n",
        "    print(f\"  {i}. {url}\")\n",
        "print(\"--- Setup Complete ---\\n\")"
      ],
      "metadata": {
        "id": "KIVTfVlOrtIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBUST Cell 4: Advanced pagination detection and navigation\n",
        "\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse\n",
        "\n",
        "def detect_pagination_advanced(soup, url):\n",
        "    \"\"\"Advanced pagination detection with multiple strategies\"\"\"\n",
        "    pagination_info = {\n",
        "        'has_pagination': False,\n",
        "        'current_page': 1,\n",
        "        'total_pages': None,\n",
        "        'next_url': None,\n",
        "        'pagination_type': None,\n",
        "        'base_url': url,\n",
        "        'page_urls': []\n",
        "    }\n",
        "\n",
        "    # Strategy 1: Look for common pagination containers\n",
        "    pagination_selectors = [\n",
        "        '.pagination',\n",
        "        '.pager',\n",
        "        '.page-numbers',\n",
        "        '.pagination-wrapper',\n",
        "        '[class*=\"page\"]',\n",
        "        '[class*=\"pagination\"]',\n",
        "        '.nav-links'\n",
        "    ]\n",
        "\n",
        "    pagination_container = None\n",
        "    for selector in pagination_selectors:\n",
        "        containers = soup.select(selector)\n",
        "        if containers:\n",
        "            pagination_container = containers[0]\n",
        "            break\n",
        "\n",
        "    if not pagination_container:\n",
        "        # Strategy 2: Look for elements with page-related text\n",
        "        page_elements = soup.find_all(['a', 'span', 'div'],\n",
        "                                     string=re.compile(r'(next|previous|page\\s*\\d+)', re.I))\n",
        "        if page_elements:\n",
        "            # Find the parent container\n",
        "            for elem in page_elements:\n",
        "                parent = elem.find_parent(['div', 'nav', 'ul', 'ol'])\n",
        "                if parent:\n",
        "                    pagination_container = parent\n",
        "                    break\n",
        "\n",
        "    if not pagination_container:\n",
        "        return pagination_info\n",
        "\n",
        "    pagination_info['has_pagination'] = True\n",
        "\n",
        "    # Strategy 3: Find all page links\n",
        "    page_links = []\n",
        "\n",
        "    # Look for numbered page links\n",
        "    numbered_links = pagination_container.find_all('a', string=lambda text:\n",
        "                                                  text and text.strip().isdigit())\n",
        "    page_links.extend(numbered_links)\n",
        "\n",
        "    # Look for links with page parameters\n",
        "    param_links = pagination_container.find_all('a', href=re.compile(r'page=\\d+', re.I))\n",
        "    page_links.extend(param_links)\n",
        "\n",
        "    # Look for links with s-push-url (like Toledo Diocese)\n",
        "    push_url_links = pagination_container.find_all('a', attrs={'s-push-url': re.compile(r'page=\\d+', re.I)})\n",
        "    page_links.extend(push_url_links)\n",
        "\n",
        "    # Extract page numbers and URLs\n",
        "    page_info = []\n",
        "    for link in page_links:\n",
        "        try:\n",
        "            # Try to get page number from text\n",
        "            text = link.get_text(strip=True)\n",
        "            if text.isdigit():\n",
        "                page_num = int(text)\n",
        "            else:\n",
        "                # Try to extract from href or s-push-url\n",
        "                href = link.get('href') or link.get('s-push-url', '')\n",
        "                page_match = re.search(r'page=(\\d+)', href, re.I)\n",
        "                if page_match:\n",
        "                    page_num = int(page_match.group(1))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            # Construct full URL\n",
        "            href = link.get('href') or link.get('s-push-url', '')\n",
        "            if href:\n",
        "                if href.startswith('/'):\n",
        "                    page_url = urljoin(url, href)\n",
        "                elif href.startswith('?'):\n",
        "                    # Handle query-only URLs\n",
        "                    parsed_base = urlparse(url)\n",
        "                    page_url = urlunparse((parsed_base.scheme, parsed_base.netloc,\n",
        "                                         parsed_base.path, parsed_base.params,\n",
        "                                         href[1:], parsed_base.fragment))\n",
        "                elif href.startswith('http'):\n",
        "                    page_url = href\n",
        "                else:\n",
        "                    # Relative path\n",
        "                    page_url = urljoin(url, href)\n",
        "\n",
        "                page_info.append((page_num, page_url))\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "\n",
        "    if page_info:\n",
        "        # Sort by page number\n",
        "        page_info.sort(key=lambda x: x[0])\n",
        "        pagination_info['page_urls'] = page_info\n",
        "        pagination_info['total_pages'] = max([p[0] for p in page_info])\n",
        "\n",
        "    # Strategy 4: Find current page\n",
        "    current_indicators = pagination_container.find_all(['span', 'a'],\n",
        "                                                       class_=lambda x: x and 'current' in x.lower())\n",
        "    if current_indicators:\n",
        "        try:\n",
        "            current_text = current_indicators[0].get_text(strip=True)\n",
        "            if current_text.isdigit():\n",
        "                pagination_info['current_page'] = int(current_text)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Strategy 5: Find next page URL\n",
        "    next_url = find_next_page_url(pagination_container, url, pagination_info)\n",
        "    pagination_info['next_url'] = next_url\n",
        "\n",
        "    return pagination_info\n",
        "\n",
        "def find_next_page_url(container, base_url, pagination_info):\n",
        "    \"\"\"Find the next page URL using multiple strategies\"\"\"\n",
        "\n",
        "    # Strategy 1: Look for \"Next\" text\n",
        "    next_link = container.find('a', string=lambda text:\n",
        "                              text and 'next' in text.lower())\n",
        "    if next_link and next_link.get('href'):\n",
        "        return construct_full_url(next_link.get('href'), base_url)\n",
        "\n",
        "    # Strategy 2: Look for right arrow icons\n",
        "    right_arrow_selectors = [\n",
        "        'a i.fa-angle-right',\n",
        "        'a i.fas.fa-angle-right',\n",
        "        'a i.fa-chevron-right',\n",
        "        'a i.fas.fa-chevron-right',\n",
        "        'a .fa-angle-right',\n",
        "        'a .fa-chevron-right'\n",
        "    ]\n",
        "\n",
        "    for selector in right_arrow_selectors:\n",
        "        arrow_link = container.select_one(selector)\n",
        "        if arrow_link:\n",
        "            # Get the parent <a> tag\n",
        "            a_tag = arrow_link.find_parent('a')\n",
        "            if a_tag and (a_tag.get('href') or a_tag.get('s-push-url')):\n",
        "                href = a_tag.get('href') or a_tag.get('s-push-url')\n",
        "                return construct_full_url(href, base_url)\n",
        "\n",
        "    # Strategy 3: Look for page number links (get next sequential page)\n",
        "    current_page = pagination_info.get('current_page', 1)\n",
        "    next_page_num = current_page + 1\n",
        "\n",
        "    if pagination_info.get('page_urls'):\n",
        "        for page_num, page_url in pagination_info['page_urls']:\n",
        "            if page_num == next_page_num:\n",
        "                return page_url\n",
        "\n",
        "    # Strategy 4: Construct next page URL based on pattern\n",
        "    if '?page=' in base_url:\n",
        "        # URL already has page parameter\n",
        "        parsed = urlparse(base_url)\n",
        "        query_params = parse_qs(parsed.query)\n",
        "        query_params['page'] = [str(next_page_num)]\n",
        "        new_query = urlencode(query_params, doseq=True)\n",
        "        return urlunparse((parsed.scheme, parsed.netloc, parsed.path,\n",
        "                          parsed.params, new_query, parsed.fragment))\n",
        "    else:\n",
        "        # Add page parameter\n",
        "        separator = '&' if '?' in base_url else '?'\n",
        "        return f\"{base_url}{separator}page={next_page_num}\"\n",
        "\n",
        "    return None\n",
        "\n",
        "def construct_full_url(href, base_url):\n",
        "    \"\"\"Construct a full URL from href and base URL\"\"\"\n",
        "    if not href:\n",
        "        return None\n",
        "\n",
        "    if href.startswith('http'):\n",
        "        return href\n",
        "    elif href.startswith('/'):\n",
        "        return urljoin(base_url, href)\n",
        "    elif href.startswith('?'):\n",
        "        parsed_base = urlparse(base_url)\n",
        "        return urlunparse((parsed_base.scheme, parsed_base.netloc,\n",
        "                         parsed_base.path, parsed_base.params,\n",
        "                         href[1:], parsed_base.fragment))\n",
        "    else:\n",
        "        return urljoin(base_url, href)\n",
        "\n",
        "def generate_page_urls(base_url, total_pages, current_page=1):\n",
        "    \"\"\"Generate all page URLs for a paginated site\"\"\"\n",
        "    page_urls = []\n",
        "\n",
        "    for page_num in range(1, min(total_pages + 1, 16)):  # Limit to 15 pages max\n",
        "        if '?page=' in base_url:\n",
        "            # Replace existing page parameter\n",
        "            parsed = urlparse(base_url)\n",
        "            query_params = parse_qs(parsed.query)\n",
        "            query_params['page'] = [str(page_num)]\n",
        "            new_query = urlencode(query_params, doseq=True)\n",
        "            page_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path,\n",
        "                                 parsed.params, new_query, parsed.fragment))\n",
        "        else:\n",
        "            # Add page parameter\n",
        "            separator = '&' if '?' in base_url else '?'\n",
        "            page_url = f\"{base_url}{separator}page={page_num}\"\n",
        "\n",
        "        page_urls.append((page_num, page_url))\n",
        "\n",
        "    return page_urls\n",
        "\n",
        "def scrape_single_page_content_robust(url, driver):\n",
        "    \"\"\"Robust single page content extraction\"\"\"\n",
        "\n",
        "    print(f\"    üìÑ Scraping: {url}\")\n",
        "\n",
        "    # Try Selenium first for JavaScript-heavy sites\n",
        "    if driver:\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Wait for content to load\n",
        "            try:\n",
        "                WebDriverWait(driver, 15).until(\n",
        "                    lambda d: len(d.find_elements(By.TAG_NAME, \"body\")) > 0\n",
        "                )\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            page_source = driver.page_source\n",
        "            soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "            if len(text_content) > 500:  # Good content found\n",
        "                save_debug_content(url, page_source, f\"selenium_page\")\n",
        "                return text_content, soup, \"selenium\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚úó Selenium failed for {url}: {e}\")\n",
        "\n",
        "    # Fallback to requests\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "        save_debug_content(url, response.text, f\"requests_page\")\n",
        "        return text_content, soup, \"requests\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚úó Requests failed for {url}: {e}\")\n",
        "        return None, None, \"failed\"\n",
        "\n",
        "def scrape_all_pages_robust(base_url, driver, max_pages=15):\n",
        "    \"\"\"\n",
        "    Robust multi-page scraping with advanced pagination detection\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Starting robust multi-page extraction from: {base_url}\")\n",
        "\n",
        "    all_content = []\n",
        "    visited_urls = set()\n",
        "\n",
        "    # Step 1: Get first page and analyze pagination\n",
        "    content, soup, method = scrape_single_page_content_robust(base_url, driver)\n",
        "\n",
        "    if not content or not soup:\n",
        "        print(f\"  ‚úó Failed to get first page content\")\n",
        "        return []\n",
        "\n",
        "    # Add first page content\n",
        "    all_content.append({\n",
        "        'page': 1,\n",
        "        'url': base_url,\n",
        "        'content': content,\n",
        "        'length': len(content)\n",
        "    })\n",
        "    visited_urls.add(base_url)\n",
        "    print(f\"    ‚úì Page 1 content extracted: {len(content)} characters ({method})\")\n",
        "\n",
        "    # Step 2: Analyze pagination\n",
        "    pagination_info = detect_pagination_advanced(soup, base_url)\n",
        "\n",
        "    if not pagination_info['has_pagination']:\n",
        "        print(f\"    üèÅ No pagination detected\")\n",
        "        return all_content\n",
        "\n",
        "    total_pages = pagination_info.get('total_pages')\n",
        "    print(f\"    üî¢ Pagination detected! Total pages: {total_pages}\")\n",
        "\n",
        "    # Step 3: Generate all page URLs\n",
        "    if pagination_info.get('page_urls'):\n",
        "        # Use detected page URLs\n",
        "        page_urls = pagination_info['page_urls']\n",
        "    elif total_pages:\n",
        "        # Generate URLs based on pattern\n",
        "        page_urls = generate_page_urls(base_url, total_pages)\n",
        "    else:\n",
        "        # Use next_url method\n",
        "        page_urls = []\n",
        "        current_url = base_url\n",
        "        for page_num in range(2, max_pages + 1):\n",
        "            pagination_info = detect_pagination_advanced(soup, current_url)\n",
        "            next_url = pagination_info.get('next_url')\n",
        "            if next_url and next_url not in visited_urls:\n",
        "                page_urls.append((page_num, next_url))\n",
        "                current_url = next_url\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # Step 4: Scrape all additional pages\n",
        "    for page_num, page_url in page_urls:\n",
        "        if page_num == 1 or page_url in visited_urls:\n",
        "            continue  # Skip first page (already processed) or duplicates\n",
        "\n",
        "        if len(all_content) >= max_pages:\n",
        "            print(f\"    üõë Reached maximum pages limit ({max_pages})\")\n",
        "            break\n",
        "\n",
        "        print(f\"    üìÑ Processing page {page_num}: {page_url}\")\n",
        "\n",
        "        content, soup, method = scrape_single_page_content_robust(page_url, driver)\n",
        "\n",
        "        if content and len(content.strip()) > 100:\n",
        "            all_content.append({\n",
        "                'page': page_num,\n",
        "                'url': page_url,\n",
        "                'content': content,\n",
        "                'length': len(content)\n",
        "            })\n",
        "            visited_urls.add(page_url)\n",
        "            print(f\"    ‚úì Page {page_num} content extracted: {len(content)} characters ({method})\")\n",
        "        else:\n",
        "            print(f\"    ‚ö† Page {page_num} had insufficient content\")\n",
        "\n",
        "        # Be respectful with delays\n",
        "        time.sleep(2)\n",
        "\n",
        "    print(f\"  üìä Total pages processed: {len(all_content)}\")\n",
        "    print(f\"  üìä Total content blocks: {len(all_content)}\")\n",
        "\n",
        "    return all_content\n",
        "\n",
        "def enhanced_content_extraction_robust(url, driver=None):\n",
        "    \"\"\"\n",
        "    ROBUST: Enhanced content extraction with advanced pagination support\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Robust content extraction from: {url}\")\n",
        "\n",
        "    # Get all pages using robust method\n",
        "    page_contents = scrape_all_pages_robust(url, driver, MAX_PAGES_PER_SITE)\n",
        "\n",
        "    if not page_contents:\n",
        "        print(f\"  ‚úó No content extracted\")\n",
        "        return None\n",
        "\n",
        "    if len(page_contents) == 1:\n",
        "        # Single page\n",
        "        content = page_contents[0]['content']\n",
        "        print(f\"  üìÑ Single-page extraction: {len(content):,} characters\")\n",
        "        return content[:50000]  # Limit for API\n",
        "    else:\n",
        "        # Multi-page - combine all content\n",
        "        combined_content = combine_page_contents(page_contents)\n",
        "\n",
        "        if combined_content:\n",
        "            # Limit total content length for API processing\n",
        "            max_content_length = 100000\n",
        "            if len(combined_content) > max_content_length:\n",
        "                print(f\"  ‚úÇÔ∏è Truncating content from {len(combined_content):,} to {max_content_length:,} characters\")\n",
        "                combined_content = combined_content[:max_content_length]\n",
        "\n",
        "            print(f\"  üéØ Multi-page extraction complete: {len(combined_content):,} characters\")\n",
        "            return combined_content\n",
        "        else:\n",
        "            # Fallback to first page\n",
        "            content = page_contents[0]['content']\n",
        "            print(f\"  ‚ö† Multi-page combination failed, using first page: {len(content):,} characters\")\n",
        "            return content[:50000]\n",
        "\n",
        "print(\"Robust pagination detection and navigation loaded!\")\n",
        "print(\"Enhanced features:\")\n",
        "print(\"  üîç Advanced pagination pattern detection\")\n",
        "print(\"  üîó Query parameter URL construction (?page=2)\")\n",
        "print(\"  üìÑ Multiple link detection strategies\")\n",
        "print(\"  üõ°Ô∏è Robust error handling and fallbacks\")\n",
        "print(\"  üìä Smart page limit management\")"
      ],
      "metadata": {
        "id": "xlQd-ThXruqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Cell 5: Improved Gemini processing for multi-page content\n",
        "\n",
        "def create_enhanced_pagination_prompt(url, content):\n",
        "    \"\"\"Create a more detailed prompt for multi-page parish extraction\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert at extracting Catholic parish information from web content that may span multiple pages.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. The URL {url} contains a parish directory or parish listing that may span multiple pages\n",
        "2. The content below may contain data from multiple pages, marked with \"=== PAGE X ===\" headers\n",
        "3. Extract information about ALL Catholic parishes found across ALL pages\n",
        "4. Do NOT duplicate parishes that appear on multiple pages - each parish should only appear once in your output\n",
        "5. Look for parishes, churches, missions, chapels, and Catholic communities\n",
        "6. Return ONLY valid JSON - no explanatory text before or after\n",
        "\n",
        "EXPECTED OUTPUT FORMAT:\n",
        "Return a JSON array with ALL parishes found across all pages:\n",
        "[\n",
        "  {{\n",
        "    \"Name\": \"Parish Name\",\n",
        "    \"Status\": \"Parish/Mission/Chapel/Cathedral\",\n",
        "    \"Deanery\": \"Deanery Name\",\n",
        "    \"EST\": \"Established Year\",\n",
        "    \"Street Address\": \"Street Address\",\n",
        "    \"City\": \"City\",\n",
        "    \"State\": \"State\",\n",
        "    \"Zipcode\": \"Zipcode\",\n",
        "    \"Phone Number\": \"Phone\",\n",
        "    \"Website\": \"URL\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "DEDUPLICATION RULES:\n",
        "- If the same parish name appears multiple times, only include it once\n",
        "- Use the most complete information available if duplicates exist\n",
        "- Parish names like \"St. Mary\" in different cities are DIFFERENT parishes\n",
        "\n",
        "Use null for missing values. Extract phone numbers, websites, and addresses carefully.\n",
        "\n",
        "MULTI-PAGE CONTENT:\n",
        "{content[:90000]}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def process_url_with_enhanced_gemini_pagination(url, content):\n",
        "    \"\"\"Process URL content with enhanced Gemini prompting for multi-page content\"\"\"\n",
        "\n",
        "    if not content:\n",
        "        print(f\"  ‚úó No content to process for {url}\")\n",
        "        return None\n",
        "\n",
        "    if not GENAI_API_KEY:\n",
        "        print(f\"  ‚úó GenAI not configured - skipping LLM processing\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"  ü§ñ Processing multi-page content with Gemini... ({len(content):,} chars)\")\n",
        "\n",
        "        # Use the enhanced prompt for multi-page content\n",
        "        if \"=== PAGE\" in content:\n",
        "            prompt = create_enhanced_pagination_prompt(url, content)\n",
        "        else:\n",
        "            # Fall back to original prompt for single-page content\n",
        "            prompt = create_enhanced_prompt(url, content)\n",
        "\n",
        "        response_text = invoke_gemini_model(prompt_text=prompt, model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "        print(f\"  üìù Gemini response length: {len(response_text)} characters\")\n",
        "\n",
        "        # Save raw response for debugging\n",
        "        if SAVE_DEBUG_FILES:\n",
        "            domain = extract_domain(url).replace('.', '_')\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            response_file = f\"debug_content/{domain}_gemini_multipage_response_{timestamp}.json\"\n",
        "            with open(response_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(response_text)\n",
        "            print(f\"  üìÑ Raw response saved: {response_file}\")\n",
        "\n",
        "        # Clean up response\n",
        "        cleaned_response = response_text.strip()\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        if cleaned_response.startswith(\"```json\"):\n",
        "            cleaned_response = cleaned_response[7:]\n",
        "        if cleaned_response.startswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[3:]\n",
        "        if cleaned_response.endswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "        cleaned_response = cleaned_response.strip()\n",
        "\n",
        "        # Parse JSON\n",
        "        try:\n",
        "            parsed_data = json.loads(cleaned_response)\n",
        "\n",
        "            # Handle both single objects and arrays\n",
        "            if isinstance(parsed_data, list):\n",
        "                print(f\"  ‚úì Extracted {len(parsed_data)} parishes from multi-page content\")\n",
        "\n",
        "                # Remove duplicates based on name + city combination\n",
        "                unique_parishes = []\n",
        "                seen_parishes = set()\n",
        "\n",
        "                for parish in parsed_data:\n",
        "                    parish_key = f\"{parish.get('Name', '')}-{parish.get('City', '')}\"\n",
        "                    if parish_key not in seen_parishes:\n",
        "                        unique_parishes.append(parish)\n",
        "                        seen_parishes.add(parish_key)\n",
        "                    else:\n",
        "                        print(f\"    üîÑ Skipped duplicate: {parish.get('Name', 'Unknown')}\")\n",
        "\n",
        "                if len(unique_parishes) != len(parsed_data):\n",
        "                    print(f\"  üßπ Removed {len(parsed_data) - len(unique_parishes)} duplicates\")\n",
        "\n",
        "                return unique_parishes\n",
        "\n",
        "            elif isinstance(parsed_data, dict):\n",
        "                print(f\"  ‚úì Extracted 1 parish: {parsed_data.get('Name', 'Unknown')}\")\n",
        "                return [parsed_data]  # Convert to array for consistent handling\n",
        "            else:\n",
        "                print(f\"  ‚ö† Unexpected data type: {type(parsed_data)}\")\n",
        "                return None\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  ‚úó JSON parsing failed: {e}\")\n",
        "            print(f\"  üìù Response preview: {cleaned_response[:200]}...\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Gemini processing failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_pagination_results(page_contents):\n",
        "    \"\"\"Analyze the results from paginated content extraction\"\"\"\n",
        "    if not page_contents:\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüìä PAGINATION ANALYSIS:\")\n",
        "    print(f\"  Total pages processed: {len(page_contents)}\")\n",
        "\n",
        "    total_chars = sum(page['length'] for page in page_contents)\n",
        "    print(f\"  Total content length: {total_chars:,} characters\")\n",
        "\n",
        "    for page in page_contents:\n",
        "        print(f\"    Page {page['page']}: {page['length']:,} chars - {page['url']}\")\n",
        "\n",
        "# Modified main processing function to use pagination\n",
        "def process_single_url_with_pagination(url, driver, attempt=1, max_attempts=2, max_pages=10):\n",
        "    \"\"\"Process a single URL with enhanced pagination support\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîÑ Processing URL with Pagination (Attempt {attempt}/{max_attempts}): {url}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract content with pagination support\n",
        "        content = enhanced_content_extraction_with_pagination(url, driver, max_pages)\n",
        "\n",
        "        if not content or len(content.strip()) < 100:\n",
        "            print(f\"  ‚úó Insufficient content extracted ({len(content) if content else 0} chars)\")\n",
        "\n",
        "            if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "                print(f\"  üîÑ Retrying with different approach...\")\n",
        "                time.sleep(5)\n",
        "                return process_single_url_with_pagination(url, driver, attempt + 1, max_attempts, max_pages)\n",
        "            else:\n",
        "                return {\n",
        "                    'url': url,\n",
        "                    'status': 'failed',\n",
        "                    'reason': 'insufficient_content',\n",
        "                    'parishes_found': 0\n",
        "                }\n",
        "\n",
        "        # Step 2: Process with Enhanced Gemini (pagination-aware)\n",
        "        parish_data_list = process_url_with_enhanced_gemini_pagination(url, content)\n",
        "\n",
        "        if not parish_data_list:\n",
        "            print(f\"  ‚úó No parish data extracted by Gemini\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'no_data',\n",
        "                'reason': 'gemini_extraction_failed',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Step 3: Save to database\n",
        "        if len(parish_data_list) == 0:\n",
        "            print(f\"  ‚ÑπÔ∏è Page appears to be a directory/landing page with no specific parish data\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'directory_page',\n",
        "                'reason': 'no_parish_details',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Use the exact mapping function\n",
        "        success = safe_upsert_to_supabase_exact(parish_data_list, url)\n",
        "\n",
        "        print(f\"  üéØ Processing complete for {url}\")\n",
        "        print(f\"     Found: {len(parish_data_list)} parishes\")\n",
        "        print(f\"     Database: {'Success' if success else 'Failed'}\")\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'status': 'success' if success else 'db_failed',\n",
        "            'reason': 'completed',\n",
        "            'parishes_found': len(parish_data_list),\n",
        "            'parishes_saved': success,\n",
        "            'extraction_method': 'pagination'\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Exception processing {url}: {e}\")\n",
        "\n",
        "        if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "            print(f\"  üîÑ Retrying due to exception...\")\n",
        "            time.sleep(5)\n",
        "            return process_single_url_with_pagination(url, driver, attempt + 1, max_attempts, max_pages)\n",
        "        else:\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'error',\n",
        "                'reason': str(e),\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "print(\"Enhanced Gemini processing with pagination support loaded!\")\n",
        "print(\"New features:\")\n",
        "print(\"  üî¢ Multi-page content processing\")\n",
        "print(\"  üßπ Automatic duplicate removal\")\n",
        "print(\"  üìä Enhanced pagination analysis\")\n",
        "print(\"  üîç Improved parish extraction accuracy\")"
      ],
      "metadata": {
        "id": "IGR1tf_gXk8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Database operations with exact column mapping\n",
        "\n",
        "def validate_parish_data(parish_data, source_url):\n",
        "    \"\"\"Validate and clean parish data before database insertion\"\"\"\n",
        "\n",
        "    if not isinstance(parish_data, dict):\n",
        "        return None\n",
        "\n",
        "    # Ensure all required fields exist\n",
        "    required_fields = ['Name', 'Status', 'Deanery', 'EST', 'Street Address',\n",
        "                       'City', 'State', 'Zipcode', 'Phone Number', 'Website']\n",
        "\n",
        "    validated_data = {}\n",
        "\n",
        "    for field in required_fields:\n",
        "        value = parish_data.get(field)\n",
        "        # Convert empty strings to None\n",
        "        if value == \"\" or value == \"null\" or value == \"NULL\":\n",
        "            value = None\n",
        "        validated_data[field] = value\n",
        "\n",
        "    # Add metadata\n",
        "    validated_data['source_url'] = source_url\n",
        "    validated_data['domain'] = extract_domain(source_url)\n",
        "    validated_data['processed_at'] = datetime.now().isoformat()\n",
        "\n",
        "    return validated_data\n",
        "\n",
        "def prepare_for_supabase_exact(parish_data):\n",
        "    \"\"\"Convert parish data to match your exact Supabase column names\"\"\"\n",
        "\n",
        "    # Map to your EXACT column names from the schema\n",
        "    return {\n",
        "        'Name': parish_data.get('Name'),\n",
        "        'Status': parish_data.get('Status'),\n",
        "        'Deanery': parish_data.get('Deanery'),\n",
        "        'Street Address': parish_data.get('Street Address'),  # Exact match\n",
        "        'City': parish_data.get('City'),\n",
        "        'State': parish_data.get('State'),\n",
        "        'Zip Code': parish_data.get('Zipcode'),  # Map Zipcode -> Zip Code\n",
        "        'Phone Number': parish_data.get('Phone Number'),  # Exact match\n",
        "        'Web': parish_data.get('Website'),  # Map Website -> Web\n",
        "        # Note: No EST column in your table, so we skip it\n",
        "        # Note: id and created_at are auto-generated by Supabase\n",
        "    }\n",
        "\n",
        "def safe_upsert_to_supabase_exact(parish_data_list, source_url):\n",
        "    \"\"\"Safely insert parish data using exact column mapping\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(f\"  ‚úó Supabase not available - skipping database write\")\n",
        "        return False\n",
        "\n",
        "    if not parish_data_list:\n",
        "        print(f\"  ‚ö† No parish data to save\")\n",
        "        return False\n",
        "\n",
        "    print(f\"  üìã Using exact column mapping for {len(parish_data_list)} parishes\")\n",
        "\n",
        "    success_count = 0\n",
        "\n",
        "    for i, parish_data in enumerate(parish_data_list):\n",
        "        try:\n",
        "            # Validate data\n",
        "            validated_data = validate_parish_data(parish_data, source_url)\n",
        "            if not validated_data:\n",
        "                print(f\"    ‚ö† Skipping invalid parish data #{i+1}\")\n",
        "                continue\n",
        "\n",
        "            # Prepare for Supabase with exact column mapping\n",
        "            supabase_data = prepare_for_supabase_exact(validated_data)\n",
        "\n",
        "            # Skip if no meaningful data (all key fields are null)\n",
        "            key_fields = ['Name', 'Street Address', 'City', 'Phone Number']\n",
        "            if all(supabase_data.get(field) is None for field in key_fields):\n",
        "                print(f\"    ‚ö† Skipping parish #{i+1} - no meaningful data\")\n",
        "                continue\n",
        "\n",
        "            parish_name = supabase_data.get('Name', f'Parish_{i+1}')\n",
        "\n",
        "            # Remove any None values to avoid issues\n",
        "            clean_data = {k: v for k, v in supabase_data.items() if v is not None}\n",
        "\n",
        "            # Simple insert (no upsert since we don't have a unique constraint)\n",
        "            try:\n",
        "                response = supabase.table('Parishes').insert(clean_data).execute()\n",
        "\n",
        "                if hasattr(response, 'error') and response.error:\n",
        "                    print(f\"    ‚úó Database error for {parish_name}: {response.error}\")\n",
        "                else:\n",
        "                    print(f\"    ‚úì Saved: {parish_name}\")\n",
        "                    success_count += 1\n",
        "\n",
        "            except Exception as db_error:\n",
        "                print(f\"    ‚úó Database exception for {parish_name}: {db_error}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚úó Processing error for parish #{i+1}: {e}\")\n",
        "\n",
        "    print(f\"  üìä Successfully saved {success_count}/{len(parish_data_list)} parishes\")\n",
        "    return success_count > 0\n",
        "\n",
        "# Test the exact mapping\n",
        "print(\"‚úÖ Exact column mapping loaded!\")\n",
        "print(\"üìã Column mapping:\")\n",
        "print(\"   AI 'Zipcode' -> DB 'Zip Code'\")\n",
        "print(\"   AI 'Website' -> DB 'Web'\")\n",
        "print(\"   AI 'EST' -> Skipped (column doesn't exist)\")\n",
        "print(\"   All other fields map directly\")"
      ],
      "metadata": {
        "id": "dcJ8LbalXnub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBUST Cell 7: Main processing with advanced pagination\n",
        "\n",
        "# Configuration\n",
        "MAX_PAGES_PER_SITE = 15\n",
        "ENABLE_PAGINATION = True\n",
        "\n",
        "def process_single_url_robust(url, driver, attempt=1, max_attempts=2):\n",
        "    \"\"\"ROBUST: Process a single URL with advanced pagination support\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîÑ ROBUST Processing (Attempt {attempt}/{max_attempts}): {url}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract content using the ROBUST function\n",
        "        content = enhanced_content_extraction_robust(url, driver)\n",
        "\n",
        "        if not content or len(content.strip()) < 100:\n",
        "            print(f\"  ‚úó Insufficient content extracted ({len(content) if content else 0} chars)\")\n",
        "\n",
        "            if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "                print(f\"  üîÑ Retrying with different approach...\")\n",
        "                time.sleep(5)\n",
        "                return process_single_url_robust(url, driver, attempt + 1, max_attempts)\n",
        "            else:\n",
        "                return {\n",
        "                    'url': url,\n",
        "                    'status': 'failed',\n",
        "                    'reason': 'insufficient_content',\n",
        "                    'parishes_found': 0\n",
        "                }\n",
        "\n",
        "        # Step 2: Process with appropriate Gemini function\n",
        "        if \"=== PAGE\" in content:\n",
        "            print(f\"  üî¢ Multi-page content detected\")\n",
        "            parish_data_list = process_url_with_enhanced_gemini_pagination(url, content)\n",
        "        else:\n",
        "            print(f\"  üìÑ Single-page content detected\")\n",
        "            parish_data_list = process_url_with_enhanced_gemini(url, content)\n",
        "\n",
        "        if not parish_data_list:\n",
        "            print(f\"  ‚úó No parish data extracted by Gemini\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'no_data',\n",
        "                'reason': 'gemini_extraction_failed',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Step 3: Save to database\n",
        "        if len(parish_data_list) == 0:\n",
        "            print(f\"  ‚ÑπÔ∏è Page appears to be a directory/landing page with no specific parish data\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'directory_page',\n",
        "                'reason': 'no_parish_details',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Use the exact mapping function\n",
        "        success = safe_upsert_to_supabase_exact(parish_data_list, url)\n",
        "\n",
        "        print(f\"  üéØ Processing complete for {url}\")\n",
        "        print(f\"     Found: {len(parish_data_list)} parishes\")\n",
        "        print(f\"     Database: {'Success' if success else 'Failed'}\")\n",
        "        print(f\"     Method: {'Multi-page' if '=== PAGE' in content else 'Single-page'}\")\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'status': 'success' if success else 'db_failed',\n",
        "            'reason': 'completed',\n",
        "            'parishes_found': len(parish_data_list),\n",
        "            'parishes_saved': success,\n",
        "            'is_multi_page': \"=== PAGE\" in content\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Exception processing {url}: {e}\")\n",
        "\n",
        "        if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "            print(f\"  üîÑ Retrying due to exception...\")\n",
        "            time.sleep(5)\n",
        "            return process_single_url_robust(url, driver, attempt + 1, max_attempts)\n",
        "        else:\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'error',\n",
        "                'reason': str(e),\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "# Initialize WebDriver\n",
        "print(f\"\\nüöÄ Starting ROBUST parish processing with advanced pagination...\")\n",
        "print(f\"üìä Configuration:\")\n",
        "print(f\"   Max pages per site: {MAX_PAGES_PER_SITE}\")\n",
        "print(f\"   Pagination enabled: {ENABLE_PAGINATION}\")\n",
        "print(f\"   Max URLs to process: {MAX_URLS_TO_PROCESS}\")\n",
        "print(f\"   Debug mode: {SAVE_DEBUG_FILES}\")\n",
        "\n",
        "driver = setup_webdriver()\n",
        "\n",
        "# Track results\n",
        "results = []\n",
        "total_parishes_found = 0\n",
        "successful_urls = 0\n",
        "multi_page_count = 0\n",
        "\n",
        "try:\n",
        "    for i, url in enumerate(urls_to_process, 1):\n",
        "        print(f\"\\n\\nüìç URL {i}/{len(urls_to_process)}\")\n",
        "\n",
        "        result = process_single_url_robust(url, driver)\n",
        "        results.append(result)\n",
        "\n",
        "        total_parishes_found += result.get('parishes_found', 0)\n",
        "        if result.get('status') == 'success':\n",
        "            successful_urls += 1\n",
        "\n",
        "        if result.get('is_multi_page', False):\n",
        "            multi_page_count += 1\n",
        "\n",
        "        # Add delay between requests to be respectful\n",
        "        if i < len(urls_to_process):\n",
        "            print(f\"\\n‚è≥ Waiting 3 seconds before next URL...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "finally:\n",
        "    # Clean up WebDriver\n",
        "    if driver:\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"\\nüßπ WebDriver closed\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Print enhanced summary\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"üìä ROBUST PROCESSING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"URLs processed: {len(results)}\")\n",
        "print(f\"Successful URLs: {successful_urls}\")\n",
        "print(f\"Multi-page sites detected: {multi_page_count}\")\n",
        "print(f\"Total parishes found: {total_parishes_found}\")\n",
        "if successful_urls > 0:\n",
        "    print(f\"Average parishes per successful URL: {total_parishes_found/successful_urls:.1f}\")\n",
        "\n",
        "print(f\"\\nDetailed results:\")\n",
        "for result in results:\n",
        "    status_emoji = {\n",
        "        'success': '‚úÖ',\n",
        "        'directory_page': 'üìÅ',\n",
        "        'no_data': '‚ùå',\n",
        "        'failed': '‚ùå',\n",
        "        'error': 'üí•',\n",
        "        'db_failed': '‚ö†Ô∏è'\n",
        "    }.get(result['status'], '‚ùì')\n",
        "\n",
        "    multi_page_indicator = \" [MULTI-PAGE]\" if result.get('is_multi_page', False) else \"\"\n",
        "\n",
        "    print(f\"  {status_emoji} {result['url']}{multi_page_indicator}\")\n",
        "    print(f\"     Status: {result['status']} | Parishes: {result['parishes_found']} | Reason: {result['reason']}\")\n",
        "\n",
        "print(f\"\\nüéâ ROBUST processing complete!\")\n",
        "if SAVE_DEBUG_FILES:\n",
        "    print(f\"üìÑ Debug files saved in 'debug_content/' folder\")\n",
        "\n",
        "if multi_page_count > 0:\n",
        "    print(f\"üî¢ Successfully processed {multi_page_count} multi-page sites\")\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(\"\\n‚úÖ ROBUST enhancements:\")\n",
        "print(\"  üîç Advanced pagination pattern detection\")\n",
        "print(\"  üîó Query parameter URL construction (?page=2)\")\n",
        "print(\"  üìÑ Multiple link detection strategies\")\n",
        "print(\"  üõ°Ô∏è Comprehensive error handling\")\n",
        "print(\"  üìä Smart content combination\")\n",
        "\n",
        "# Test function to verify pagination detection\n",
        "def test_pagination_detection():\n",
        "    \"\"\"Test the pagination detection on a sample\"\"\"\n",
        "    print(f\"\\nüß™ Testing pagination detection...\")\n",
        "\n",
        "    if driver and urls_to_process:\n",
        "        test_url = urls_to_process[1]  # Use Toledo Diocese\n",
        "        print(f\"Testing with: {test_url}\")\n",
        "\n",
        "        try:\n",
        "            driver.get(test_url)\n",
        "            time.sleep(3)\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "            pagination_info = detect_pagination_advanced(soup, test_url)\n",
        "\n",
        "            print(f\"  Has pagination: {pagination_info['has_pagination']}\")\n",
        "            print(f\"  Total pages: {pagination_info.get('total_pages')}\")\n",
        "            print(f\"  Current page: {pagination_info.get('current_page')}\")\n",
        "            print(f\"  Next URL: {pagination_info.get('next_url')}\")\n",
        "\n",
        "            if pagination_info.get('page_urls'):\n",
        "                print(f\"  Found {len(pagination_info['page_urls'])} page URLs:\")\n",
        "                for page_num, page_url in pagination_info['page_urls'][:5]:  # Show first 5\n",
        "                    print(f\"    Page {page_num}: {page_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Test failed: {e}\")\n",
        "    else:\n",
        "        print(f\"  Cannot test - no driver or URLs available\")\n",
        "\n",
        "# Run the test\n",
        "test_pagination_detection()"
      ],
      "metadata": {
        "id": "oJC6J2EdXtku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cell 8: Fixed analysis and debugging tools\n",
        "\n",
        "# def analyze_debug_files():\n",
        "#     \"\"\"Analyze saved debug files to understand extraction issues\"\"\"\n",
        "\n",
        "#     if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "#         print(\"No debug files to analyze\")\n",
        "#         return\n",
        "\n",
        "#     debug_files = [f for f in os.listdir('debug_content') if f.endswith('.html')]\n",
        "\n",
        "#     if not debug_files:\n",
        "#         print(\"No HTML debug files found\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"\\nüîç Analyzing {len(debug_files)} debug files...\")\n",
        "\n",
        "#     for file in debug_files:  # Analyze all files\n",
        "#         file_path = f\"debug_content/{file}\"\n",
        "\n",
        "#         try:\n",
        "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                 content = f.read()\n",
        "\n",
        "#             # Basic analysis\n",
        "#             soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "#             # Count potential parish indicators\n",
        "#             parish_keywords = ['parish', 'church', 'cathedral', 'mission', 'chapel', 'catholic']\n",
        "#             phone_patterns = content.count('(') + content.count('-')\n",
        "#             address_indicators = content.lower().count('street') + content.lower().count('avenue') + content.lower().count('road') + content.lower().count('drive')\n",
        "\n",
        "#             keyword_count = sum(content.lower().count(keyword) for keyword in parish_keywords)\n",
        "\n",
        "#             print(f\"\\nüìÑ {file}:\")\n",
        "#             print(f\"   Content length: {len(content):,} chars\")\n",
        "#             print(f\"   Parish keywords: {keyword_count}\")\n",
        "#             print(f\"   Phone indicators: {phone_patterns}\")\n",
        "#             print(f\"   Address indicators: {address_indicators}\")\n",
        "#             print(f\"   Links found: {len(soup.find_all('a'))}\")\n",
        "\n",
        "#             # Get clean text content\n",
        "#             for script in soup([\"script\", \"style\"]):\n",
        "#                 script.decompose()\n",
        "#             text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "#             print(f\"   Clean text length: {len(text_content):,} chars\")\n",
        "#             print(f\"   Sample text: {text_content[:200]}...\")\n",
        "\n",
        "#             # Look for specific parish data patterns\n",
        "#             if 'phone' in text_content.lower() or 'address' in text_content.lower():\n",
        "#                 print(f\"   ‚úì Contains potential contact info\")\n",
        "#             if any(keyword in text_content.lower() for keyword in parish_keywords):\n",
        "#                 print(f\"   ‚úì Contains parish-related keywords\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"   Error analyzing {file}: {e}\")\n",
        "\n",
        "# def show_gemini_responses():\n",
        "#     \"\"\"Show Gemini response files for debugging\"\"\"\n",
        "\n",
        "#     if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "#         print(\"No debug files available\")\n",
        "#         return\n",
        "\n",
        "#     response_files = [f for f in os.listdir('debug_content') if 'gemini_response' in f]\n",
        "\n",
        "#     if not response_files:\n",
        "#         print(\"No Gemini response files found\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"\\nü§ñ Found {len(response_files)} Gemini response files:\")\n",
        "\n",
        "#     for file in response_files:\n",
        "#         file_path = f\"debug_content/{file}\"\n",
        "#         try:\n",
        "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                 content = f.read()\n",
        "\n",
        "#             print(f\"\\nüìÑ {file}:\")\n",
        "#             print(f\"   Length: {len(content)} chars\")\n",
        "#             print(f\"   Content: '{content}'\")\n",
        "\n",
        "#             if content.strip() == \"[]\":\n",
        "#                 print(f\"   ‚ö† Empty array - Gemini found no parish data\")\n",
        "#             elif len(content.strip()) < 10:\n",
        "#                 print(f\"   ‚ö† Very short response - possible error\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"   Error reading {file}: {e}\")\n",
        "\n",
        "# def test_gemini_with_sample():\n",
        "#     \"\"\"Test Gemini with a sample parish text to see if it's working\"\"\"\n",
        "\n",
        "#     if not GENAI_API_KEY:\n",
        "#         print(\"‚ùå GenAI not configured\")\n",
        "#         return\n",
        "\n",
        "#     sample_text = \"\"\"\n",
        "#     St. Mary's Catholic Church\n",
        "#     123 Main Street\n",
        "#     Anytown, TX 75001\n",
        "#     Phone: (555) 123-4567\n",
        "#     Website: www.stmarys.org\n",
        "\n",
        "#     Our Lady of Guadalupe Parish\n",
        "#     456 Oak Avenue\n",
        "#     Somewhere, TX 75002\n",
        "#     Phone: (555) 987-6543\n",
        "#     Website: www.olg.org\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"üß™ Testing Gemini with sample parish data...\")\n",
        "\n",
        "#     try:\n",
        "#         from llm_utils import invoke_gemini_model\n",
        "\n",
        "#         prompt = f\"\"\"\n",
        "# Extract parish information from this text. Return valid JSON only:\n",
        "\n",
        "# {sample_text}\n",
        "\n",
        "# Format as array of objects with keys: Name, Status, Deanery, EST, Street Address, City, State, Zipcode, Phone Number, Website\n",
        "# \"\"\"\n",
        "\n",
        "#         response = invoke_gemini_model(prompt_text=prompt)\n",
        "#         print(f\"\\nüìù Gemini response:\")\n",
        "#         print(f\"Length: {len(response)} chars\")\n",
        "#         print(f\"Content: {response}\")\n",
        "\n",
        "#         # Try to parse\n",
        "#         try:\n",
        "#             import json\n",
        "#             parsed = json.loads(response.strip())\n",
        "#             print(f\"‚úì Valid JSON parsed successfully\")\n",
        "#             print(f\"‚úì Found {len(parsed)} items\")\n",
        "#         except:\n",
        "#             print(f\"‚úó Invalid JSON response\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå Error testing Gemini: {e}\")\n",
        "\n",
        "# def inspect_actual_content():\n",
        "#     \"\"\"Look at the actual content being sent to Gemini\"\"\"\n",
        "\n",
        "#     if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "#         print(\"No debug files available\")\n",
        "#         return\n",
        "\n",
        "#     debug_files = [f for f in os.listdir('debug_content') if f.endswith('.html')]\n",
        "\n",
        "#     for file in debug_files[:2]:  # Check first 2 files\n",
        "#         file_path = f\"debug_content/{file}\"\n",
        "\n",
        "#         try:\n",
        "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                 content = f.read()\n",
        "\n",
        "#             # Extract the same way our script does\n",
        "#             soup = BeautifulSoup(content, 'html.parser')\n",
        "#             for script in soup([\"script\", \"style\"]):\n",
        "#                 script.decompose()\n",
        "\n",
        "#             text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "#             print(f\"\\nüìÑ Content analysis for {file}:\")\n",
        "#             print(f\"Raw HTML length: {len(content):,} chars\")\n",
        "#             print(f\"Processed text length: {len(text_content):,} chars\")\n",
        "#             print(f\"\\nFirst 500 chars of processed text:\")\n",
        "#             print(f\"'{text_content[:500]}'\")\n",
        "#             print(f\"\\nLast 500 chars of processed text:\")\n",
        "#             print(f\"'{text_content[-500:]}'\")\n",
        "\n",
        "#             # Check if it looks like a parish directory\n",
        "#             has_parish_words = any(word in text_content.lower() for word in ['parish', 'church', 'catholic'])\n",
        "#             has_contact_info = any(word in text_content.lower() for word in ['phone', 'address', 'email'])\n",
        "\n",
        "#             print(f\"\\nContent assessment:\")\n",
        "#             print(f\"  Contains parish words: {has_parish_words}\")\n",
        "#             print(f\"  Contains contact info: {has_contact_info}\")\n",
        "\n",
        "#             if not has_parish_words:\n",
        "#                 print(f\"  ‚ö† This might not be a parish directory page\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error inspecting {file}: {e}\")\n",
        "\n",
        "# def check_supabase_connection():\n",
        "#     \"\"\"Test Supabase connection and table structure\"\"\"\n",
        "\n",
        "#     if not supabase:\n",
        "#         print(\"‚ùå Supabase not configured\")\n",
        "#         return\n",
        "\n",
        "#     try:\n",
        "#         # Test basic connection\n",
        "#         response = supabase.table('Parishes').select('*').limit(5).execute()\n",
        "#         print(f\"‚úÖ Supabase connection working\")\n",
        "#         print(f\"üìä Sample records in Parishes table: {len(response.data)}\")\n",
        "\n",
        "#         if response.data:\n",
        "#             print(f\"üìã Sample record structure:\")\n",
        "#             for key in response.data[0].keys():\n",
        "#                 print(f\"   - {key}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå Supabase connection test failed: {e}\")\n",
        "\n",
        "# print(\"\\nüõ†Ô∏è Enhanced debug tools loaded!\")\n",
        "# print(\"Available functions:\")\n",
        "# print(\"  - analyze_debug_files() - Analyze scraped HTML content\")\n",
        "# print(\"  - show_gemini_responses() - View AI responses\")\n",
        "# print(\"  - test_gemini_with_sample() - Test Gemini with known parish data\")\n",
        "# print(\"  - inspect_actual_content() - See exactly what text is sent to Gemini\")\n",
        "# print(\"  - check_supabase_connection() - Test database connection\")\n",
        "\n",
        "# print(\"\\nüîç Let's analyze what happened in your last run:\")\n",
        "# analyze_debug_files()\n",
        "# show_gemini_responses()\n",
        "\n",
        "# print(\"\\nüß™ Testing Gemini with sample data:\")\n",
        "# test_gemini_with_sample()"
      ],
      "metadata": {
        "id": "mFKNpgmWXx6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}