{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Build_Parishes_Database_Using_AgenticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import required code and install packages\n",
        "!pip install supabase google-generativeai psycopg2-binary tenacity selenium webdriver-manager\n",
        "!wget https://raw.githubusercontent.com/tomknightatl/USCCB/main/llm_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kZpoTCpVIut",
        "outputId": "29747c51-cd1f-4ab4-8e4e-3a9a6059ccb4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supabase in /usr/local/lib/python3.11/dist-packages (2.15.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.11/dist-packages (2.9.10)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: gotrue<3.0.0,>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from supabase) (2.12.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: postgrest<1.1,>0.19 in /usr/local/lib/python3.11/dist-packages (from supabase) (1.0.2)\n",
            "Requirement already satisfied: realtime<2.5.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from supabase) (2.4.3)\n",
            "Requirement already satisfied: storage3<0.12,>=0.10 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.11.3)\n",
            "Requirement already satisfied: supafunc<0.10,>=0.9 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.9.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Requirement already satisfied: pytest-mock<4.0.0,>=3.14.0 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (3.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from postgrest<1.1,>0.19->supabase) (2.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.18 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (3.12.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: websockets<15,>=11 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (14.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: strenum<0.5.0,>=0.4.15 in /usr/local/lib/python3.11/dist-packages (from supafunc<0.10,>=0.9->supabase) (0.4.15)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.20.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.5.0,>=2.4.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.6.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, python-dotenv, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n",
            "--2025-05-27 20:12:19--  https://raw.githubusercontent.com/tomknightatl/USCCB/main/llm_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1638 (1.6K) [text/plain]\n",
            "Saving to: ‘llm_utils.py.1’\n",
            "\n",
            "llm_utils.py.1      100%[===================>]   1.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-27 20:12:20 (24.6 MB/s) - ‘llm_utils.py.1’ saved [1638/1638]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qIm-qDFgrqK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c609711-d760-4df8-f257-980f2724c2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Import required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Import Selenium for JavaScript-heavy sites\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enhanced configuration and setup\n",
        "print(\"=== ENHANCED PARISH DATABASE BUILDER ===\")\n",
        "print(\"--- User Configurable Parameters & Setup ---\")\n",
        "\n",
        "# --- Processing Configuration ---\n",
        "MAX_URLS_TO_PROCESS = 3  # Start small for testing\n",
        "USE_SELENIUM = True  # Enable JavaScript rendering\n",
        "SAVE_DEBUG_FILES = True  # Save scraped content for debugging\n",
        "RETRY_FAILED_URLS = True  # Retry failed URLs with different methods\n",
        "\n",
        "# Create debug directory\n",
        "if SAVE_DEBUG_FILES:\n",
        "    os.makedirs('debug_content', exist_ok=True)\n",
        "    print(\"Debug directory created for saving scraped content\")\n",
        "\n",
        "print(f\"Processing will be limited to {MAX_URLS_TO_PROCESS} URLs.\")\n",
        "print(f\"JavaScript rendering: {'Enabled' if USE_SELENIUM else 'Disabled'}\")\n",
        "print(f\"Debug mode: {'Enabled' if SAVE_DEBUG_FILES else 'Disabled'}\")\n",
        "\n",
        "# --- Supabase Configuration ---\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "supabase: Client = None\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    try:\n",
        "        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "        print(\"✓ Supabase client initialized successfully\")\n",
        "\n",
        "        # Test connection and check table structure\n",
        "        try:\n",
        "            test_response = supabase.table('Parishes').select('*').limit(1).execute()\n",
        "            print(\"✓ Parishes table accessible\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Could not access Parishes table: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error initializing Supabase client: {e}\")\n",
        "        supabase = None\n",
        "else:\n",
        "    print(\"✗ Supabase credentials not found in secrets\")\n",
        "    print(\"Required secrets: SUPABASE_URL, SUPABASE_KEY\")\n",
        "\n",
        "# --- GenAI Configuration ---\n",
        "GENAI_API_KEY = userdata.get('GENAI_API_KEY_USCCB')\n",
        "\n",
        "if GENAI_API_KEY:\n",
        "    try:\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        # Test the API\n",
        "        test_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        test_response = test_model.generate_content(\"Say 'API working'\")\n",
        "        print(\"✓ GenAI configured and tested successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error configuring GenAI: {e}\")\n",
        "        GENAI_API_KEY = None\n",
        "else:\n",
        "    print(\"✗ GenAI API Key not found (Secret: GENAI_API_KEY_USCCB)\")\n",
        "\n",
        "# --- Setup Selenium WebDriver ---\n",
        "def setup_webdriver():\n",
        "    if not USE_SELENIUM:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        print(\"✓ Selenium WebDriver initialized\")\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Could not initialize Selenium: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Data Retrieval from Supabase ---\n",
        "urls_to_process = []\n",
        "if supabase:\n",
        "    try:\n",
        "        print(\"\\nFetching parish directory URLs...\")\n",
        "        response = supabase.table('DiocesesParishDirectory').select('parish_directory_url').not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "        if response.data:\n",
        "            fetched_urls = [item['parish_directory_url'] for item in response.data if item['parish_directory_url']]\n",
        "            print(f\"Found {len(fetched_urls)} URLs in database\")\n",
        "\n",
        "            if MAX_URLS_TO_PROCESS and len(fetched_urls) > MAX_URLS_TO_PROCESS:\n",
        "                urls_to_process = random.sample(fetched_urls, MAX_URLS_TO_PROCESS)\n",
        "                print(f\"Selected {len(urls_to_process)} URLs for processing\")\n",
        "            else:\n",
        "                urls_to_process = fetched_urls\n",
        "                print(f\"Will process all {len(urls_to_process)} URLs\")\n",
        "        else:\n",
        "            print(\"No parish directory URLs found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URLs: {e}\")\n",
        "        urls_to_process = []\n",
        "\n",
        "if not urls_to_process:\n",
        "    print(\"\\n⚠ No URLs to process - using test URLs\")\n",
        "    urls_to_process = [\n",
        "        \"https://www.diopueblo.org/parishes\",\n",
        "        \"https://www.rcbo.org/directories/parishes/\",\n",
        "        \"https://archdiosf.org/directory-for-the-archdiocese\"\n",
        "    ]\n",
        "\n",
        "print(f\"\\n📋 Ready to process {len(urls_to_process)} URLs\")\n",
        "print(\"--- Setup Complete ---\\n\")"
      ],
      "metadata": {
        "id": "KIVTfVlOrtIe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "01602caf-1888-4aa1-deb1-1703ad0fa7eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENHANCED PARISH DATABASE BUILDER ===\n",
            "--- User Configurable Parameters & Setup ---\n",
            "Debug directory created for saving scraped content\n",
            "Processing will be limited to 3 URLs.\n",
            "JavaScript rendering: Enabled\n",
            "Debug mode: Enabled\n",
            "✓ Supabase client initialized successfully\n",
            "⚠ Warning: Could not access Parishes table: {'message': 'relation \"public.Parishes\" does not exist', 'code': '42P01', 'hint': None, 'details': None}\n",
            "✓ GenAI configured and tested successfully\n",
            "\n",
            "Fetching parish directory URLs...\n",
            "Found 192 URLs in database\n",
            "Selected 3 URLs for processing\n",
            "\n",
            "📋 Ready to process 3 URLs\n",
            "--- Setup Complete ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Enhanced web scraping and content extraction functions\n",
        "from llm_utils import invoke_gemini_model\n",
        "\n",
        "def extract_domain(url):\n",
        "    \"\"\"Extract domain from URL\"\"\"\n",
        "    return urlparse(url).netloc\n",
        "\n",
        "def save_debug_content(url, content, method=\"unknown\"):\n",
        "    \"\"\"Save scraped content for debugging\"\"\"\n",
        "    if not SAVE_DEBUG_FILES:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        domain = extract_domain(url).replace('.', '_')\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"debug_content/{domain}_{method}_{timestamp}.html\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"<!-- URL: {url} -->\\n\")\n",
        "            f.write(f\"<!-- Method: {method} -->\\n\")\n",
        "            f.write(f\"<!-- Timestamp: {timestamp} -->\\n\\n\")\n",
        "            f.write(content)\n",
        "\n",
        "        print(f\"  📄 Debug content saved: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ Could not save debug content: {e}\")\n",
        "\n",
        "def scrape_with_requests(url, timeout=10):\n",
        "    \"\"\"Scrape content using requests + BeautifulSoup\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text content\n",
        "        text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "        save_debug_content(url, response.text, \"requests\")\n",
        "\n",
        "        return text_content, len(text_content), \"requests\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Requests failed: {e}\")\n",
        "        return None, 0, \"requests_failed\"\n",
        "\n",
        "def scrape_with_selenium(url, driver, timeout=15):\n",
        "    \"\"\"Scrape content using Selenium for JavaScript rendering\"\"\"\n",
        "    if not driver:\n",
        "        return None, 0, \"selenium_unavailable\"\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for page to load\n",
        "        time.sleep(3)\n",
        "\n",
        "        # Try to wait for common content indicators\n",
        "        try:\n",
        "            WebDriverWait(driver, timeout).until(\n",
        "                lambda d: len(d.find_elements(By.TAG_NAME, \"body\")) > 0\n",
        "            )\n",
        "        except:\n",
        "            pass  # Continue anyway\n",
        "\n",
        "        # Get page source after JavaScript execution\n",
        "        page_source = driver.page_source\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        text_content = ' '.join([s.strip() for s in soup.stripped_strings if s.strip()])\n",
        "\n",
        "        save_debug_content(url, page_source, \"selenium\")\n",
        "\n",
        "        return text_content, len(text_content), \"selenium\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Selenium failed: {e}\")\n",
        "        return None, 0, \"selenium_failed\"\n",
        "\n",
        "def enhanced_content_extraction(url, driver=None):\n",
        "    \"\"\"Try multiple methods to extract content from a URL\"\"\"\n",
        "    print(f\"\\n🔍 Extracting content from: {url}\")\n",
        "\n",
        "    methods_tried = []\n",
        "    best_content = None\n",
        "    best_length = 0\n",
        "    best_method = None\n",
        "\n",
        "    # Method 1: Try requests first (faster)\n",
        "    print(\"  📡 Trying requests method...\")\n",
        "    content, length, method = scrape_with_requests(url)\n",
        "    methods_tried.append((content, length, method))\n",
        "\n",
        "    if content and length > best_length:\n",
        "        best_content, best_length, best_method = content, length, method\n",
        "        print(f\"  ✓ Requests success: {length} characters\")\n",
        "\n",
        "    # Method 2: Try Selenium if requests failed or content is too short\n",
        "    if USE_SELENIUM and (not content or length < 1000):\n",
        "        print(\"  🌐 Trying Selenium method...\")\n",
        "        content, length, method = scrape_with_selenium(url, driver)\n",
        "        methods_tried.append((content, length, method))\n",
        "\n",
        "        if content and length > best_length:\n",
        "            best_content, best_length, best_method = content, length, method\n",
        "            print(f\"  ✓ Selenium success: {length} characters\")\n",
        "\n",
        "    if best_content:\n",
        "        print(f\"  🎯 Best method: {best_method} ({best_length} characters)\")\n",
        "        return best_content[:50000]  # Limit content length for API\n",
        "    else:\n",
        "        print(f\"  ✗ All methods failed for {url}\")\n",
        "        return None\n",
        "\n",
        "print(\"Enhanced scraping functions loaded!\")"
      ],
      "metadata": {
        "id": "xlQd-ThXruqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27f269b-85bd-4ac3-fb2b-f4d70182f4f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced scraping functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Enhanced Gemini processing with better prompts\n",
        "\n",
        "def create_enhanced_prompt(url, content):\n",
        "    \"\"\"Create a more detailed prompt for better parish extraction\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert at extracting Catholic parish information from web content.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. The URL {url} contains a parish directory or parish listing page\n",
        "2. Extract information about Catholic parishes from the provided content\n",
        "3. If the page contains MULTIPLE parishes, extract data for ALL parishes found\n",
        "4. If the page is a directory/landing page with NO specific parish details, return an empty array\n",
        "5. Look for parishes, churches, missions, and Catholic communities\n",
        "6. Return ONLY valid JSON - no explanatory text before or after\n",
        "\n",
        "EXPECTED OUTPUT FORMAT:\n",
        "For multiple parishes, return a JSON array:\n",
        "[\n",
        "  {{\n",
        "    \"Name\": \"Parish Name\",\n",
        "    \"Status\": \"Parish/Mission/Chapel\",\n",
        "    \"Deanery\": \"Deanery Name\",\n",
        "    \"EST\": \"Established Year\",\n",
        "    \"Street Address\": \"Street Address\",\n",
        "    \"City\": \"City\",\n",
        "    \"State\": \"State\",\n",
        "    \"Zipcode\": \"Zipcode\",\n",
        "    \"Phone Number\": \"Phone\",\n",
        "    \"Website\": \"URL\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "For a single parish, return a JSON object:\n",
        "{{\n",
        "  \"Name\": \"Parish Name\",\n",
        "  \"Status\": \"Parish/Mission/Chapel\",\n",
        "  \"Deanery\": \"Deanery Name\",\n",
        "  \"EST\": \"Established Year\",\n",
        "  \"Street Address\": \"Street Address\",\n",
        "  \"City\": \"City\",\n",
        "  \"State\": \"State\",\n",
        "  \"Zipcode\": \"Zipcode\",\n",
        "  \"Phone Number\": \"Phone\",\n",
        "  \"Website\": \"URL\"\n",
        "}}\n",
        "\n",
        "If no parish information is found, return: []\n",
        "\n",
        "Use null for missing values. Extract phone numbers, websites, and addresses carefully.\n",
        "\n",
        "WEBPAGE CONTENT:\n",
        "{content[:45000]}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def process_url_with_enhanced_gemini(url, content):\n",
        "    \"\"\"Process URL content with enhanced Gemini prompting\"\"\"\n",
        "\n",
        "    if not content:\n",
        "        print(f\"  ✗ No content to process for {url}\")\n",
        "        return None\n",
        "\n",
        "    if not GENAI_API_KEY:\n",
        "        print(f\"  ✗ GenAI not configured - skipping LLM processing\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"  🤖 Processing with Gemini... ({len(content)} chars)\")\n",
        "\n",
        "        prompt = create_enhanced_prompt(url, content)\n",
        "        response_text = invoke_gemini_model(prompt_text=prompt, model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "        print(f\"  📝 Gemini response length: {len(response_text)} characters\")\n",
        "\n",
        "        # Save raw response for debugging\n",
        "        if SAVE_DEBUG_FILES:\n",
        "            domain = extract_domain(url).replace('.', '_')\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            response_file = f\"debug_content/{domain}_gemini_response_{timestamp}.json\"\n",
        "            with open(response_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(response_text)\n",
        "            print(f\"  📄 Raw response saved: {response_file}\")\n",
        "\n",
        "        # Clean up response\n",
        "        cleaned_response = response_text.strip()\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        if cleaned_response.startswith(\"```json\"):\n",
        "            cleaned_response = cleaned_response[7:]\n",
        "        if cleaned_response.startswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[3:]\n",
        "        if cleaned_response.endswith(\"```\"):\n",
        "            cleaned_response = cleaned_response[:-3]\n",
        "\n",
        "        cleaned_response = cleaned_response.strip()\n",
        "\n",
        "        # Parse JSON\n",
        "        try:\n",
        "            parsed_data = json.loads(cleaned_response)\n",
        "\n",
        "            # Handle both single objects and arrays\n",
        "            if isinstance(parsed_data, list):\n",
        "                print(f\"  ✓ Extracted {len(parsed_data)} parishes\")\n",
        "                return parsed_data\n",
        "            elif isinstance(parsed_data, dict):\n",
        "                print(f\"  ✓ Extracted 1 parish: {parsed_data.get('Name', 'Unknown')}\")\n",
        "                return [parsed_data]  # Convert to array for consistent handling\n",
        "            else:\n",
        "                print(f\"  ⚠ Unexpected data type: {type(parsed_data)}\")\n",
        "                return None\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  ✗ JSON parsing failed: {e}\")\n",
        "            print(f\"  📝 Response preview: {cleaned_response[:200]}...\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Gemini processing failed: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Enhanced Gemini processing functions loaded!\")"
      ],
      "metadata": {
        "id": "IGR1tf_gXk8N",
        "outputId": "5e0fc327-f3a6-4d49-f4c5-324c9e0316f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Gemini processing functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Enhanced database operations\n",
        "\n",
        "def validate_parish_data(parish_data, source_url):\n",
        "    \"\"\"Validate and clean parish data before database insertion\"\"\"\n",
        "\n",
        "    if not isinstance(parish_data, dict):\n",
        "        return None\n",
        "\n",
        "    # Ensure all required fields exist\n",
        "    required_fields = ['Name', 'Status', 'Deanery', 'EST', 'Street Address',\n",
        "                       'City', 'State', 'Zipcode', 'Phone Number', 'Website']\n",
        "\n",
        "    validated_data = {}\n",
        "\n",
        "    for field in required_fields:\n",
        "        value = parish_data.get(field)\n",
        "        # Convert empty strings to None\n",
        "        if value == \"\" or value == \"null\" or value == \"NULL\":\n",
        "            value = None\n",
        "        validated_data[field] = value\n",
        "\n",
        "    # Add metadata\n",
        "    validated_data['source_url'] = source_url\n",
        "    validated_data['domain'] = extract_domain(source_url)\n",
        "    validated_data['processed_at'] = datetime.now().isoformat()\n",
        "\n",
        "    return validated_data\n",
        "\n",
        "def prepare_for_supabase(parish_data):\n",
        "    \"\"\"Convert parish data to Supabase-compatible format\"\"\"\n",
        "\n",
        "    return {\n",
        "        'Name': parish_data.get('Name'),\n",
        "        'Status': parish_data.get('Status'),\n",
        "        'Deanery': parish_data.get('Deanery'),\n",
        "        'EST': parish_data.get('EST'),\n",
        "        'StreetAddress': parish_data.get('Street Address'),  # Note: field name change\n",
        "        'City': parish_data.get('City'),\n",
        "        'State': parish_data.get('State'),\n",
        "        'Zipcode': parish_data.get('Zipcode'),\n",
        "        'PhoneNumber': parish_data.get('Phone Number'),  # Note: field name change\n",
        "        'Website': parish_data.get('Website'),\n",
        "        'source_url': parish_data.get('source_url'),\n",
        "        'domain': parish_data.get('domain'),\n",
        "        'processed_at': parish_data.get('processed_at')\n",
        "    }\n",
        "\n",
        "def safe_upsert_to_supabase(parish_data_list, source_url):\n",
        "    \"\"\"Safely upsert parish data to Supabase with enhanced error handling\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(f\"  ✗ Supabase not available - skipping database write\")\n",
        "        return False\n",
        "\n",
        "    if not parish_data_list:\n",
        "        print(f\"  ⚠ No parish data to save\")\n",
        "        return False\n",
        "\n",
        "    success_count = 0\n",
        "\n",
        "    for i, parish_data in enumerate(parish_data_list):\n",
        "        try:\n",
        "            # Validate data\n",
        "            validated_data = validate_parish_data(parish_data, source_url)\n",
        "            if not validated_data:\n",
        "                print(f\"    ⚠ Skipping invalid parish data #{i+1}\")\n",
        "                continue\n",
        "\n",
        "            # Prepare for Supabase\n",
        "            supabase_data = prepare_for_supabase(validated_data)\n",
        "\n",
        "            # Skip if no meaningful data (all key fields are null)\n",
        "            key_fields = ['Name', 'StreetAddress', 'City', 'PhoneNumber']\n",
        "            if all(supabase_data.get(field) is None for field in key_fields):\n",
        "                print(f\"    ⚠ Skipping parish #{i+1} - no meaningful data\")\n",
        "                continue\n",
        "\n",
        "            # Create a unique identifier for upsert\n",
        "            parish_name = supabase_data.get('Name', 'Unknown')\n",
        "            unique_id = f\"{source_url}_{i}\"\n",
        "            supabase_data['unique_id'] = unique_id\n",
        "\n",
        "            # Attempt upsert\n",
        "            try:\n",
        "                response = supabase.table('Parishes').upsert(\n",
        "                    supabase_data,\n",
        "                    on_conflict='unique_id'\n",
        "                ).execute()\n",
        "\n",
        "                if hasattr(response, 'error') and response.error:\n",
        "                    print(f\"    ✗ Database error for {parish_name}: {response.error}\")\n",
        "                else:\n",
        "                    print(f\"    ✓ Saved: {parish_name}\")\n",
        "                    success_count += 1\n",
        "\n",
        "            except Exception as db_error:\n",
        "                print(f\"    ✗ Database exception for {parish_name}: {db_error}\")\n",
        "\n",
        "                # Try alternative approach - insert without upsert\n",
        "                try:\n",
        "                    # Remove unique_id and try simple insert\n",
        "                    insert_data = {k: v for k, v in supabase_data.items() if k != 'unique_id'}\n",
        "                    response = supabase.table('Parishes').insert(insert_data).execute()\n",
        "\n",
        "                    if not (hasattr(response, 'error') and response.error):\n",
        "                        print(f\"    ✓ Saved via insert: {parish_name}\")\n",
        "                        success_count += 1\n",
        "                    else:\n",
        "                        print(f\"    ✗ Insert also failed: {response.error}\")\n",
        "\n",
        "                except Exception as insert_error:\n",
        "                    print(f\"    ✗ Insert exception: {insert_error}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ Processing error for parish #{i+1}: {e}\")\n",
        "\n",
        "    print(f\"  📊 Successfully saved {success_count}/{len(parish_data_list)} parishes\")\n",
        "    return success_count > 0\n",
        "\n",
        "print(\"Enhanced database operations loaded!\")"
      ],
      "metadata": {
        "id": "dcJ8LbalXnub",
        "outputId": "b5c44401-c74a-4649-b813-c9b8bbc9dc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced database operations loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Main processing loop with comprehensive error handling\n",
        "\n",
        "def process_single_url(url, driver, attempt=1, max_attempts=2):\n",
        "    \"\"\"Process a single URL with retry logic\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🔄 Processing URL (Attempt {attempt}/{max_attempts}): {url}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract content\n",
        "        content = enhanced_content_extraction(url, driver)\n",
        "\n",
        "        if not content or len(content.strip()) < 100:\n",
        "            print(f\"  ✗ Insufficient content extracted ({len(content) if content else 0} chars)\")\n",
        "\n",
        "            if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "                print(f\"  🔄 Retrying with different approach...\")\n",
        "                time.sleep(5)  # Wait before retry\n",
        "                return process_single_url(url, driver, attempt + 1, max_attempts)\n",
        "            else:\n",
        "                return {\n",
        "                    'url': url,\n",
        "                    'status': 'failed',\n",
        "                    'reason': 'insufficient_content',\n",
        "                    'parishes_found': 0\n",
        "                }\n",
        "\n",
        "        # Step 2: Process with Gemini\n",
        "        parish_data_list = process_url_with_enhanced_gemini(url, content)\n",
        "\n",
        "        if not parish_data_list:\n",
        "            print(f\"  ✗ No parish data extracted by Gemini\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'no_data',\n",
        "                'reason': 'gemini_extraction_failed',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        # Step 3: Save to database\n",
        "        if len(parish_data_list) == 0:\n",
        "            print(f\"  ℹ️ Page appears to be a directory/landing page with no specific parish data\")\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'directory_page',\n",
        "                'reason': 'no_parish_details',\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "        success = safe_upsert_to_supabase(parish_data_list, url)\n",
        "\n",
        "        print(f\"  🎯 Processing complete for {url}\")\n",
        "        print(f\"     Found: {len(parish_data_list)} parishes\")\n",
        "        print(f\"     Database: {'Success' if success else 'Failed'}\")\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'status': 'success' if success else 'db_failed',\n",
        "            'reason': 'completed',\n",
        "            'parishes_found': len(parish_data_list),\n",
        "            'parishes_saved': success\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Exception processing {url}: {e}\")\n",
        "\n",
        "        if attempt < max_attempts and RETRY_FAILED_URLS:\n",
        "            print(f\"  🔄 Retrying due to exception...\")\n",
        "            time.sleep(5)\n",
        "            return process_single_url(url, driver, attempt + 1, max_attempts)\n",
        "        else:\n",
        "            return {\n",
        "                'url': url,\n",
        "                'status': 'error',\n",
        "                'reason': str(e),\n",
        "                'parishes_found': 0\n",
        "            }\n",
        "\n",
        "# Initialize WebDriver\n",
        "print(\"\\n🚀 Starting enhanced parish processing...\")\n",
        "driver = setup_webdriver()\n",
        "\n",
        "# Track results\n",
        "results = []\n",
        "total_parishes_found = 0\n",
        "successful_urls = 0\n",
        "\n",
        "try:\n",
        "    for i, url in enumerate(urls_to_process, 1):\n",
        "        print(f\"\\n\\n📍 URL {i}/{len(urls_to_process)}\")\n",
        "\n",
        "        result = process_single_url(url, driver)\n",
        "        results.append(result)\n",
        "\n",
        "        total_parishes_found += result.get('parishes_found', 0)\n",
        "        if result.get('status') == 'success':\n",
        "            successful_urls += 1\n",
        "\n",
        "        # Add delay between requests to be respectful\n",
        "        if i < len(urls_to_process):\n",
        "            print(f\"\\n⏳ Waiting 3 seconds before next URL...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "finally:\n",
        "    # Clean up WebDriver\n",
        "    if driver:\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"\\n🧹 WebDriver closed\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"📊 PROCESSING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"URLs processed: {len(results)}\")\n",
        "print(f\"Successful URLs: {successful_urls}\")\n",
        "print(f\"Total parishes found: {total_parishes_found}\")\n",
        "print(f\"\\nDetailed results:\")\n",
        "\n",
        "for result in results:\n",
        "    status_emoji = {\n",
        "        'success': '✅',\n",
        "        'directory_page': '📁',\n",
        "        'no_data': '❌',\n",
        "        'failed': '❌',\n",
        "        'error': '💥',\n",
        "        'db_failed': '⚠️'\n",
        "    }.get(result['status'], '❓')\n",
        "\n",
        "    print(f\"  {status_emoji} {result['url']}\")\n",
        "    print(f\"     Status: {result['status']} | Parishes: {result['parishes_found']} | Reason: {result['reason']}\")\n",
        "\n",
        "print(f\"\\n🎉 Enhanced processing complete!\")\n",
        "if SAVE_DEBUG_FILES:\n",
        "    print(f\"📄 Debug files saved in 'debug_content/' folder\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "oJC6J2EdXtku",
        "outputId": "8e13b348-e5a9-4069-f3d4-59a86436ca8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Starting enhanced parish processing...\n",
            "⚠ Warning: Could not initialize Selenium: Message: unknown error: cannot find Chrome binary\n",
            "Stacktrace:\n",
            "#0 0x5cd36b80d4e3 <unknown>\n",
            "#1 0x5cd36b53cc76 <unknown>\n",
            "#2 0x5cd36b563757 <unknown>\n",
            "#3 0x5cd36b562029 <unknown>\n",
            "#4 0x5cd36b5a0ccc <unknown>\n",
            "#5 0x5cd36b5a047f <unknown>\n",
            "#6 0x5cd36b597de3 <unknown>\n",
            "#7 0x5cd36b56d2dd <unknown>\n",
            "#8 0x5cd36b56e34e <unknown>\n",
            "#9 0x5cd36b7cd3e4 <unknown>\n",
            "#10 0x5cd36b7d13d7 <unknown>\n",
            "#11 0x5cd36b7dbb20 <unknown>\n",
            "#12 0x5cd36b7d2023 <unknown>\n",
            "#13 0x5cd36b7a01aa <unknown>\n",
            "#14 0x5cd36b7f66b8 <unknown>\n",
            "#15 0x5cd36b7f6847 <unknown>\n",
            "#16 0x5cd36b806243 <unknown>\n",
            "#17 0x7ae71bee5ac3 <unknown>\n",
            "\n",
            "\n",
            "\n",
            "📍 URL 1/3\n",
            "\n",
            "============================================================\n",
            "🔄 Processing URL (Attempt 1/2): https://www.dioceseoftyler.org/parishes/\n",
            "============================================================\n",
            "\n",
            "🔍 Extracting content from: https://www.dioceseoftyler.org/parishes/\n",
            "  📡 Trying requests method...\n",
            "  📄 Debug content saved: debug_content/www_dioceseoftyler_org_requests_20250527_201331.html\n",
            "  ✓ Requests success: 1279 characters\n",
            "  🎯 Best method: requests (1279 characters)\n",
            "  🤖 Processing with Gemini... (1279 chars)\n",
            "  📝 Gemini response length: 3 characters\n",
            "  📄 Raw response saved: debug_content/www_dioceseoftyler_org_gemini_response_20250527_201332.json\n",
            "  ✓ Extracted 0 parishes\n",
            "  ✗ No parish data extracted by Gemini\n",
            "\n",
            "⏳ Waiting 3 seconds before next URL...\n",
            "\n",
            "\n",
            "📍 URL 2/3\n",
            "\n",
            "============================================================\n",
            "🔄 Processing URL (Attempt 1/2): https://www.diopueblo.org/parishes\n",
            "============================================================\n",
            "\n",
            "🔍 Extracting content from: https://www.diopueblo.org/parishes\n",
            "  📡 Trying requests method...\n",
            "  📄 Debug content saved: debug_content/www_diopueblo_org_requests_20250527_201335.html\n",
            "  ✓ Requests success: 591 characters\n",
            "  🌐 Trying Selenium method...\n",
            "  🎯 Best method: requests (591 characters)\n",
            "  🤖 Processing with Gemini... (591 chars)\n",
            "  📝 Gemini response length: 3 characters\n",
            "  📄 Raw response saved: debug_content/www_diopueblo_org_gemini_response_20250527_201336.json\n",
            "  ✓ Extracted 0 parishes\n",
            "  ✗ No parish data extracted by Gemini\n",
            "\n",
            "⏳ Waiting 3 seconds before next URL...\n",
            "\n",
            "\n",
            "📍 URL 3/3\n",
            "\n",
            "============================================================\n",
            "🔄 Processing URL (Attempt 1/2): http://www.miamiarch.org/CatholicDiocese.php\n",
            "============================================================\n",
            "\n",
            "🔍 Extracting content from: http://www.miamiarch.org/CatholicDiocese.php\n",
            "  📡 Trying requests method...\n",
            "  📄 Debug content saved: debug_content/www_miamiarch_org_requests_20250527_201341.html\n",
            "  ✓ Requests success: 5736 characters\n",
            "  🎯 Best method: requests (5736 characters)\n",
            "  🤖 Processing with Gemini... (5736 chars)\n",
            "  📝 Gemini response length: 3 characters\n",
            "  📄 Raw response saved: debug_content/www_miamiarch_org_gemini_response_20250527_201342.json\n",
            "  ✓ Extracted 0 parishes\n",
            "  ✗ No parish data extracted by Gemini\n",
            "\n",
            "\n",
            "============================================================\n",
            "📊 PROCESSING SUMMARY\n",
            "============================================================\n",
            "URLs processed: 3\n",
            "Successful URLs: 0\n",
            "Total parishes found: 0\n",
            "\n",
            "Detailed results:\n",
            "  ❌ https://www.dioceseoftyler.org/parishes/\n",
            "     Status: no_data | Parishes: 0 | Reason: gemini_extraction_failed\n",
            "  ❌ https://www.diopueblo.org/parishes\n",
            "     Status: no_data | Parishes: 0 | Reason: gemini_extraction_failed\n",
            "  ❌ http://www.miamiarch.org/CatholicDiocese.php\n",
            "     Status: no_data | Parishes: 0 | Reason: gemini_extraction_failed\n",
            "\n",
            "🎉 Enhanced processing complete!\n",
            "📄 Debug files saved in 'debug_content/' folder\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Optional - Analysis and debugging tools\n",
        "\n",
        "def analyze_debug_files():\n",
        "    \"\"\"Analyze saved debug files to understand extraction issues\"\"\"\n",
        "\n",
        "    if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "        print(\"No debug files to analyze\")\n",
        "        return\n",
        "\n",
        "    debug_files = [f for f in os.listdir('debug_content') if f.endswith('.html')]\n",
        "\n",
        "    if not debug_files:\n",
        "        print(\"No HTML debug files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🔍 Analyzing {len(debug_files)} debug files...\")\n",
        "\n",
        "    for file in debug_files[:3]:  # Analyze first 3 files\n",
        "        file_path = f\"debug_content/{file}\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Basic analysis\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # Count potential parish indicators\n",
        "            parish_keywords = ['parish', 'church', 'cathedral', 'mission', 'chapel']\n",
        "            phone_patterns = content.count('(')\n",
        "            address_indicators = content.count('street') + content.count('avenue') + content.count('road')\n",
        "\n",
        "            keyword_count = sum(content.lower().count(keyword) for keyword in parish_keywords)\n",
        "\n",
        "            print(f\"\\n📄 {file}:\")\n",
        "            print(f\"   Content length: {len(content):,} chars\")\n",
        "            print(f\"   Parish keywords: {keyword_count}\")\n",
        "            print(f\"   Phone indicators: {phone_patterns}\")\n",
        "            print(f\"   Address indicators: {address_indicators}\")\n",
        "            print(f\"   Links found: {len(soup.find_all('a'))}\")\n",
        "\n",
        "            # Sample text\n",
        "            text_content = soup.get_text()[:500]\n",
        "            print(f\"   Sample text: {text_content[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error analyzing {file}: {e}\")\n",
        "\n",
        "def test_single_url(test_url):\n",
        "    \"\"\"Test processing on a single URL for debugging\"\"\"\n",
        "\n",
        "    print(f\"\\n🧪 Testing single URL: {test_url}\")\n",
        "\n",
        "    driver = setup_webdriver()\n",
        "\n",
        "    try:\n",
        "        result = process_single_url(test_url, driver)\n",
        "        print(f\"\\n🧪 Test result: {result}\")\n",
        "        return result\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "def check_supabase_connection():\n",
        "    \"\"\"Test Supabase connection and table structure\"\"\"\n",
        "\n",
        "    if not supabase:\n",
        "        print(\"❌ Supabase not configured\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Test basic connection\n",
        "        response = supabase.table('Parishes').select('*').limit(5).execute()\n",
        "        print(f\"✅ Supabase connection working\")\n",
        "        print(f\"📊 Sample records in Parishes table: {len(response.data)}\")\n",
        "\n",
        "        if response.data:\n",
        "            print(f\"📋 Sample record structure:\")\n",
        "            for key in response.data[0].keys():\n",
        "                print(f\"   - {key}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Supabase connection test failed: {e}\")\n",
        "\n",
        "def show_gemini_responses():\n",
        "    \"\"\"Show Gemini response files for debugging\"\"\"\n",
        "\n",
        "    if not SAVE_DEBUG_FILES or not os.path.exists('debug_content'):\n",
        "        print(\"No debug files available\")\n",
        "        return\n",
        "\n",
        "    response_files = [f for f in os.listdir('debug_content') if 'gemini_response' in f]\n",
        "\n",
        "    if not response_files:\n",
        "        print(\"No Gemini response files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🤖 Found {len(response_files)} Gemini response files:\")\n",
        "\n",
        "    for file in response_files[:3]:  # Show first 3\n",
        "        file_path = f\"debug_content/{file}\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            print(f\"\\n📄 {file}:\")\n",
        "            print(f\"   Length: {len(content)} chars\")\n",
        "            print(f\"   Preview: {content[:200]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error reading {file}: {e}\")\n",
        "\n",
        "def quick_test_urls():\n",
        "    \"\"\"Quick test of a few URLs to see what content is being extracted\"\"\"\n",
        "\n",
        "    test_urls = [\n",
        "        \"https://www.diopueblo.org/parishes\",\n",
        "        \"https://www.rcbo.org/directories/parishes/\"\n",
        "    ]\n",
        "\n",
        "    driver = setup_webdriver()\n",
        "\n",
        "    try:\n",
        "        for url in test_urls:\n",
        "            print(f\"\\n🔍 Quick test: {url}\")\n",
        "            content = enhanced_content_extraction(url, driver)\n",
        "\n",
        "            if content:\n",
        "                print(f\"   ✓ Extracted {len(content)} characters\")\n",
        "                print(f\"   Sample: {content[:300]}...\")\n",
        "            else:\n",
        "                print(f\"   ✗ No content extracted\")\n",
        "\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "print(\"\\n🛠️ Debug tools loaded!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"  - analyze_debug_files() - Analyze scraped HTML content\")\n",
        "print(\"  - test_single_url('your_url_here') - Test a specific URL\")\n",
        "print(\"  - check_supabase_connection() - Test database connection\")\n",
        "print(\"  - show_gemini_responses() - View AI responses\")\n",
        "print(\"  - quick_test_urls() - Quick content extraction test\")\n",
        "\n",
        "# Uncomment any of these to run analysis:\n",
        "# analyze_debug_files()\n",
        "# check_supabase_connection()\n",
        "# show_gemini_responses()\n",
        "# quick_test_urls()"
      ],
      "metadata": {
        "id": "mFKNpgmWXx6o",
        "outputId": "98e367e6-bcc3-4997-8ae5-22b0ba811e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🛠️ Debug tools loaded!\n",
            "Available functions:\n",
            "  - analyze_debug_files() - Analyze scraped HTML content\n",
            "  - test_single_url('your_url_here') - Test a specific URL\n",
            "  - check_supabase_connection() - Test database connection\n",
            "  - show_gemini_responses() - View AI responses\n",
            "  - quick_test_urls() - Quick content extraction test\n"
          ]
        }
      ]
    }
  ]
}