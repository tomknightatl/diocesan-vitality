{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/USCCB/blob/main/Build_Parishes_Database_Using_AgenticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIm-qDFgrqK3"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import required libraries\n",
        "!pip install supabase google-generativeai psycopg2-binary tenacity\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from urllib.parse import urlparse\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: User-configurable parameters, Supabase Setup, and Data Retrieval\n",
        "\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client, Client\n",
        "import os # For environment variables if needed, and MAX_URLS_TO_PROCESS logic\n",
        "import random\n",
        "\n",
        "print(\"--- User Configurable Parameters & Supabase Setup ---\")\n",
        "\n",
        "# --- Processing Limit Configuration ---\n",
        "# Set the maximum number of parish directory URLs to process (None = process all)\n",
        "MAX_URLS_TO_PROCESS = 5  # Change this number or set to None to process all\n",
        "                         # STARTING WITH A SMALL NUMBER FOR TESTING\n",
        "\n",
        "if MAX_URLS_TO_PROCESS:\n",
        "    print(f\"Processing will be limited to {MAX_URLS_TO_PROCESS} randomly selected URLs.\")\n",
        "else:\n",
        "    print(\"Processing will include all relevant URLs from DiocesesParishDirectory.\")\n",
        "\n",
        "# --- Supabase Configuration ---\n",
        "SUPABASE_URL = None\n",
        "SUPABASE_KEY = None\n",
        "SUPABASE_URL_FROM_USERDATA = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY_FROM_USERDATA = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "if SUPABASE_URL_FROM_USERDATA:\n",
        "    SUPABASE_URL = SUPABASE_URL_FROM_USERDATA\n",
        "if SUPABASE_KEY_FROM_USERDATA:\n",
        "    SUPABASE_KEY = SUPABASE_KEY_FROM_USERDATA\n",
        "\n",
        "supabase: Client = None\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    try:\n",
        "        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "        print(\"Supabase client initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Supabase client: {e}\")\n",
        "        supabase = None\n",
        "else:\n",
        "    print(\"Supabase URL and/or Key NOT loaded. Please check Colab Secrets.\")\n",
        "    print(\"Required secrets: SUPABASE_URL, SUPABASE_KEY\")\n",
        "\n",
        "# --- GenAI API Key Setup ---\n",
        "GENAI_API_KEY_FROM_USERDATA = userdata.get('GENAI_API_KEY_USCCB') # Assuming same secret name as other notebook\n",
        "GENAI_API_KEY = None\n",
        "\n",
        "if GENAI_API_KEY_FROM_USERDATA and GENAI_API_KEY_FROM_USERDATA not in [\"YOUR_API_KEY_PLACEHOLDER\", \"SET_YOUR_KEY_HERE\"]:\n",
        "    GENAI_API_KEY = GENAI_API_KEY_FROM_USERDATA\n",
        "\n",
        "if GENAI_API_KEY:\n",
        "    try:\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        print(\"GenAI configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GenAI with key: {e}. GenAI features might not work.\")\n",
        "        GENAI_API_KEY = None # Ensure it's None if configuration fails\n",
        "else:\n",
        "    print(\"GenAI API Key is not set (Secret: GENAI_API_KEY_USCCB). LLM features will not work.\")\n",
        "\n",
        "# --- Data Retrieval from Supabase ---\n",
        "urls_to_process = []\n",
        "if supabase:\n",
        "    try:\n",
        "        print(\"Fetching parish directory URLs from DiocesesParishDirectory table...\")\n",
        "        # Fetch non-null, non-empty parish_directory_url\n",
        "        query = supabase.table('DiocesesParishDirectory').select('parish_directory_url').not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '')\n",
        "        \n",
        "        response = query.execute()\n",
        "        \n",
        "        if response.data:\n",
        "            fetched_urls = [item['parish_directory_url'] for item in response.data if item['parish_directory_url']]\n",
        "            print(f\"Successfully fetched {len(fetched_urls)} URLs from Supabase.\")\n",
        "            \n",
        "            if MAX_URLS_TO_PROCESS and len(fetched_urls) > MAX_URLS_TO_PROCESS:\n",
        "                urls_to_process = random.sample(fetched_urls, MAX_URLS_TO_PROCESS)\n",
        "                print(f\"Randomly selected {len(urls_to_process)} URLs for processing.\")\n",
        "            else:\n",
        "                urls_to_process = fetched_urls\n",
        "                print(f\"Processing all {len(urls_to_process)} fetched URLs.\")\n",
        "        else:\n",
        "            print(\"No parish directory URLs found in DiocesesParishDirectory or error in fetching.\")\n",
        "            if hasattr(response, 'error') and response.error:\n",
        "                 print(f\"Supabase error: {response.error}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URLs from Supabase: {e}\")\n",
        "        urls_to_process = []\n",
        "else:\n",
        "    print(\"Supabase client not initialized. Cannot fetch URLs.\")\n",
        "\n",
        "if not urls_to_process:\n",
        "    print(\"No URLs to process. Further steps might be skipped or fail.\")\n",
        "else:\n",
        "    print(f\"Prepared {len(urls_to_process)} URLs for processing.\")\n",
        "\n",
        "# For subsequent cells to use, we will name the list of URLs `urls` as in the original notebook\n",
        "urls = [(url,) for url in urls_to_process] # Keep the tuple structure if downstream code expects it\n",
        "\n",
        "print(\"--- End User Configurable Parameters & Supabase Setup ---\")"
      ],
      "metadata": {
        "id": "KIVTfVlOrtIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Process each URL using Gemini API\n",
        "from llm_utils import invoke_gemini_model\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def extract_domain(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.netloc\n",
        "\n",
        "def process_url_with_gemini(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract visible text from the webpage\n",
        "    visible_text = ' '.join([s for s in soup.stripped_strings])\n",
        "\n",
        "    # Prepare the prompt for Gemini\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following webpage content, which is from the URL {url}.\n",
        "    Extract parish information. The information should include:\n",
        "    Name, Status, Deanery, EST (Established Date), Street Address, City, State, Zipcode, Phone Number, and Website.\n",
        "    If any specific piece of information is not found or not applicable, use the JSON value null for that field.\n",
        "    Format the output as a single, valid JSON object with these exact keys:\n",
        "    \"Name\", \"Status\", \"Deanery\", \"EST\", \"Street Address\", \"City\", \"State\", \"Zipcode\", \"Phone Number\", \"Website\".\n",
        "\n",
        "    Webpage content (first 40000 characters):\n",
        "    {visible_text[:40000]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Call Gemini API\n",
        "    try:\n",
        "        # Ensure GENAI_API_KEY is loaded and genai is configured (done in Cell 3)\n",
        "        if 'GENAI_API_KEY' not in globals() or not GENAI_API_KEY:\n",
        "            print(\"GenAI API Key not configured. Skipping LLM call.\")\n",
        "            return None\n",
        "\n",
        "        # Call the shared Gemini function\n",
        "        response_text = invoke_gemini_model(prompt_text=prompt) # invoke_gemini_model is from llm_utils\n",
        "\n",
        "        print(f\"Gemini API Response: {response_text}\") # Log the raw API response\n",
        "\n",
        "        # Attempt to parse the JSON response\n",
        "        # Gemini might sometimes wrap JSON in ```json ... ```, so try to strip that\n",
        "        if response_text.strip().startswith(\"```json\"):\n",
        "            content_to_parse = response_text.strip()[7:-3].strip()\n",
        "        elif response_text.strip().startswith(\"```\"): # Generic backticks\n",
        "            content_to_parse = response_text.strip()[3:-3].strip()\n",
        "        else:\n",
        "            content_to_parse = response_text.strip()\n",
        "        \n",
        "        extracted_data = json.loads(content_to_parse)\n",
        "        return extracted_data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON Decode Error: {str(e)}\")\n",
        "        print(f\"Raw API Response that failed parsing: {response_text}\")\n",
        "        # Return a dict with nulls to indicate parsing failure but allow DB storage of this failure\n",
        "        return {\"Name\": None, \"Status\": \"JSON Decode Error\", \"Deanery\": None, \"EST\": None, \n",
        "                \"Street Address\": str(e), \"City\": None, \"State\": None, \"Zipcode\": None, \n",
        "                \"Phone Number\": None, \"Website\": None, \"source_url\": url, \"domain\": extract_domain(url)}\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {str(e)}\")\n",
        "        # Return a dict with nulls to indicate API error\n",
        "        return {\"Name\": None, \"Status\": \"API Error\", \"Deanery\": None, \"EST\": None, \n",
        "                \"Street Address\": str(e), \"City\": None, \"State\": None, \"Zipcode\": None, \n",
        "                \"Phone Number\": None, \"Website\": None, \"source_url\": url, \"domain\": extract_domain(url)}\n",
        "\n",
        "# Process each URL\n",
        "for url_tuple in urls: # Assuming urls is a list of tuples from Cell 3\n",
        "    url = url_tuple[0] \n",
        "    print(f\"Processing URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        parish_data = process_url_with_gemini(url)\n",
        "\n",
        "        if parish_data and supabase: # Ensure data and supabase client exist\n",
        "            # Add source_url and domain if not already added by error handling in process_url_with_gemini\n",
        "            if 'source_url' not in parish_data:\n",
        "                 parish_data['source_url'] = url\n",
        "            if 'domain' not in parish_data:\n",
        "                 parish_data['domain'] = extract_domain(url)\n",
        "\n",
        "            # Prepare data for Supabase, ensuring all keys are present, defaulting to None if missing\n",
        "            data_to_upsert = {\n",
        "                'Name': parish_data.get('Name'),\n",
        "                'Status': parish_data.get('Status'),\n",
        "                'Deanery': parish_data.get('Deanery'),\n",
        "                'EST': parish_data.get('EST'),\n",
        "                # Supabase table uses 'StreetAddress', JSON uses 'Street Address'\n",
        "                'StreetAddress': parish_data.get('Street Address'), \n",
        "                'City': parish_data.get('City'),\n",
        "                'State': parish_data.get('State'),\n",
        "                'Zipcode': parish_data.get('Zipcode'),\n",
        "                'PhoneNumber': parish_data.get('Phone Number'), # Supabase table uses 'PhoneNumber'\n",
        "                'Website': parish_data.get('Website'),\n",
        "                'source_url': parish_data['source_url'],\n",
        "                'domain': parish_data['domain']\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Upsert into Parishes table. Assuming 'source_url' can be a unique identifier for upsert.\n",
        "                # If not, adjust conflict resolution or use insert.\n",
        "                # For now, using 'source_url' as the conflict target for an upsert.\n",
        "                # This implies 'source_url' column in 'Parishes' table must have a unique constraint.\n",
        "                response = supabase.table('Parishes').upsert(data_to_upsert, on_conflict='source_url').execute()\n",
        "                \n",
        "                if hasattr(response, 'error') and response.error:\n",
        "                    print(f\"Error upserting data to Supabase for {url}: {response.error}\")\n",
        "                else:\n",
        "                    print(f\"Data upserted to Supabase for: {data_to_upsert.get('Name', 'Unknown Parish')} from {url}\")\n",
        "\n",
        "            except Exception as supa_error:\n",
        "                print(f\"Supabase API error during upsert for {url}: {supa_error}\")\n",
        "        elif not supabase:\n",
        "            print(f\"Supabase client not available. Skipping database write for {url}.\")\n",
        "        elif not parish_data:\n",
        "             print(f\"No data extracted for {url}. Skipping database write.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "print(\"All URLs processed.\")"
      ],
      "metadata": {
        "id": "xlQd-ThXruqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}